[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "COMP/STAT112 Notebook",
    "section": "",
    "text": "Welcome\nWelcome to my online portfolio for COMP/STAT112 course taken at Macalester College. Please, use the side bar on the left for navigation.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "bw/bw-uni.html",
    "href": "bw/bw-uni.html",
    "title": "\n1  Univariate Viz\n",
    "section": "",
    "text": "# Load packages and data into R\nlibrary(ggplot2)\nhikes &lt;- read.csv(\"https://mac-stat.github.io/data/high_peaks.csv\")\n\n\n# Creating the visualization\n\nggplot(hikes, aes(x = elevation)) +\n  geom_density(color = \"black\") +\n  geom_density(fill = \"black\") +\n  labs(title = \"Elevation Distribution of Hikes in the Adirondack mountains\", \n       subtitle = \"From data on 46 hikes in the Adirondack mountains published by Adirondack.net\",\n       x = \"Elevation at Peak\",\n       y = \"Proportion of Hikes\",\n       alt = \"A density plot titled 'Elevation distribution of hikes in the adirondack mountains' of numerical data from 46 hikes in the Adirondack mountains showing the relative occurrences of elevation hikes at the peaks. There is a trend that the data is right skewed, and seems that the most common elevation was about 4200 feet.\")\n\n\n\n\n\n\n\nI have chosen to show a density plot of the elevations of 46 hikes in the Adirondack mountains as my effective visualization for a univariate graph. What is effective is determined by what we want to show, and I think this graph does a good job help people understand the general trends and range of hike elevations in the Adirondack mountains. It was important to me to include the size of the data set in the caption of the image so that people have an idea of the sample size, despite the arbitrarily numbered y axis. In terms of accessible visualization, I modified the graph so that it is very high contrast, with the trend line and shape being shown in bold black against a pale background.",
    "crumbs": [
      "Best Work",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Univariate Viz</span>"
    ]
  },
  {
    "objectID": "bw/bw-bi.html",
    "href": "bw/bw-bi.html",
    "title": "\n2  Bivariate Viz\n",
    "section": "",
    "text": "# Loading packages and data into R\n\nlibrary(ggplot2)\nlibrary(viridisLite)\nlibrary(viridis)\n\nelections &lt;- read.csv(\"https://mac-stat.github.io/data/election_2020_county.csv\")\n\n\n# Code to create the bar graph for the visualization\n\nggplot(elections, aes(x = historical, fill = winner_20)) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"County Votes in 2020 compared to Historical Trends\", \n       subtitle = \"Compare the outcomes by county for the 2020 US Presidential election with their historical leanings\",\n       x = \"Historical Voting Pattern\",\n       y = \"Proportion of 2020 outcome\",\n       alt = \"A filled bar graph titled 'County votes in 2020 compared to historical trends' showing the proportion of counties that voted Republican and democrat, with different bars for the historical preference of each county, labelled blue, purple, or red. A trend appears that 0nly 67.5 percent of historical blue counties voted republican in 2020, while 80 percent of historically purple counties woted republican, and over 90 percent of historically republican counties also voted Republican in the 2020 election.\",\n       fill = \"Democratic or Republican in 2020\") +\n  scale_fill_viridis(discrete = TRUE)\n\n\n\n\n\n\n\nI chose to use my presidential election in 2020 versus historically bar graph for my best work for bivariate visualization. I added more accessible colors using the viridis addon, and although it may seem less intuitive, the yellow and purple are higher contrast and more accessible for color-blind readers of the chart. The well-labelled legend helps interpret and overcome any confusion caused by the colors, and the non-intuitive colors are okay as otherwise the visualization is pretty simple. I think the filled bars are effective visualization because they show the proportions, which we are interested in, rather than distracting us with the exact number of counties that voted a given way. It allows the data to be easily comparable across categories.",
    "crumbs": [
      "Best Work",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bivariate Viz</span>"
    ]
  },
  {
    "objectID": "bw/bw-tri.html",
    "href": "bw/bw-tri.html",
    "title": "\n3  Trivariate Viz\n",
    "section": "",
    "text": "# Import data\neducation &lt;- read.csv(\"https://mac-stat.github.io/data/sat.csv\")\n\n# Load packages and data into R\nlibrary(ggplot2)\nlibrary(viridis)\n\nLoading required package: viridisLite\n\nlibrary(viridisLite)\n\n\nggplot(education, aes(x = expend, y = sat, color = fracCat)) +\n  geom_point() +\n  geom_smooth(se = FALSE, method = \"lm\") +\n  labs(title = \"1995 SAT scores compared with state education exenditure per pupil\", \n       subtitle = \"Subcategory trend lines showing the fraction of eligible students per state who took the SAT\",\n       x = \"Average States Education Expenditure per Pupil (Thousands USD)\",\n       y = \"SAT Score\",\n       color = \"Percent Participation\",\n       alt = \"A scatterplot with trend lines for three separate categories. The chart is titled '1995 SAT scores compared with state education expenditure per pupil' and shows SAT score on the Y axis and the average state expenditure per pupil on the x axis. A third variable, the fraction is shown by colors and trend lines, with categories of states in which 0 to 15, 15 to 45, and 45 to 100 percent of eligible students took the SAT. Generally, states in which lower percentages of students to the SAT had higher average scores, but within each category, the SAT scores went up slightly with higher per pupil education expenditure.\") +\n  scale_color_manual(values = c(\"#648FFF\", \"#DC267F\", \"#FFB000\"))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFor my trivariate visualization, I have chosen this as it effectively utilizes trend lines within categories of a third variable (the fraction of students in a state that took the SAT) to displace some conclusions that might otherwise be drawn by merely viewing the points on the graph. We can see that within states where similar proportions of students took the SAT as each other, there is a general trend that the higher the state expenditure per pupil, the higher the SAT scores. The color palette that I used involves colors from the IBM accessible color palette. The viridis default was very low contrast with three values, and was giving a bright yellow color which was very difficult to see on the grey background. Due to adding my own manual values, I have a high contrast, appropriate, and accessible visualization.",
    "crumbs": [
      "Best Work",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Trivariate Viz</span>"
    ]
  },
  {
    "objectID": "bw/bw-quad.html",
    "href": "bw/bw-quad.html",
    "title": "\n4  Quadvariate (Multivariate) Viz\n",
    "section": "",
    "text": "#Load data\neducation &lt;- read.csv(\"https://mac-stat.github.io/data/sat.csv\")\n\n# Load libraries\nlibrary(ggplot2)\nlibrary(viridis)\n\nLoading required package: viridisLite\n\nlibrary(viridisLite)\nlibrary(tibble)\nlibrary(gplots)\n\n\nAttaching package: 'gplots'\n\n\nThe following object is masked from 'package:stats':\n\n    lowess\n\n\n\n# Remove the \"State\" column and use it to label the rows\n# Then scale the variables\nplot_data &lt;- education |&gt; \n  column_to_rownames(\"State\") |&gt; \n  data.matrix() |&gt; \n  scale()\n\n# Construct the heatmap\nheatmap.2(plot_data,\n  dendrogram = \"none\",\n  Rowv = TRUE,            \n  scale = \"column\",\n  keysize = 0.7, \n  density.info = \"none\",\n  col = hcl.colors(256), \n  margins = c(10, 20),\n  colsep = c(1:7), rowsep = (1:50), sepwidth = c(0.05, 0.05),\n  sepcolor = \"white\", trace = \"none\",\n  main = \"SAT data heatmap in 1994-95\",\n  alt = \"A heatmap titled 'SAT data heatmap in 1995-95' which lists the states along the x-axis and 'frac, fracCat, expend, salary, ratio, verbal, sat, and math' as variables along the x axis. The colors represent the Z-score of each variable for each state. Particular areas with high Z scores are the expenditure variable for New York and New Jersey, and the ratio variable for Utah and Californa.\"\n)\n\nWarning in plot.window(...): \"alt\" is not a graphical parameter\n\n\nWarning in plot.xy(xy, type, ...): \"alt\" is not a graphical parameter\n\n\nWarning in title(...): \"alt\" is not a graphical parameter\n\n\n\n\n\n\n\n\nAlthough this plot has many more than four variables, I chose it to be in my best work folder because it does a very good job showing areas of outliers and to which attention should be paid for many continuous numerical variables. This heat map does require some knowledge of the x axis titles, as well as an understanding that Z-score is the distance of a value from data points of the same parameter. I wish I knew how to change the x-axis titles to be more accessible, but the color palette in existence is accessible from a physical point of view, as it is high contrast. From looking at this graph we can clearly see the ratio variable (ratio of pupils to teacher) is an outlier in Utah and California, and New York and New Jersey have particularly high expend (education expenditure per student).",
    "crumbs": [
      "Best Work",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Quadvariate (Multivariate) Viz</span>"
    ]
  },
  {
    "objectID": "bw/bw-spatial.html",
    "href": "bw/bw-spatial.html",
    "title": "\n5  Spatial Viz\n",
    "section": "",
    "text": "# Load packages\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\n\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\n\n\nAttaching package: 'mosaic'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(rnaturalearth)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.4     ✔ tibble    3.2.1\n✔ purrr     1.0.4     ✔ tidyr     1.3.1\n✔ readr     2.1.5     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ mosaic::count() masks dplyr::count()\n✖ purrr::cross()  masks mosaic::cross()\n✖ mosaic::do()    masks dplyr::do()\n✖ tidyr::expand() masks Matrix::expand()\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ tidyr::pack()   masks Matrix::pack()\n✖ mosaic::stat()  masks ggplot2::stat()\n✖ mosaic::tally() masks dplyr::tally()\n✖ tidyr::unpack() masks Matrix::unpack()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Import starbucks location data\nstarbucks &lt;- read.csv(\"https://mac-stat.github.io/data/starbucks.csv\")\n\n# Get info about country boundaries across the world\n# in a \"sf\" or simple feature format\nworld_boundaries &lt;- ne_countries(returnclass = \"sf\")\n\n\nstarbucks_cma &lt;- starbucks |&gt; \n  filter(Country %in% c('CA', 'MX', 'US'))\n\ncma_boundaries &lt;- ne_states(\n  country = c(\"canada\", \"mexico\", \"united states of america\"),\n  returnclass = \"sf\")\n\n# Add the points\n# And zoom in\nggplot(cma_boundaries) + \n  geom_sf() + \n  geom_point(\n    data = starbucks_cma,\n    aes(x = Longitude, y = Latitude),\n    alpha = 0.3,\n    size = 0.2,\n    color = \"darkgreen\"\n  ) +\n  coord_sf(xlim = c(-179.14, -50)) +\n  theme_map() +\n  labs(title = \"Map of Starbucks Locations in North America\",\n       subtitle = \"Each green dot represents one Starbucks location\",\n       alt = \"A line map titled 'Map of Starbucks Locations in North America' of the United States, Canada, and Mexico, with green dots across it, each representing a Starbucks location. There is a concentration of Starbucks on the east coast of the US, as well as several clusters along the west coast. Ther is a correlation between the locations of major cities in the US and Starbucks location clusters on the map. There are the most Starbucks in the US, with generally lower densities in Mexico and Canada.\")\n\n\n\n\n\n\n\nThis is an effective, high-contrast visualization that is at the correct scale size to help viewers see trends in the number of Starbucks across areas of North America. If we wanted to know where a specific Starbucks was, we would need a different, more close-up map, but this map helps see trends such as many Starbucks being on the east coast and in California, and how many Starbucks locations align with the location of major cities in the US (based on an understanding of US geography and city locations. Regardless of prior knowledge, this graph would let people know where the most Starbucks are, as well as identify general areas where there are fewer Starbucks.",
    "crumbs": [
      "Best Work",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Spatial Viz</span>"
    ]
  },
  {
    "objectID": "bw/bw-exam1.html",
    "href": "bw/bw-exam1.html",
    "title": "\n6  Exam 1 Visualization Process\n",
    "section": "",
    "text": "This document is used to create a visualization working with the Food Consumption and CO2 Emissions dataset used in Week 8 of the year 2020 in the TidyTuesday project. The data includes four variables: country: A character variable with the country name of the observation. food_category: A character variable that tells what type of food category the observation is measuring consumption: A double variable that tells the consumption of the observation in kg/person/year co2_emmission: A double variable which tells the CO2 emission in kg/person/year of each observation.\nThe main question to answer throughout this exam and data exploration is “What does the consumption of each food category in each country look like?\n\nlibrary(tidytuesdayR, quietly = TRUE)\nlibrary(tidyverse, quietly = TRUE)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nNow we will create a dataset in our R workspace called tuesdata from the tidy tuesday dataset on the desired date. Then we load the file of interest into the fc dataset in our workspace.\n\ntuesdata &lt;- tt_load('2020-02-18')\n\n---- Compiling #TidyTuesday Information for 2020-02-18 ----\n--- There is 1 file available ---\n\n\n── Downloading files ───────────────────────────────────────────────────────────\n\n  1 of 1: \"food_consumption.csv\"\n\nfc &lt;- tuesdata$food_consumption\n\nhead(fc)\n\n# A tibble: 6 × 4\n  country   food_category consumption co2_emmission\n  &lt;chr&gt;     &lt;chr&gt;               &lt;dbl&gt;         &lt;dbl&gt;\n1 Argentina Pork                10.5          37.2 \n2 Argentina Poultry             38.7          41.5 \n3 Argentina Beef                55.5        1712   \n4 Argentina Lamb & Goat          1.56         54.6 \n5 Argentina Fish                 4.36          6.96\n6 Argentina Eggs                11.4          10.5 \n\ndim(fc)\n\n[1] 1430    4\n\nnames(fc)\n\n[1] \"country\"       \"food_category\" \"consumption\"   \"co2_emmission\"\n\n\nLooking at the top and bottom 22 rows of data, we can determine the units of observation to be combinations of each of 11 food categories and each of 130 countries (determined using the dim() command in the chunk above).\n\nhead(fc, 22)\n\n# A tibble: 22 × 4\n   country   food_category            consumption co2_emmission\n   &lt;chr&gt;     &lt;chr&gt;                          &lt;dbl&gt;         &lt;dbl&gt;\n 1 Argentina Pork                           10.5          37.2 \n 2 Argentina Poultry                        38.7          41.5 \n 3 Argentina Beef                           55.5        1712   \n 4 Argentina Lamb & Goat                     1.56         54.6 \n 5 Argentina Fish                            4.36          6.96\n 6 Argentina Eggs                           11.4          10.5 \n 7 Argentina Milk - inc. cheese            195.          278.  \n 8 Argentina Wheat and Wheat Products      103.           19.7 \n 9 Argentina Rice                            8.77         11.2 \n10 Argentina Soybeans                        0             0   \n# ℹ 12 more rows\n\ntail(fc, 22)\n\n# A tibble: 22 × 4\n   country food_category            consumption co2_emmission\n   &lt;chr&gt;   &lt;chr&gt;                          &lt;dbl&gt;         &lt;dbl&gt;\n 1 Liberia Pork                            4.01         14.2 \n 2 Liberia Poultry                         8.91          9.57\n 3 Liberia Beef                            0.78         24.1 \n 4 Liberia Lamb & Goat                     0.48         16.8 \n 5 Liberia Fish                            4.13          6.59\n 6 Liberia Eggs                            2.05          1.88\n 7 Liberia Milk - inc. cheese              3.04          4.33\n 8 Liberia Wheat and Wheat Products       11.0           2.09\n 9 Liberia Rice                           94.8         121.  \n10 Liberia Soybeans                        0.63          0.28\n# ℹ 12 more rows\n\n\nNow we can begin to explore some of our variables of interest with visualizations. What are our variables of interest? Because we want to know the distribution of food category consumption between countries, we are interested in country, food_category, and consumption. First, let’s see how many observations there are per country. We can do this by creating a bar chart, which will show how many rows in the dataset each country name corresponds to. The bar chart below shows that each country has 11 observed rows (or occurrences) in the dataset, corresponding to the 11 food categories possible.\n\nggplot(fc, aes(y = country)) +\n  geom_bar()\n\n\n\n\n\n\n\nNext, we can explore the food category variable, and see how many countries have listings for. Below we see that each food category has 130 observations, corresponding to the 130 countries in the dataset.\n\nggplot(fc, aes(y = food_category)) +\n  geom_bar() +\n  scale_x_continuous(breaks = seq(0, 150, by = 10))\n\n\n\n\n\n\n\nFinally for our exploration of unique variable of interest, we will create a density plot of the total per person consumption distributions across every food category in every country. We see that most of the observations of food categories and per person consumption in each country are just above 0, very low compared to some of the outliers, as we see small peaks just above 100 and even at 250 kg/person/year. Thinking critically, we might expect that most of the consumption categories, like soybeans, pork, and eggs, have very low numbers in most countries, whereas some of the broader categories like wheat and wheat products generally have much higher consumption rates across most countries.\n\nggplot(fc, aes(x = consumption)) +\n  geom_density()\n\n\n\n\n\n\n\nNow we can move on to investigating two variables at once and how they relate to each other. We will start by comparing overall food consumption per person to the food category. The below, the faceted boxplots show us that the median of each the milk inc. cheese food category is much higher than the others, and that only wheat and wheat products comes in at a close second, with far fewer high outliers. The categories with visually the lowest median consumption rates are lamb & goat, soybeans, and nuts inc. peanut butter. Looking at both the boxplots and the numerical summaries, both eggs and soybeans have relatively consistent rates of consumption across countries.\n\nggplot(fc, aes(y = consumption)) +\n  geom_boxplot() +\n  facet_wrap(~food_category)\n\n\n\n\n\n\nfc |&gt;\n  group_by(food_category) |&gt;\n  summarize(mean(consumption), max(consumption), min(consumption))\n\n# A tibble: 11 × 4\n   food_category       `mean(consumption)` `max(consumption)` `min(consumption)`\n   &lt;chr&gt;                             &lt;dbl&gt;              &lt;dbl&gt;              &lt;dbl&gt;\n 1 Beef                             12.1                 55.5               0.78\n 2 Eggs                              8.16                19.2               0.16\n 3 Fish                             17.3                180.                0.24\n 4 Lamb & Goat                       2.60                21.1               0   \n 5 Milk - inc. cheese              126.                 431.                3.04\n 6 Nuts inc. Peanut B…               4.14                23.0               0.18\n 7 Pork                             16.1                 67.1               0   \n 8 Poultry                          21.2                 62.5               0.47\n 9 Rice                             29.4                172.                0.95\n10 Soybeans                          0.861               17.0               0   \n11 Wheat and Wheat Pr…              71.5                198.                2.74\n\n\nA second bivariate visualization will explore the relationship between overall food consumption per person and the country variable. This chart shows the total kg/person/year consumed of all food categories combined. I have created a bar chart that arranges the countries on the y axis by the total consumption per year per person. We can see that Finland has the highest consumption total, followed by Lithuania and Sweden. The countries with the lowest total consumption per person per year are Zambia, Malawi, and Rwanda, with people only reporting to eat about 5-7 kg of food per year, at least of the categories of food listed here.\n\nfc |&gt;\n  group_by(country) |&gt;\n  summarize(total_consumption = sum(consumption)) |&gt;\n  arrange(desc(total_consumption)) |&gt;\n  ggplot( aes(y = fct_reorder(country, total_consumption, .desc = FALSE), x = total_consumption)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\nit is time to answer the main question of this data visualization process! We will use a heat map to visualize countries on the y axis, food catgories on the x axis, and a colored box to show the numerical variable in each box.\n\n# Construct heatmap\n\nlibrary (gplots)\n\n\nAttaching package: 'gplots'\n\n\nThe following object is masked from 'package:stats':\n\n    lowess\n\n#| fig-height: 30\n#| fig-width: 10\n\nfc_wide &lt;- fc |&gt;\n  mutate(food_category = gsub(\" \", \"_\", food_category)) |&gt;\n  mutate(food_category = gsub(\"&\", \"and\", food_category)) |&gt;\n  pivot_wider(names_from = \"food_category\", values_from = \"consumption\")\n\n\nfc_wide &lt;- fc_wide |&gt;\n  group_by(country) |&gt;\n  summarize(Poultry = median(Poultry, na.rm = TRUE),\n            Pork = median(Pork, na.rm = TRUE),\n            Beef = median(Beef, na.rm = TRUE),\n            Lamb_and_Goat = median(Lamb_and_Goat, na.rm = TRUE),\n            Fish = median(Fish, na.rm = TRUE),\n            Eggs = median(Eggs, na.rm = TRUE),\n            `Milk_-_inc._cheese` = median(`Milk_-_inc._cheese`, na.rm = TRUE),\n            Wheat_and_Wheat_Products = median(Wheat_and_Wheat_Products, na.rm = TRUE),\n            Rice = median(Rice, na.rm = TRUE),\n            Soybeans = median(Soybeans, na.rm = TRUE),\n            `Nuts_-_inc._pb` = median(`Nuts_inc._Peanut_Butter`, na.rm = TRUE))\n\n\nplot_data &lt;- fc_wide |&gt; \n  column_to_rownames(\"country\") |&gt; \n  data.matrix()\n\nhead(plot_data) # This shows us that we have successfully restructured the data so that it is ready to be put in a heatmap\n\n          Poultry  Pork  Beef Lamb_and_Goat  Fish  Eggs Milk_-_inc._cheese\nAlbania     13.23 10.88 22.50         15.32  3.85 12.45             303.72\nAlgeria      7.42  0.00  5.60          7.69  3.74  8.06             141.53\nAngola      17.33  8.89  8.42          1.08 15.24  1.11              12.30\nArgentina   38.66 10.51 55.48          1.56  4.36 11.39             195.08\nArmenia     13.35  9.67 19.66          3.02  4.36 11.69             209.03\nAustralia   46.12 24.14 33.86          9.87 17.69  8.51             234.49\n          Wheat_and_Wheat_Products  Rice Soybeans Nuts_-_inc._pb\nAlbania                     138.64  7.78     0.00           4.36\nAlgeria                     185.42  2.97     0.00           2.08\nAngola                       40.72  8.12     0.52           2.26\nArgentina                   103.11  8.77     0.00           0.49\nArmenia                     130.60  3.18     0.00           2.55\nAustralia                    70.46 11.03     0.19           8.73\n\n\nBelow is that actual code answer the main question addressed in this exploration of data. How does food consumption vary across countries and food categories? This heatmap easily allows us to see some outliers in the trends, which are noticeable by the bright yellow squares on the heatmap. For example, the United Arab Emirates has a relatively high per person annual consumption of nuts and peanut butter, as do Hong Kong and the Maldives, among others. The brightest sections (highest consumption value) on the map is for fish consumption in the Maldives. I’ve added another section of code that lets us see that the actual number per person per year of fish consumed in the Maldives is 179.71 kg. Another very bright spot on the chart, which is an outlier, is the intersection of the soybeans category and Taiwan, showing that those in Taiwan eat a relatively large number of soybeans on average. Another interesting trend that we can see is that most of the European countries eat less rice than the asian countries. This separation occurs as a result of the clustering that the heatmap function allows us to do. Asian countries such as Vietnam, Bangladesh, and Cambodia eat more rice on average per person than countries like Luxembourg, France, and Norway.\n\nheatmap.2(plot_data,\n  dendrogram = \"none\",\n  Rowv = TRUE,\n  scale = \"column\",\n  keysize = 0.7,\n  density.info = \"none\",\n  col = hcl.colors(256),\n  margins = c(10, 10),\n  cexRow = 1, cexCol = 1,\n  sepcolor = \"white\", trace = \"none\",\n  par(mar = c(4, 1, 1, 1))\n)\n\n\n\n\n\n\nfc_wide |&gt;\n  filter(country == \"Maldives\") |&gt;\n  select(Fish, country)\n\n# A tibble: 1 × 2\n   Fish country \n  &lt;dbl&gt; &lt;chr&gt;   \n1  180. Maldives",
    "crumbs": [
      "Best Work",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Exam 1 Visualization Process</span>"
    ]
  },
  {
    "objectID": "bw/bw-exam2.html",
    "href": "bw/bw-exam2.html",
    "title": "\n7  Exam 2: Food Consumption 2\n",
    "section": "",
    "text": "First we need to load all packages necessary for the spatial and processing parts of this visualization process.\n\nlibrary(tidytuesdayR)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(forcats)\nlibrary(rnaturalearth)\nlibrary(sf)\n\nLinking to GEOS 3.13.0, GDAL 3.10.1, PROJ 9.5.1; sf_use_s2() is TRUE\n\n\n\ntuesdata &lt;- tt_load('2020-02-18')\n\n---- Compiling #TidyTuesday Information for 2020-02-18 ----\n--- There is 1 file available ---\n\n\n── Downloading files ───────────────────────────────────────────────────────────\n\n  1 of 1: \"food_consumption.csv\"\n\nfc &lt;- as.data.frame(tuesdata$food_consumption)\nstr(fc)\n\n'data.frame':   1430 obs. of  4 variables:\n $ country      : chr  \"Argentina\" \"Argentina\" \"Argentina\" \"Argentina\" ...\n $ food_category: chr  \"Pork\" \"Poultry\" \"Beef\" \"Lamb & Goat\" ...\n $ consumption  : num  10.51 38.66 55.48 1.56 4.36 ...\n $ co2_emmission: num  37.2 41.53 1712 54.63 6.96 ...\n\nhead(fc, 22)\n\n     country            food_category consumption co2_emmission\n1  Argentina                     Pork       10.51         37.20\n2  Argentina                  Poultry       38.66         41.53\n3  Argentina                     Beef       55.48       1712.00\n4  Argentina              Lamb & Goat        1.56         54.63\n5  Argentina                     Fish        4.36          6.96\n6  Argentina                     Eggs       11.39         10.46\n7  Argentina       Milk - inc. cheese      195.08        277.87\n8  Argentina Wheat and Wheat Products      103.11         19.66\n9  Argentina                     Rice        8.77         11.22\n10 Argentina                 Soybeans        0.00          0.00\n11 Argentina  Nuts inc. Peanut Butter        0.49          0.87\n12 Australia                     Pork       24.14         85.44\n13 Australia                  Poultry       46.12         49.54\n14 Australia                     Beef       33.86       1044.85\n15 Australia              Lamb & Goat        9.87        345.65\n16 Australia                     Fish       17.69         28.25\n17 Australia                     Eggs        8.51          7.82\n18 Australia       Milk - inc. cheese      234.49        334.01\n19 Australia Wheat and Wheat Products       70.46         13.44\n20 Australia                     Rice       11.03         14.12\n21 Australia                 Soybeans        0.19          0.09\n22 Australia  Nuts inc. Peanut Butter        8.73         15.45\n\n\n\nfc |&gt;\ndistinct(food_category) # This part of analysis shows that we need to shorten some of these names before we can visualize them.\n\n              food_category\n1                      Pork\n2                   Poultry\n3                      Beef\n4               Lamb & Goat\n5                      Fish\n6                      Eggs\n7        Milk - inc. cheese\n8  Wheat and Wheat Products\n9                      Rice\n10                 Soybeans\n11  Nuts inc. Peanut Butter\n\n\n\nfc$food_category &lt;- fct_recode(fc$food_category,\n             Lamb = \"Lamb & Goat\",\n             Dairy = \"Milk - inc. cheese\",\n             Wheat = \"Wheat and Wheat Products\",\n             Nuts = \"Nuts inc. Peanut Butter\")\n\nfc |&gt;\ndistinct(food_category) #To check that the categories have been successfully changed\n\n   food_category\n1           Pork\n2        Poultry\n3           Beef\n4           Lamb\n5           Fish\n6           Eggs\n7          Dairy\n8          Wheat\n9           Rice\n10      Soybeans\n11          Nuts\n\n\nResearch Question 1: Which countries consume the most food total?\n\nfc_mostfood &lt;- fc |&gt;\n  group_by(country) |&gt;\n  mutate(total_consumption = sum(consumption)) |&gt;\n  summarize(country, mean = mean(total_consumption)) |&gt;\n  unique() |&gt;\n  arrange(desc(mean)) |&gt;\n  head(5)\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\n\n`summarise()` has grouped output by 'country'. You can override using the\n`.groups` argument.\n\nfc_mostfood |&gt;\n  ggplot(aes(x = country, y = mean)) +\n  geom_bar(stat = \"identity\") +\n  labs(y = \"Total Consumption\")\n\n\n\n\n\n\n\nResearch Question 2: Which countries consume the most of each unique food category?\n\nfc_mosteachcat &lt;- fc |&gt;\n  group_by(food_category) |&gt;\n  slice_max(consumption, n = 5)\n\nfc_mosteachcat |&gt;\n  ggplot(aes(x = reorder(as.factor(country), consumption), y = consumption)) +\n  geom_point(stat = \"identity\") +\n  facet_wrap(~food_category, scales = \"free\") +\n  theme(axis.text.x = element_text(angle = 90)) +\n  labs(x = \"Country\")\n\n\n\n\n\n\n\nResearch Question 3: What does the consumption of each food look like? Visualization Try 1\nThere are some names in this data set that do not match up with the names for those countries in the other data set. Therefore, the next step will be to correct that.\n\nne_countries(returnclass = \"sf\") |&gt;\n  select(name, geometry) |&gt;\n  left_join(fc |&gt; select(-co2_emmission),\n            join_by(name == country)) |&gt;\n  ggplot() +\n  geom_sf(aes(fill = consumption)) +\n  facet_wrap(~food_category) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nQuestion 3, Visualization Try 2\nThis graph does a better job showing the country boundaries even for countries that we do not have consumption data for. It also has improved recognition of all country names between the data set. However, one problem remains, that milk still proportionally dominates the color representation of all food categories, and stops other food categories from being seen.\n\nne_countries(returnclass = \"sf\") |&gt;\n  select(name, geometry) |&gt;\n  mutate(name = ifelse(name == \"United States of America\", \"USA\", name)) |&gt;\n  mutate(name = ifelse(name == \"Bosnia and Herz.\", \"Bosnia and Herzegovina\", name)) |&gt;\n  mutate(name = ifelse(name == \"Czechia\", \"Czech Republic\", name)) |&gt;\n  mutate(name = ifelse(name == \"Taiwan\", \"Taiwan. ROC\", name)) |&gt;\n  left_join(fc |&gt; select(-co2_emmission),\n            join_by(name == country)) |&gt;\n  pivot_wider(names_from = food_category,\n              values_from = consumption) |&gt;\n  select(-\"NA\") |&gt;\n  pivot_longer(cols = c(-name, -geometry),\n               names_to = \"food_category\",\n               values_to = \"consumption\") |&gt;\n  ggplot() +\n  geom_sf(aes(fill = consumption)) +\n  facet_wrap(~food_category) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nQuestion 3, Visualization Try 3\nIn this visualization, we have modified the color scale to make sure the hotspots of each food category show up.\n\nne_countries(returnclass = \"sf\") |&gt;\n  select(name, geometry) |&gt;\n  mutate(name = ifelse(name == \"United States of America\", \"USA\", name)) |&gt;\n  mutate(name = ifelse(name == \"Bosnia and Herz.\", \"Bosnia and Herzegovina\", name)) |&gt;\n  mutate(name = ifelse(name == \"Czechia\", \"Czech Republic\", name)) |&gt;\n  mutate(name = ifelse(name == \"Taiwan\", \"Taiwan. ROC\", name)) |&gt;\n  left_join(\n    fc |&gt;\n      select(-co2_emmission) |&gt;\n      group_by(food_category) |&gt;\n      mutate(consumption = (consumption - mean(consumption))/sd(consumption)),\n      join_by(name == country)) |&gt;\n  pivot_wider(names_from = food_category,\n              values_from = consumption) |&gt;\n  select(-\"NA\") |&gt;\n  pivot_longer(cols = c(-name, -geometry),\n               names_to = \"food_category\",\n               values_to = \"consumption\") |&gt;\n  ggplot() +\n  geom_sf(aes(fill = consumption)) +\n  facet_wrap(~food_category) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nLooking at the above visualization, we can make some statements to answer our third primary research question. Beef and poultry are mostly eaten in North and South America, lots of wheat is eaten in Russia and North Africa, and rice consumption is concentrated mostly in East Asian countries. We can see some general trends that are particularly apparent in larger countries in the above maps, but one problem is that the small scale of each map keeps us from seeing small countries’ data points. One way we could overcome this in subsequent visualizations is to focus on only one food category so that the map visualization for that category is much bigger, and we can see how the outlines of smaller countries are filled in.",
    "crumbs": [
      "Best Work",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exam 2: Food Consumption 2</span>"
    ]
  },
  {
    "objectID": "bw/bw-SoloProject.html",
    "href": "bw/bw-SoloProject.html",
    "title": "\n8  Solo Project: Libraries in Scotland Local Authority Areas\n",
    "section": "",
    "text": "8.1 Process and Setup\nNotes and Steps to follow:\nlibrary(gplots)\n\n\nAttaching package: 'gplots'\n\n\nThe following object is masked from 'package:stats':\n\n    lowess\n\nlibrary(ggplot2)\nlibrary(rnaturalearth)\nlibrary(sf)\n\nLinking to GEOS 3.13.0, GDAL 3.10.1, PROJ 9.5.1; sf_use_s2() is TRUE\n\nlibrary(sfd)\n\nLoading required package: tibble\n\nlibrary(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(forcats)\nlibrary(ggthemes)\nlibrary_data &lt;- read_csv(\"LibrariesScotland.csv\", show_col_types = FALSE)\nboundaries &lt;- read_sf(\"./scotland/pub_las.shp\")\n#We are interested in the number of libraries in each Local Authority in Scotland, so we will create a count of the number of libraries in each county first\nclean_libraries &lt;- library_data |&gt;\n  group_by(local_authority) |&gt;\n  summarize(number_libraries = n()) |&gt;\n  left_join(boundaries, join_by(local_authority == local_auth))",
    "crumbs": [
      "Best Work",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Solo Project: Libraries in Scotland Local Authority Areas</span>"
    ]
  },
  {
    "objectID": "bw/bw-SoloProject.html#process-and-setup",
    "href": "bw/bw-SoloProject.html#process-and-setup",
    "title": "\n8  Solo Project: Libraries in Scotland Local Authority Areas\n",
    "section": "",
    "text": "Natural earth package to find state boundaries or boundaries within a non-US country, or search internet for “__ shapefile” of a specific area of interest.\nFind dataset that corresponds with country or state that will be used to color within boundaries.\nUse sf package to draw boundaries in a plot.\nConstruct a chloropleth map of the variable of interest for the data set on the boundaries applied.",
    "crumbs": [
      "Best Work",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Solo Project: Libraries in Scotland Local Authority Areas</span>"
    ]
  },
  {
    "objectID": "bw/bw-SoloProject.html#making-the-visualization",
    "href": "bw/bw-SoloProject.html#making-the-visualization",
    "title": "\n8  Solo Project: Libraries in Scotland Local Authority Areas\n",
    "section": "\n8.2 Making the Visualization",
    "text": "8.2 Making the Visualization\n\nggplot() +\n  geom_sf(data = clean_libraries,\n          aes(geometry = geometry,\n              fill = number_libraries),\n              color = NA) +\n  coord_sf(crs = \"+proj=merc\") +\n  theme_map() +\n  labs(fill = \"Number of Libraries\",\n       title = \"Number of Libraries in Different Scottish Local Authorities\",\n       subtitle = \"Data collected in 2021\",\n       alt = \"A chloropleth map that uses color coding of each local authority in Scotland to show how many libraries are in each. There are the darkest colors, indicating the highest number of libraries in northern Scotland (about 40 in two local authorities to the north), with fewer total libraries found on the surrounding islands that make up Scotland. There is a medium density of libraries (about 20) in the furthest south local authority of Scotland.\") +\n  scale_fill_continuous(high = \"#132B43\", low = \"#56B1F7\") +\n  theme(legend.position=\"right\")",
    "crumbs": [
      "Best Work",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Solo Project: Libraries in Scotland Local Authority Areas</span>"
    ]
  },
  {
    "objectID": "bw/bw-SoloProject.html#short-storysummary-of-visualization",
    "href": "bw/bw-SoloProject.html#short-storysummary-of-visualization",
    "title": "\n8  Solo Project: Libraries in Scotland Local Authority Areas\n",
    "section": "\n8.3 Short Story/Summary of Visualization",
    "text": "8.3 Short Story/Summary of Visualization\nIn the above visualization of the number of libraries in different Scottish local authorities, there are the highest number of libraries in northern Scotland (about 40 in two local authorities to the north), with fewer total libraries found on the surrounding islands that make up Scotland. There is a medium density of libraries (about 20) in the furthest south local authority of Scotland. It is important to note that these counts are just based on sheer numbers of libraries, and not the number of people or area of land in each local authority. Thus, there may be a bias for large local authorities and/or those with large populations to have more libraries. An interesting area for further research would be to divide the number of libraries by the local population of each local authority, and color the map by the per-capita number of libraries in each local authority.",
    "crumbs": [
      "Best Work",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Solo Project: Libraries in Scotland Local Authority Areas</span>"
    ]
  },
  {
    "objectID": "bw/bw-SoloProject.html#data-sources",
    "href": "bw/bw-SoloProject.html#data-sources",
    "title": "\n8  Solo Project: Libraries in Scotland Local Authority Areas\n",
    "section": "\n8.4 Data Sources",
    "text": "8.4 Data Sources\nLibrary location and number data is from Improvement Service, and boundary data is from Scottish Government published data.\nShapefile for Scotland local authorities: https://www.spatialdata.gov.scot/geonetwork/srv/api/records/1cd57ea6-8d6e-412b-a9dd-d1c89a80ad62 (Under API pub-las section, download the shape-zip ZIP file)\nCSV file for libraries in Scotland: https://data.spatialhub.scot/dataset/libraries-is/resource/570de9e0-999e-4bfd-9ed6-71b09545965e (Download button, and select CSV file, only using stationary libraries data and not “mobile libraries” data)",
    "crumbs": [
      "Best Work",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Solo Project: Libraries in Scotland Local Authority Areas</span>"
    ]
  },
  {
    "objectID": "ica/ica-uni.html",
    "href": "ica/ica-uni.html",
    "title": "\n9  Univariate Viz\n",
    "section": "",
    "text": "9.1 Exercises\n# Import data\nhikes &lt;- read.csv(\"https://mac-stat.github.io/data/high_peaks.csv\")",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Univariate Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-uni.html#exercises",
    "href": "ica/ica-uni.html#exercises",
    "title": "\n9  Univariate Viz\n",
    "section": "",
    "text": "Exercise 1: Research Questions\nLet’s dig into the hikes data, starting with the elevation and difficulty ratings of the hikes:\n\nhead(hikes)\n\n             peak elevation difficulty ascent length time    rating\n1     Mt. Marcy        5344          5   3166   14.8 10.0  moderate\n2 Algonquin Peak       5114          5   2936    9.6  9.0  moderate\n3   Mt. Haystack       4960          7   3570   17.8 12.0 difficult\n4   Mt. Skylight       4926          7   4265   17.9 15.0 difficult\n5 Whiteface Mtn.       4867          4   2535   10.4  8.5      easy\n6       Dix Mtn.       4857          5   2800   13.2 10.0  moderate\n\n\n\nWhat features would we like a visualization of the categorical difficulty rating variable to capture?\n\n\nIt should be in numerical order from lowest to highest difficulty, and the numbers of hikes of different difficulties should be shown together as categories.\n\n\nWhat about a visualization of the quantitative elevation variable?\n\n\nThe peaks should be shown on a number line or in numerical order based on their elevation, from lowest to highest.\nExercise 2: Load tidyverse\nWe’ll address the above questions using ggplot tools. Try running the following chunk and simply take note of the error message – this is one you’ll get a lot!\n\n# Use the ggplot function\nggplot(hikes, aes(x = rating))\n\n\n\n\n\n\n\nIn order to use ggplot tools, we have to first load the tidyverse package in which they live. We’ve installed the package but we need to tell R when we want to use it. Run the chunk below to load the library. You’ll need to do this within any .qmd file that uses ggplot().\n\n# Load the package\nlibrary(tidyverse)\n\nExercise 3: Bar Chart of Ratings - Part 1\nConsider some specific research questions about the difficulty rating of the hikes:\n\nHow many hikes fall into each category?\n\n\n# Shows the number of each discrete value that occurs in the variable difficulty, from the dataset hikes.\ntable(hikes$difficulty)\n\n\n 2  3  4  5  6  7 \n 1  1  9 14 13  8 \n\n\nThere is one hike of difficulty 3, 1 hike of difficulty 2, 9 hikes of rating difficulty rating 4, 14 hikes with difficulty 5, 13 sets with difficulty 13, and 8 hikes with difficulty 7.\n\nAre the hikes evenly distributed among these categories, or are some more common than others? Some ratings of hikes are more common than others. Rating 5 is the most common. Low rated hikes are less common.\n\nAll of these questions can be answered with: (1) a bar chart; of (2) the categorical data recorded in the rating column. First, set up the plotting frame:\n\nggplot(hikes, aes(x = rating))\n\n\n\n\n\n\n\nThink about:\n\nWhat did this do? What do you observe? A blank chart shows up\nWhat, in general, is the first argument of the ggplot() function? Describes the dataset that the data should come from.\nWhat is the purpose of writing x = rating? It labels the x axis\nWhat do you think aes stands for?!? Aesthetics of the chart\nExercise 4: Bar Chart of Ratings - Part 2\nNow let’s add a geometric layer to the frame / canvas, and start customizing the plot’s theme. To this end, try each chunk below, one by one. In each chunk, make a comment about how both the code and the corresponding plot both changed.\nNOTE:\n\nPay attention to the general code properties and structure, not memorization.\nNot all of these are “good” plots. We’re just exploring ggplot.\n\n\n# COMMENT on the change in the code and the corresponding change in the plot\nggplot(hikes, aes(x = rating)) +\n  geom_bar()\n\n\n\n\n\n\n# The ratings of each chart are shown, but not in the order we might expect. This code adds bars to the graph of the number of occurences of a variable in the designated column.\n\n\n# COMMENT on the change in the code and the corresponding change in the plot\nggplot(hikes, aes(x = rating)) +\n  geom_bar() +\n  labs(x = \"Rating\", y = \"Number of hikes\")\n\n\n\n\n\n\n# The \"labs()\" function allows us to designate and write what we want the x and y axis of the chart to be labeled.\n\n\n# COMMENT on the change in the code and the corresponding change in the plot\nggplot(hikes, aes(x = rating)) +\n  geom_bar(fill = \"blue\") +\n  labs(x = \"Rating\", y = \"Number of hikes\")\n\n\n\n\n\n\n# Adding the 'fill = \"blue\"' argument to the geom_bar function changes the color of the bars in the chart to a bright blue.\n\n\n# COMMENT on the change in the code and the corresponding change in the plot\nggplot(hikes, aes(x = rating)) +\n  geom_bar(color = \"orange\", fill = \"blue\") +\n  labs(x = \"Rating\", y = \"Number of hikes\")\n\n\n\n\n\n\n# While the inside of the bars remains bright blue, the \"color = 'orange'\" command creates an orange outline around each of the bars in the chart.\n\n\n# COMMENT on the change in the code and the corresponding change in the plot\nggplot(hikes, aes(x = rating)) +\n  geom_bar(color = \"orange\", fill = \"blue\")  +\n  labs(x = \"Rating\", y = \"Number of hikes\") +\n  theme_minimal()\n\n\n\n\n\n\n# Adding a theme_minimal() function removes the grey background of the chart.\n\nExercise 5: Bar Chart Follow-up\nPart a\nReflect on the ggplot() code.\n\nWhat’s the purpose of the +? When do we use it?\n\nThe + looks like a way to combine functions to each other and add functions from the ggplot code to one another to allow them to work together.\n\nWe added the bars using geom_bar()? Why “geom”?\n\nGeometric lets ggplot know that we want the graph to be numerically proportional in showing values.\n\nWhat does labs() stand for?\n\nLabels, as in labeling the x and y axis.\n\nWhat’s the difference between color and fill?\n\nColor changes the outline of a bar on the chart, and fill changes the whole color inside each bar.\nPart b\nIn general, bar charts allow us to examine the following properties of a categorical variable:\n\nobserved categories: What categories did we observe? Moderate, easy, and difficult.\nvariability between categories: Are observations evenly spread out among the categories, or are some categories more common than others?\nThe numbers of easy and difficult hikes were about the same, but the number of moderate hikes was much higher.\n\nWe must then translate this information into the context of our analysis, here hikes in the Adirondacks. Summarize below what you learned from the bar chart, in context.\n\nThe most common rating of hike was “Moderate” and there were over 25 hikes with that rating. There were around 11 hikes rated easy and 8 labeled difficult.\nPart c\nIs there anything you don’t like about this barplot? For example: check out the x-axis again.\n\nThe ratings are not labeled as expected. It would have been improved if the x-axis order had moderate in the middle.\nExercise 6: Sad Bar Chart\nLet’s now consider some research questions related to the quantitative elevation variable:\n\nAmong the hikes, what’s the range of elevation and how are the hikes distributed within this range (e.g. evenly, in clumps, “normally”)?\nWhat’s a typical elevation?\nAre there any outliers, i.e. hikes that have unusually high or low elevations?\n\nHere:\n\nConstruct a bar chart of the quantitative elevation variable.\nExplain why this might not be an effective visualization for this and other quantitative variables. (What questions does / doesn’t it help answer?)\n\nThis is not a very effective visualization because it doesn’t give us the numbers of each elevation. Although I can see some elevations had two hikes at that elevation, the spread of the y axis is so big that it makes the graph hard to interpret precisely and with the numbers that we want to see. It might be more helpful to separate the distinct numbers into groups and show larger bars that are easier to interpret the real numbers of.\n\nggplot(hikes, aes(x = elevation)) +\n  geom_bar() +\n  theme_minimal()\n\n\n\n\n\n\n\nExercise 7: A Histogram of Elevation\nQuantitative variables require different viz than categorical variables. Especially when there are many possible outcomes of the quantitative variable. It’s typically insufficient to simply count up the number of times we’ve observed a particular outcome as the bar graph did above. It gives us a sense of ranges and typical outcomes, but not a good sense of how the observations are distributed across this range. We’ll explore two methods for graphing quantitative variables: histograms and density plots.\nHistograms are constructed by (1) dividing up the observed range of the variable into ‘bins’ of equal width; and (2) counting up the number of cases that fall into each bin. Check out the example below:\n\nPart a\nLet’s dig into some details.\n\nHow many hikes have an elevation between 4500 and 4700 feet?\n\nAccording to the above histogram, 6 hikes from the hikes dataset have an elevation between 4500 and 4700\n\nHow many total hikes have an elevation of at least 5100 feet?\n\nOnly 2 hikes from the dataset have an elevation of at least 5100 feet\nPart b\nNow the bigger picture. In general, histograms allow us to examine the following properties of a quantitative variable:\n\n\ntypical outcome: Where’s the center of the data points? What’s typical? The most typical data point in this set is a hike between 3900 and 4100 feet.\n\nvariability & range: How spread out are the outcomes? What are the max and min outcomes? The outcomes are not massively spread out, and have a range of about 2000 feet. The minimum elevation of a hike is about 3700 feet and the maximum is about 5500 feet.\n\nshape: How are values distributed along the observed range? Is the distribution symmetric, right-skewed, left-skewed, bi-modal, or uniform (flat)? The distribution is mostly symmetric but slightly right skewed.\n\noutliers: Are there any outliers, i.e. outcomes that are unusually large/small? It doesn’t appear that there are any outliers, as there are no large empty gaps between filled bars on the x axis, showing that all the data points are next to other data points on the designated scale.\n\nWe must then translate this information into the context of our analysis, here hikes in the Adirondacks. Addressing each of the features in the above list, summarize below what you learned from the histogram, in context.\n\nHikes in the Adirondacks range from between 3700 feet to 5500 feet, with the greatest number falling between 3900 and 4100 feet in elevation. Most of the hikes seem to be clustered between 3900 and 4900 feet in elevation, but on either side of that range there are not any outliers, or hikes that have a drastically higher or lower elevation, set apart from the other data points.\nExercise 8: Building Histograms - Part 1\n2-MINUTE CHALLENGE: Thinking of the bar chart code, try to intuit what line you can tack on to the below frame of elevation to add a histogram layer. Don’t forget a +. If it doesn’t come to you within 2 minutes, no problem – all will be revealed in the next exercise.\n\nggplot(hikes, aes(x = elevation)) +\n    geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nExercise 9: Building Histograms - Part 2\nLet’s build some histograms. Try each chunk below, one by one. In each chunk, make a comment about how both the code and the corresponding plot both changed.\n\n# COMMENT on the change in the code and the corresponding change in the plot\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n# Instead of making a bar plot, the addition of `geom_histogram` created a histogram with discrete categories on the x axis.\n\n\n# COMMENT on the change in the code and the corresponding change in the plot\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n# The addition of the argument `color = \"white\"` created white lines around the bars of the histogram, which helps differentiate them from each other and keep them from looking like a blob.\n\n\n# COMMENT on the change in the code and the corresponding change in the plot\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\", fill = \"blue\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n# The addition of the `fill = \"blue\"` argument filled each of the bars with a bright blue color.\n\n\n# COMMENT on the change in the code and the corresponding change in the plot\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\") +\n  labs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n# The addition of the `labs()` function added text to the x and y axis of the histogram\n\n\n# COMMENT on the change in the code and the corresponding change in the plot\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\", binwidth = 1000) +\n  labs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\n\n\n\n\n\n# The addition of the `binwidth = 1000` argument to the geom_histogram function changed the range of each of the bins into which the points are categorized to 1000 instead of 30, as the default text above was telling us.\n\n\n# COMMENT on the change in the code and the corresponding change in the plot\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\", binwidth = 5) +\n  labs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\n\n\n\n\n\n# This change of binwidth from 1000 to 5 made the width of the bins into which the numbers are categorized much narrower.\n\n\n# COMMENT on the change in the code and the corresponding change in the plot\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\", binwidth = 200) +\n  labs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\n\n\n\n\n\n# Switching the binwidth argument to 200 created reasonable sized bins that split the numerical data into categories that each cover 200 distinct values, such as 4500 to 4700, and 4700 to 4900, and so on.\n\nExercise 10: Histogram Follow-up\n\nWhat function added the histogram layer / geometry? geom_histogram\n\nWhat’s the difference between color and fill? color changes the color of the lines on the outside of each of the bars and fill changes the color of the inside blocky part of the bars.\nWhy does adding color = \"white\" improve the visualization? It helps keep the bars from becoming too blocky and running together, especially when there are similar numbers in adjacent bins.\nWhat did binwidth do? Binwidth changes the width of inclusion of each category. Wider bins mean fewer bars across the whole x axis of the histogram.\nWhy does the histogram become ineffective if the binwidth is too big (e.g. 1000 feet)? It is difficult to see the actual trends and distribution. It gets down to being just about as useful as stating the range of the whole dataset, which doesn’t help us see outliers or clusters within the data.\nWhy does the histogram become ineffective if the binwidth is too small (e.g. 5 feet)? There is almost no point in making the bins and the discrete values themselves may as well be shown, because it is harder to make generalizations about the number of data points in each range and is thus harder to see trends in the data.\nExercise 11: Density Plots\nDensity plots are essentially smooth versions of the histogram. Instead of sorting observations into discrete bins, the “density” of observations is calculated across the entire range of outcomes. The greater the number of observations, the greater the density! The density is then scaled so that the area under the density curve always equals 1 and the area under any fraction of the curve represents the fraction of cases that lie in that range.\nCheck out a density plot of elevation. Notice that the y-axis (density) has no contextual interpretation – it’s a relative measure. The higher the density, the more common are elevations in that range.\n\nggplot(hikes, aes(x = elevation)) +\n  geom_density()\n\n\n\n\n\n\n\nQuestions\n\nINTUITION CHECK: Before tweaking the code and thinking back to geom_bar() and geom_histogram(), how do you anticipate the following code will change the plot?\n\nI think that the first line will change the color of the curved line on the plot itself to blue, and the second line will change the area underneath the line, between the line and the x axis, to a bright orange color.\n\ngeom_density(color = \"blue\")\ngeom_density(fill = \"orange\")\n\n\nTRY IT! Test out those lines in the chunk below. Was your intuition correct?\n\n\nggplot(hikes, aes(x = elevation)) +\n  geom_density(color = \"blue\") +\n  geom_density(fill = \"orange\")\n\n\n\n\n\n\n\n\nExamine the density plot. How does it compare to the histogram? What does it tell you about the typical elevation, variability / range in elevations, and shape of the distribution of elevations within this range? The typical, or most common elevation is just over 4000 feet. The elevations are definitely right skewed, although there don’t seem to be any massive outliers. The data is clustered between 4100 and 4500 feet and the number of hikes above this elevation drop rapidly. The density plot also tells us that the range is between about 3700 and 5700 feet.\nExercise 12: Density Plots vs Histograms\nThe histogram and density plot both allow us to visualize the behavior of a quantitative variable: typical outcome, variability / range, shape, and outliers. What are the pros/cons of each? What do you like/not like about each? Density plots do a good job of showing the general trends with a smooth line and make it easier to understand the general split of the data without being too distracted by outliers. Histograms will show outliers in their own discrete categories and allow you to see clusters of data. I imagine histograms would be less useful for detecting outliers if there were a huge number of data points in one bin, compared to 1 in another; a density plot would help smooth out any huge excess in one bin and might help trends be seen in large data sets more easily. I visually prefer histograms, at least for the example of trails in the Adirondacks from this practice set.\nExercise 13: Code = communication\nWe obviously won’t be done until we talk about communication. All code above has a similar general structure (where the details can change):\n\nggplot(___, aes(x = ___)) + \n  geom___(color = \"___\", fill = \"___\") + \n  labs(x = \"___\", y = \"___\")\n\n\nThough not necessary to the code working, it’s common, good practice to indent or tab the lines of code after the first line (counterexample below). Why? This is helpful so that we can see what code functions build on each other and which ones are their own separate functions.\n\n\n# YUCK\nggplot(hikes, aes(x = elevation)) +\ngeom_histogram(color = \"white\", binwidth = 200) +\nlabs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\n\n\n\n\n\n\n\nThough not necessary to the code working, it’s common, good practice to put a line break after each + (counterexample below). Why? If it is all in the same line as in the example below, if gets hard to mentally separate the sections from one another and identify their unique purposes and functions. If you were looking for one in particular to edit, it may be hard ot find.\n\n\n# YUCK \nggplot(hikes, aes(x = elevation)) + geom_histogram(color = \"white\", binwidth = 200) + labs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\n\n\n\n\n\n\nExercise 14: Practice\nPart a\nPractice your viz skills to learn about some of the variables in one of the following datasets from the previous class:\n\n# Data on students in this class\nsurvey &lt;- read.csv(\"https://hash-mac.github.io/stat112site-s25/data/survey.csv\")\n\n# World Cup data\nworld_cup &lt;- read.csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-11-29/worldcups.csv\")\n\nggplot(survey, aes(x = minutes_to_campus)) +\n  geom_histogram(color = \"white\", binwidth = 2) +\n  labs(x = \"Time to campus (min)\", y = \"Number of students\")\n\n\n\n\n\n\nggplot(survey, aes(x = hangout)) +\n  geom_bar(color = \"white\") +\n  labs(x = \"Favorite Hangout Spot\", y = \"Number of students\")\n\n\n\n\n\n\n\nSummary of data from Exercise 14 Part a\n\nI learned about two variables from the data in the class survey, the time it takes for them to get to campus and their ideal hangout spot. I learned that the vast majority of students in the class live within 5 minutes from campus, while over 15 of the students live 0 minutes from campus, presumably on campus itself. Over 20 students in the class have the beach as their ideal hangout spot, and the fewest students, only about 3, said that of the options given, a prairie would be their ideal hangout spot.\nPart b\nCheck out the RStudio Data Visualization cheat sheet to learn more features of ggplot.\n\n\n\n\n\n\nCheck → Commit → Push\n\n\n\nWhen done, don’t forgot to click Render Book and check the resulting HTML files. If happy, jump to GitHub Desktop and commit the changes with the message Finish activity 3 and push to GitHub. Wait few seconds, then visit your portfolio website and make sure the changes are there.",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Univariate Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-bi.html",
    "href": "ica/ica-bi.html",
    "title": "\n10  Bivariate Viz\n",
    "section": "",
    "text": "10.1 Exercises (required)\nGithub user Tony McGovern has compiled and made available 2020/2016/2012 presidential election results for most of 3000+ U.S. counties, except Alaska. (Image: Wikimedia Commons)\nA wrangled version of this data, is imported below, after being combined with:\n# Load data\nelections &lt;- read.csv(\"https://mac-stat.github.io/data/election_2020_county.csv\")\n\n# Check it out\nhead(elections)\n\n  state_name state_abbr historical    county_name county_fips total_votes_20\n1    Alabama         AL        red Autauga County        1001          27770\n2    Alabama         AL        red Baldwin County        1003         109679\n3    Alabama         AL        red Barbour County        1005          10518\n4    Alabama         AL        red    Bibb County        1007           9595\n5    Alabama         AL        red  Blount County        1009          27588\n6    Alabama         AL        red Bullock County        1011           4613\n  repub_pct_20 dem_pct_20 winner_20 total_votes_16 repub_pct_16 dem_pct_16\n1        71.44      27.02     repub          24661        73.44      23.96\n2        76.17      22.41     repub          94090        77.35      19.57\n3        53.45      45.79     repub          10390        52.27      46.66\n4        78.43      20.70     repub           8748        76.97      21.42\n5        89.57       9.57     repub          25384        89.85       8.47\n6        24.84      74.70       dem           4701        24.23      75.09\n  winner_16 total_votes_12 repub_pct_12 dem_pct_12 winner_12 total_population\n1     repub          23909        72.63      26.58     repub            54907\n2     repub          84988        77.39      21.57     repub           187114\n3     repub          11459        48.34      51.25       dem            27321\n4     repub           8391        73.07      26.22     repub            22754\n5     repub          23980        86.49      12.35     repub            57623\n6       dem           5318        23.51      76.31       dem            10746\n  percent_white percent_black percent_asian percent_hispanic per_capita_income\n1            76            18             1                2             24571\n2            83             9             1                4             26766\n3            46            46             0                5             16829\n4            75            22             0                2             17427\n5            88             1             0                8             20730\n6            22            71             0                6             18628\n  median_rent median_age\n1         668       37.5\n2         693       41.5\n3         382       38.3\n4         351       39.4\n5         403       39.6\n6         276       39.6\nWe’ll use this data to explore voting outcomes within the U.S.’s 2-party system. Here’s a list of candidates by year:",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bivariate Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-bi.html#exercises-required",
    "href": "ica/ica-bi.html#exercises-required",
    "title": "\n10  Bivariate Viz\n",
    "section": "",
    "text": "2013 county-level demographics from the df_county_demographics data set from the choroplethr R package\nhistorical voting trends in the state in which the county falls (from https://www.270towin.com/content/blue-and-red-states):\n\nred = consistently Republican\nblue = consistently Democratic\npurple = something in between\n\n\n\n\n\n\n\nyear\nRepublican candidate\nDemocratic candidate\n\n\n\n2020\nDonald Trump\nJoe Biden\n\n\n2016\nDonald Trump\nHillary Clinton\n\n\n2012\nMitt Romney\nBarack Obama\n\n\n\nExercise 0: Review\nPart a\nHow many, or roughly what percent, of the 3000+ counties did the Republican candidate win in 2020?\n\nTake a guess.\n1400\nThen make a plot of the winner variable. (Plot in r chunk below)\nThen discuss what follow-up questions you might have (and that our data might help us answer).\n\n\nggplot(elections, aes(x = winner_20)) +\n      geom_bar()\n\n\n\n\n\n\n\nPart b\nThe repub_pct_20 variable provides more detail about the Republican support in each county. Construct a plot of repub_pct_20.\nNotice that the distribution of Republican support from county to county is slightly left skewed or negatively skewed.\nWhat follow-up questions do you have?\n\nWhat does the variable repub_pct_20 mean? What are the units in which people are counted? Is the y axis by percentage?\n\n\nggplot(elections, aes(x = repub_pct_20)) +\n  geom_histogram(color = \"white\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nExercise 1: Quantitative vs Quantitative Intuition Check\n\n\n\n\n\n\nBe Quick\n\n\n\nDon’t spend more than 3 minutes on this!\n\n\nBelow is a scatterplot of the Republican support in 2020 vs 2016. Notice that:\n\nboth variables are quantitative, and get their own axes\nthe response variable is on the y-axis, demonstrating how repub_pct_20 might be predicted by repub_pct_16, not vice versa\n\nTry to replicate this using ggplot(). THINK:\n\nWhat info do you need to set up the canvas?\nWhat geometric layer (geom_???) might add these dots / points for each county? We haven’t learned this yet, just take some guesses.\n\n\n\nggplot(elections, aes(x = repub_pct_16, y = repub_pct_20)) +\n  geom_point()\n\n\n\n\n\n\n\nExercise 2: 2 Quantitiative Variables\nRun each chunk below to build up a a scatterplot of repub_pct_20 vs repub_pct_16 with different glyphs representing each county. Address or think about any prompts in the comments (#).\n\n# Set up the plotting frame\n# How does this differ than the frame for our histogram of repub_pct_20 alone?\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16))\n\n\n# Add a layer of points for each county\n# Take note of the geom!\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_point()\n\n\n# Change the shape of the points\n# What happens if you change the shape to another number? The shape of each point changes\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_point(shape = 3)\n\n\n# YOU TRY: Modify the code to make the points \"orange\"\n# NOTE: Try to anticipate if \"color\" or \"fill\" will be useful here. Then try both.\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_point(color = \"orange\")\n\n\n\n\n\n\n\n\n# Add a layer that represents each county by the state it's in\n# Take note of the geom and the info it needs to run!\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_text(aes(label = state_abbr))\n\nExercise 3: Reflect\nSummarize the relationship between the Republican support in 2020 and 2016. Be sure to comment on:\n\nthe strength of the relationship (weak/moderate/strong)\nThe relationship is strong between who voted republican in 2016 and who did in 2020.\nthe direction of the relationship (positive/negative)\nThe relationship is positive, meaning that counties that voted republican in 2016 were also more likely to vote republican in 2020\noutliers (in what state do counties deviate from the national trend? Any ideas why this might be the case?)\nIn Texas, more counties voted republican in 2020 than in 2016, which indicates that there may have been some event that happened between these years that changed people’s point of view on issues.\nExercise 4: Visualizing trend\nThe trend of the relationship between repub_pct_20 and repub_pct_16 is clearly positive and (mostly) linear. We can highlight this trend by adding a model “smooth” to the plot:\n\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_point() +\n  geom_smooth()\n\nPart a\nConstruct a new plot that contains the model smooth but does not include the individual point glyphs.\n\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\nPart b\nBy default, geom_smooth() adds a smooth, localized model line. To examine the “best” linear model, we can specify method = \"lm\". It’s pretty similar in this example!\n\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", color = \"orange\")\n# \n\nExercise 5: Your Turn\nTo examine how the 2020 results are related to some county demographics, construct scatterplots of repub_pct_20 vs median_rent, and repub_pct_20 vs median_age. Summarize the relationship between these two variables and comment on which is the better predictor of repub_pct_20, median_rent or median_age.\n\n# Scatterplot of repub_pct_20 vs median_rent\nggplot(elections, aes(y = repub_pct_20, x = median_rent)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", color = \"orange\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n# Scatterplot of repub_pct_20 vs median_age\n\nggplot(elections, aes(y = repub_pct_20, x = median_age)) +\n  geom_point() +\n  geom_smooth(color = \"blue\") +\n  geom_smooth(method = \"lm\", color = \"orange\")\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nExercise 6: A Sad Scatterplot\nNext, let’s explore the relationship between a county’s 2020 Republican support repub_pct_20 and the historical political trends in its state. In this case repub_pct_20 is quantitative, but historical is categorical. Explain why a scatterplot might not be an effective visualization for exploring this relationship. (What questions does / doesn’t it help answer?)\n\nA scatterplot is not a good solution for making this visualization because the historical variable is not able to be shown on a numbered axis. Categorical variables need to be shown using colors or symbols. I think a bar chart would be the best way to show the historical trends in correlation with the repub_pct_20 variable.\n\n\nggplot(elections, aes(y = repub_pct_20, x = historical)) +\n  geom_point()\n\n\n\n\n\n\n\nExercise 7: Quantitative vs Categorical – Violins & Boxes\nThough the above scatterplot did group the counties by historical category, it’s nearly impossible to pick out meaningful patterns in 2020 Republican support in each category. Let’s try adding 2 different geom layers to the frame:\n\n# Side-by-side violin plots\nggplot(elections, aes(y = repub_pct_20, x = historical)) +\n  geom_violin()\n\n\n# Side-by-side boxplots (defined below)\nggplot(elections, aes(y = repub_pct_20, x = historical)) +\n  geom_boxplot()\n\nBox plots are constructed from five numbers - the minimum, 25th percentile, median, 75th percentile, and maximum value of a quantitative variable:\n\nREFLECT:\nSummarize what you’ve learned about the 2020 Republican county-level support within and between red/purple/blue states.\n\nI have learned that counties that historically voted Republican were more likely to vote republican in 2020, but there were more negative outliers, counties that did not have high republican votes in 2020 but did have a strong history of voting republican in the past.\nExercise 8: Quantitative vs Categorical – Intuition Check\n\n\n\n\n\n\nBe Quick\n\n\n\nDon’t spend more than 3 minutes on this!\n\n\nWe can also visualize the relationship between repub_pct_20 and historical using our familiar density plots. In the plot below, notice that we simply created a separate density plot for each historical category. (The plot itself is “bad” but we’ll fix it below.) Try to adjust the code chunk below, which starts with a density plot of repub_pct_20 alone, to re-create this image.\n\n\nggplot(elections, aes(x = repub_pct_20, fill = historical)) +\n  geom_density()\n\n\n\n\n\n\n\nExercise 9: Quantitative vs Categorical – Density Plots\nWork through the chunks below and address the comments therein.\n\n# Name two \"bad\" things about this plot\n# 1 I cannot see the overlapped sections. I think there may be a background to the \"blue\" `historical` tagged section but I cannot see it.\n# 2 The key says that the fill of blue is pink, which is not intuitive at all.\nggplot(elections, aes(x = repub_pct_20, fill = historical)) +\n  geom_density()\n\n\n# What does scale_fill_manual do?\n# Changes the color so that blue is blue, purple is purple, and red is red.\nggplot(elections, aes(x = repub_pct_20, fill = historical)) +\n  geom_density() +\n  scale_fill_manual(values = c(\"blue\", \"purple\", \"red\"))\n\n\n# What does alpha = 0.5 do?\n# Alpha changes the transparency of each of the filled in density plots.\n# Play around with different values of alpha, between 0 and 1\nggplot(elections, aes(x = repub_pct_20, fill = historical)) +\n  geom_density(alpha = 0.5) +\n  scale_fill_manual(values = c(\"blue\", \"purple\", \"red\"))\n\n\n# What does facet_wrap do?!\n# facet_wrap creates a whole new plot for each historical categorization\nggplot(elections, aes(x = repub_pct_20, fill = historical)) +\n  geom_density() +\n  scale_fill_manual(values = c(\"blue\", \"purple\", \"red\")) +\n  facet_wrap(~ historical)\n\n\n# Let's try a similar grouping strategy with a histogram instead of density plot.\n# Why is this terrible?\n# I can't compare the numbers of blue and purple votes because they are all displaced already by red vote numbers\nggplot(elections, aes(x = repub_pct_20, fill = historical)) +\n  geom_histogram(color = \"white\") +\n  scale_fill_manual(values = c(\"blue\", \"purple\", \"red\"))\n\nExercise 10\nWe’ve now learned 3 (of many) ways to visualize the relationship between a quantitative and categorical variable: side-by-side violins, boxplots, and density plots.\n\nWhich do you like best?\nI like transparent density plots the best.\nWhat is one pro of density plots relative to boxplots?\nThey are more visually appealing and show trends very intuitively.\nWhat is one con of density plots relative to boxplots?\nThey do not give you very precise numbers or statistics about the data, you can’t see the median or percentile ranges very intuitively, you are just guessing on those.\nExercise 11: Categorical vs Categorical – Intuition Check\nFinally, let’s simply explore who won each county in 2020 (winner_20) and how this breaks down by historical voting trends in the state. That is, let’s explore the relationship between 2 categorical variables! Following the same themes as above, we can utilize grouping features such as fill/color or facets to distinguish between different categories of winner_20 and historical.\n\n\n\n\n\n\nBe Quick\n\n\n\nSpend at most 5 minutes on the following intuition check. Adjust the code below to recreate the following two plots.\n\n\n\n\n# Plot 1: adjust this to recreate the top plot\nggplot(elections, aes(x = historical, fill = winner_20)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n# Plot 2: adjust this to recreate the bottom plot\nggplot(elections, aes(x = winner_20)) +\n  geom_bar() + \n  facet_wrap(~ historical)\n\n\n\n\n\n\n\nExercise 12: Categorical vs Categorical\nConstruct the following 4 bar plot visualizations.\n\n# A stacked bar plot\n# How are the \"historical\" and \"winner_20\" variables mapped to the plot, i.e. what roles do they play?\n# \"historical\" is shown on the x axis and is the independent variable, so we are interested in how it affects the winner in 2020, which is shown by colors on the bars, and is the independent variable.\nggplot(elections, aes(x = historical, fill = winner_20)) +\n  geom_bar()\n\n\n# A faceted bar plot\nggplot(elections, aes(x = winner_20)) +\n  geom_bar() +\n  facet_wrap(~ historical)\n\n\n# A side-by-side bar plot\n# Note the new argument to geom_bar\nggplot(elections, aes(x = historical, fill = winner_20)) +\n  geom_bar(position = \"dodge\")\n\n\n# A proportional bar plot\n# Note the new argument to geom_bar\nggplot(elections, aes(x = historical, fill = winner_20)) +\n  geom_bar(position = \"fill\")\n\nPart a\nName one pro and one con of using the “proportional bar plot” instead of one of the other three options.\n\nA pro is that you can compare easily the percentages and proportions of different categories between historical trends in this case. A con is that you don’t get any idea of the actual number of counties, and how big the historical data sets are compared to each other.\nPart b\nWhat’s your favorite bar plot from part and why?\n\nI like the “dodge” bar graph because it lets me see democrat versus republican votes in 2020 very easily among counties with the same historical background.\nExercise 13: Practice (now or later)\n\n\n\n\n\n\nDecide\n\n\n\nDecide what’s best for you:\n\nTry this extra practice now.\nReflect on the above exercises and come back to this extra practice later (but before the next class).\n\n\n\nImport some daily weather data from a few locations in Australia:\n\nweather &lt;- read.csv(\"https://mac-stat.github.io/data/weather_3_locations.csv\")\n\nConstruct plots that address the research questions in each chunk. You might make multiple plots–there are many ways to do things!. However, don’t just throw spaghetti at the wall.\nReflect before doing anything. What types of variables are these? How might you plot just 1 of the variables, and then tweak the plot to incorporate the other?\n\n# How do 3pm temperatures (temp3pm) differ by location?\nggplot(weather, aes(y = temp3pm, x = location)) +\n  geom_boxplot()\n\nWarning: Removed 19 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n# Temperatures in Uluru are the highest at 3pm compared to the other places, where temperatures at 3p are similar to each other.\n\n\n# How might we predict the 3pm temperature (temp3pm) by the 9am temperature (temp9am)?\nggplot(weather, aes(y = temp3pm, x = temp9am)) +\n  geom_point()\n\nWarning: Removed 27 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n# Making a scatter plot where the temperature at 9 am (the independent variable) is on the x axis and the dependent variable of temp at 3 pm is on the y axis.\n\n\n# How do the number of rainy days (raintoday) differ by location?\nggplot(weather, aes(x = location, fill = raintoday)) +\n  geom_bar(position = \"dodge\")",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bivariate Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-bi.html#exercises-optional",
    "href": "ica/ica-bi.html#exercises-optional",
    "title": "\n10  Bivariate Viz\n",
    "section": "\n10.2 Exercises (optional)",
    "text": "10.2 Exercises (optional)\nThe above visualizations are foundational and important. But they’re not the only way to visualize the variables in our dataset.\nOptional Exercise 1: Many Categories\nSuppose we wanted to better understand how the 2020 Republican support varied from county to county within each state. Since repub_pct_20 is quantitative and state_abbr is categorical, we could make a density plot of repub_pct_20 for each state. Reflect on why this is bad.\n\nggplot(elections, aes(x = repub_pct_20, fill = state_abbr)) + \n  geom_density(alpha = 0.5)\n\nWarning: Groups with fewer than two data points have been dropped.\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nA facet wrap would also be bad!\n\nggplot(elections, aes(x = repub_pct_20)) + \n  geom_density(alpha = 0.5) + \n  facet_wrap(~ state_abbr)\n\nWarning: Groups with fewer than two data points have been dropped.\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nWhen we want to compare the distribution of some quantitative outcome among many groups / categories, a ridgeline plot can be a good option. These are also called joy plots, named after the album cover for “Unknown Pleasures” by Joy Division. (Look it up!) To make a ridgeline plot, we can use the geom_density_ridges() function from the ggridges package.\n\n# Install ggridges package\nlibrary(ggridges)\n\n# Make our first joy plot\n# THINK: What DON'T you like about this?\nggplot(elections, aes(x = repub_pct_20, y = state_abbr)) + \n  geom_density_ridges()\n\n\n# Let's put the states in order by Republican support, not alphabet\n# How do you think fct_reorder works? We'll learn about this later in the semester.\nggplot(elections, aes(x = repub_pct_20, y = fct_reorder(state_abbr, repub_pct_20))) + \n  geom_density_ridges(alpha = 0.5)\n\n\n# YOUR TURN: color/fill the ridges according to a state's historical voting patterns \n# and add meaningful axis labels\n\nFollow-up questions\n\nWhich states tend to have the most variability in outcomes from county to county? The least?\nWhat other interesting patterns do you notice?\nDoes this plot prompt any other questions?\nOptional Exercise 2: Total Outcomes by State\nLet’s import some new data and counts up the total votes (Republican and Democratic) by state, not county. This was wrangled from the elections data!\n\nelections_by_state &lt;- read.csv(\"https://mac-stat.github.io/data/election_2020_by_state.csv\")\n\nFor example, we might make a scatterplot of the 2020 vs 2016 outcomes:\n\nggplot(elections_by_state, aes(y = repub_pct_20, x = repub_pct_16)) + \n  geom_point()\n\n\n\n\n\n\n\nBUT this isn’t the easiest way to communicate or identify the changes from 1 year to the next.\n\n# YOU TRY\n# Start by creating a \"scatterplot\" of state_abbr (y-axis) by 2020 Republican support on the x-axis\n# Color the points red\n# Scroll to solutions below when you're ready\n\n\n# Check it out\nggplot(elections_by_state, aes(x = repub_pct_20, y = state_abbr)) + \n  geom_point(color = \"red\")\n\n\n\n\n\n\n\n\n# YOU TRY\n# Reorder the states in terms of their 2020 Republican support (not alphabet)\n# Scroll to solutions below when you're ready\n\n\n# Check it out\nlibrary(forcats)\nggplot(elections_by_state, aes(x = repub_pct_20, y = fct_reorder(state_abbr, repub_pct_20))) + \n  geom_point(color = \"red\")\n\n\n\n\n\n\n\n\n# Finally, add ANOTHER layer of points for the 2016 outcomes\n# What info does this new geom_point() layer need to run?\nggplot(elections_by_state, aes(x = repub_pct_20, y = fct_reorder(state_abbr, repub_pct_20))) + \n  geom_point(color = \"red\") + \n  geom_point(aes(x = repub_pct_16, y = state_abbr))\n\n\n\n\n\n\n\nReflect on the following\n\nWhat do you think this plot needs? Try it! You might need to do some digging online.\nSummarize the main takeaways from the plots. Which states changed the most from 2016 to 2020? The least? Where did the Republican support increase? Where did it decrease?\nWhat other questions are you left with?",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bivariate Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-multi.html",
    "href": "ica/ica-multi.html",
    "title": "\n11  Mulivariate Viz\n",
    "section": "",
    "text": "11.1 Exercises (required)",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Mulivariate Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-multi.html#exercises-required",
    "href": "ica/ica-multi.html#exercises-required",
    "title": "\n11  Mulivariate Viz\n",
    "section": "",
    "text": "The story\nThough far from a perfect assessment of academic preparedness, SAT scores have historically been used as one measurement of a state’s education system. The education dataset contains various education variables for each state:\n\n# Import and check out data\neducation &lt;- read.csv(\"https://mac-stat.github.io/data/sat.csv\")\nhead(education)\n\n       State expend ratio salary frac verbal math  sat  fracCat\n1    Alabama  4.405  17.2 31.144    8    491  538 1029   (0,15]\n2     Alaska  8.963  17.6 47.951   47    445  489  934 (45,100]\n3    Arizona  4.778  19.3 32.175   27    448  496  944  (15,45]\n4   Arkansas  4.459  17.1 28.934    6    482  523 1005   (0,15]\n5 California  4.992  24.0 41.078   45    417  485  902  (15,45]\n6   Colorado  5.443  18.4 34.571   29    462  518  980  (15,45]\n\n\nA codebook is provided by Danny Kaplan who also made these data accessible:\n\nExercise 1: SAT scores\nPart a\nConstruct a plot of how the average sat scores vary from state to state. (Just use 1 variable – sat not State!)\n\nggplot(education, aes(x = sat)) +\n  geom_histogram(color = \"white\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nPart b\nSummarize your observations from the plot. Comment on the basics: range, typical outcomes, shape. (Any theories about what might explain this non-normal shape?)\n\nThe range of the data is unusual and seems to be skewed towards both sides, with the middle range of data very empty, meaning people in the data set mostly got either slightly higher than average or lower than average scores, and not many people got average scores. The typical outcome (mean) was just under 900, with 6 students scoring in that range.\nExercise 2: SAT Scores vs Per Pupil Spending & SAT Scores vs Salaries\nThe first question we’d like to answer is: Can the variability in sat scores from state to state be partially explained by how much a state spends on education, specifically its per pupil spending (expend) and typical teacher salary?\nPart a\n\n# Construct a plot of sat vs expend\n# Include a \"best fit linear regression model\" (HINT: method = \"lm\")\nggplot(education, aes(x = expend, y = sat)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n# Construct a plot of sat vs salary\n# Include a \"best fit linear regression model\" (HINT: method = \"lm\")\nggplot(education, aes(x = salary, y = sat)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nPart b\nWhat are the relationship trends between SAT scores and spending? Is there anything that surprises you?\n\nThe relationship between SAT scores and spending is negative, meaning that as spending increases, the average expected SAT score decreases. This surprising, especially because I thought that school districts and states that were spending more on education would be scoring higher on the SAT.\nExercise 3: SAT Scores vs Per Pupil Spending and Teacher Salaries\nConstruct one visualization of the relationship of sat with salary and expend. HINT: Start with just 2 variables and tweak that code to add the third variable. Try out a few things!\n\nggplot(education, aes(x = expend, y = sat, size = salary)) +\n  geom_point()\n\n\n\n\n\n\n\nExercise 4: Another way to Incorporate Scale\nIt can be tough to distinguish color scales and size scales for quantitative variables. Another option is to discretize a quantitative variable, or basically cut it up into categories.\nConstruct the plot below. Check out the code and think about what’s happening here. What happens if you change “2” to “3”?\n\nWhen I change the 2 to a 3, there are now three different trend lines and categories of data created from the expend variable.\n\n\nggplot(education, aes(y = sat, x = salary, color = cut(expend, 2))) + \n  geom_point() + \n  geom_smooth(se = FALSE, method = \"lm\")\n\nggplot(education, aes(y = sat, x = salary, color = cut(expend, 3))) + \n  geom_point() + \n  geom_smooth(se = FALSE, method = \"lm\")\n\nDescribe the trivariate relationship between sat, salary, and expend.\n\nThere is a positive correlation between salary and expend variables, and generally as salary increases, expenditure by the school also increases (as can be seen by most of the upper range color variable of expend being found on the higher end of the salary x-axis). Both of salary and expend have negative correlation with sat, meaning that in general, the higher the expenditure and salary, the lower the SAT score will be.\nExercise 5: Finally an Explanation\nIt’s strange that SAT scores seem to decrease with spending. But we’re leaving out an important variable from our analysis: the fraction of a state’s students that actually take the SAT. The fracCat variable indicates this fraction: low (under 15% take the SAT), medium (15-45% take the SAT), and high (at least 45% take the SAT).\nPart a\nBuild a univariate viz of fracCat to better understand how many states fall into each category.\n\nggplot(education, aes(x = fracCat)) +\n  geom_bar()\n\n\n\n\n\n\n\nPart b\nBuild 2 bivariate visualizations that demonstrate the relationship between sat and fracCat. What story does your graphic tell and why does this make contextual sense?\n\nggplot(education, aes(x = fracCat, y = sat)) +\n  geom_point()\n\n\n\n\n\n\n\nPart c\nMake a trivariate visualization that demonstrates the relationship of sat with expend AND fracCat. Highlight the differences in fracCat groups through color AND unique trend lines. What story does your graphic tell?\nDoes it still seem that SAT scores decrease as spending increases?\n\nggplot(education, aes(x = expend, y = sat, color = fracCat)) +\n  geom_point() +\n geom_smooth(se = FALSE, method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nPart d\nPutting all of this together, explain this example of Simpson’s Paradox. That is, why did it appear that SAT scores decrease as spending increases even though the opposite is true?\n\nThe SAT scores seemed to be decreasing because the sample that gave results was less selective and biased. The smaller sample of students who had lower funding but still took the SAT was biased towards students who studied and did well on the test. In the categories of states where only up to 15% of people took the SAT, there was a slight positive correlation between higher expenditure and better test results. Within each categorization of the fraction of students who took the SAT, there was a positive trend between expenditure and test score.",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Mulivariate Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-multi.html#exercises-optional",
    "href": "ica/ica-multi.html#exercises-optional",
    "title": "\n11  Mulivariate Viz\n",
    "section": "\n11.2 Exercises (optional)",
    "text": "11.2 Exercises (optional)\nExercise 6: Heat Maps\nAs usual, we’ve only just scratched the surface! There are lots of other data viz techniques for exploring multivariate relationships. Let’s start with a heat map.\nPart a\nRun the chunks below. Check out the code, but don’t worry about every little detail! NOTES:\n\nThis is not part of the ggplot() grammar, making it a bit complicated.\nIf you’re curious about what a line in the plot does, comment it out (#) and check out what happens!\nIn the plot, for each state (row), each variable (column) is scaled to indicate whether the state has a relative high value (yellow), a relatively low value (purple), or something in between (blues/greens).\nYou can also play with the color scheme. Type ?cm.colors in the console to learn about various options.\nWe’ll improve the plot later, so don’t spend too much time trying to learn something from this plot.\n\n\n# Load the gplots package needed for heatmaps and the tibble package necessary for heatmaps\nlibrary(gplots)\n\n\nAttaching package: 'gplots'\n\n\nThe following object is masked from 'package:stats':\n\n    lowess\n\nlibrary(tibble)\n\n# Remove the \"State\" column and use it to label the rows\n# Then scale the variables\nplot_data &lt;- education |&gt; \n  column_to_rownames(\"State\") |&gt; \n  data.matrix() |&gt; \n  scale()\n\n# Construct heatmap 1\nheatmap.2(plot_data,\n  dendrogram = \"none\",\n  Rowv = NA, \n  scale = \"column\",\n  keysize = 0.7, \n  density.info = \"none\",\n  col = hcl.colors(256), \n  margins = c(10, 20),\n  colsep = c(1:7), rowsep = (1:50), sepwidth = c(0.05, 0.05),\n  sepcolor = \"white\", trace = \"none\"\n)\n\n\n\n\n\n\n\n\n# Construct heatmap 2\nheatmap.2(plot_data,\n  dendrogram = \"none\",\n  Rowv = TRUE,             ### WE CHANGED THIS FROM NA TO TRUE\n  scale = \"column\",\n  keysize = 0.7, \n  density.info = \"none\",\n  col = hcl.colors(256), \n  margins = c(10, 20),\n  colsep = c(1:7), rowsep = (1:50), sepwidth = c(0.05, 0.05),\n  sepcolor = \"white\", trace = \"none\"\n)\n\n\n\n\n\n\n\n\n# Construct heatmap 3\nheatmap.2(plot_data,\n  dendrogram = \"row\",       ### WE CHANGED THIS FROM \"none\" TO \"row\"\n  Rowv = TRUE,            \n  scale = \"column\",\n  keysize = 0.7, \n  density.info = \"none\",\n  col = hcl.colors(256), \n  margins = c(10, 20),\n  colsep = c(1:7), rowsep = (1:50), sepwidth = c(0.05, 0.05),\n  sepcolor = \"white\", trace = \"none\"\n)\n\n\n\n\n\n\n\nPart b\nIn the final two plots, the states (rows) are rearranged by similarity with respect to these education metrics. The final plot includes a dendrogram which further indicates clusters of similar states. In short, states that have a shorter path to connection are more similar than others.\nPutting this all together, what insight do you gain about the education trends across U.S. states? Which states are similar? In what ways are they similar? Are there any outliers with respect to 1 or more of the education metrics?\n+ Nebraska, Missouri, Kansas, Iowa, North Dakota, and South Dakota all fall pretty close to each other on the dendrogram, due to their similarities in the data collected. These states are also in the same region of the US, which is not a metric that is measured in the data, but interesting that there is still a connection between the states. I liked the dendrogram because it looked like what I am familiar with seeing in biology for evolution. I wonder if R could be used to make evolutionary hypotheses and diagrams with similar data?\n+ Utah and California are both outliers in the ratio of students to teachers that they have, which is made very clear by the bright yellow boxes that they have in this heat map in that category.New York and New Jersey both have similar high levels of expenditure on education. \nExercise 7: Star plots\nLike heat maps, star plots indicate the relative scale of each variable for each state. Thus, we can use star maps to identify similar groups of states, and unusual states!\nPart a\nConstruct and check out the star plot below. Note that each state has a “pie”, with each segment corresponding to a different variable. The larger a segment, the larger that variable’s value is in that state. For example:\n\nCheck out Minnesota. How does Minnesota’s education metrics compare to those in other states? What metrics are relatively high? Relatively low?\n\n+ Minnesota has very high math and verbal and SAT scores, but low frac and fracCat statistics. It has similar proportions of stats to other states in the midwest. \n\nWhat states appear to be similar? Do these observations agree with those that you gained from the heat map?\n\n+ New York and New Jersey appear to have very similar pie charts, as do many of the midwestern states, which is similar to what I saw in the heat maps. \n\nstars(plot_data,\n  flip.labels = FALSE,\n  key.loc = c(10, 1.5),\n  cex = 1, \n  draw.segments = TRUE\n)\n\n\n\n\n\n\n\nPart b\nFinally, let’s plot the state stars by geographic location! What new insight do you gain here?!\n+ The pie charts are so small, but I can see the geographic trend of many states in the east matching the trend I noticed in New Jersey and New York, and many of the midwestern states having similar trends. There is a much wider variety of spending and SAT results in the west, much more variation between states, which this method of visualization makes clearer.\n\nstars(plot_data,\n  flip.labels = FALSE,\n  locations = data.matrix(as.data.frame(state.center)),  # added external data to arrange by geo location\n  key.loc = c(-110, 28),\n  cex = 1, \n  draw.segments = TRUE\n)",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Mulivariate Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-spatial.html",
    "href": "ica/ica-spatial.html",
    "title": "\n12  Spatial Viz\n",
    "section": "",
    "text": "library(leaflet)\n\nUse this file for practice with the spatial viz in-class activity. Refer to the class website for details. ## Exercises\n\n12.0.1 Preview\nYou’ll explore some R spatial viz tools below. In general, there are two important pieces to every map:\nPiece 1: A dataset\nThis dataset must include either:\n\nlocation coordinates for your points of interest (for point maps); or\nvariable outcomes for your regions of interest (for choropleth maps)\n\n\nPiece 2: A background map\nWe need latitude and longitude coordinates to specify the boundaries for your regions of interest (eg: countries, states). This is where it gets really sticky!\n\nCounty-level, state-level, country-level, continent-level info live in multiple places.\nWhere we grab this info can depend upon whether we want to make a point map or a choropleth map. (The background maps can be used somewhat interchangeably, but it requires extra code :/)\nWhere we grab this info can also depend upon the structure of our data and how much data wrangling / cleaning we’re up for. For choropleth maps, the labels of regions in our data must match those in the background map. For example, if our data labels states with their abbreviations (eg: MN) and the background map refers to them as full names in lower case (eg: minnesota), we have to wrangle our data so that it matches the background map.\n\nIn short, the code for spatial viz gets very specialized. The goal of these exercises is to:\n\nplay around and experience the wide variety of spatial viz tools out there\nunderstand the difference between point maps and choropleth maps\nhave fun\n\nYou can skip around as you wish and it’s totally fine if you don’t finish everything. Just come back at some point to play around.\nPart 1: Interactive points on a map with leaflet\n\nLeaflet is an open-source JavaScript library for creating maps. We can use it inside R through the leaflet package.\nThis uses a different plotting framework than ggplot2, but still has a tidyverse feel (which will become more clear as we learn other tidyverse tools!).\nThe general steps are as follows:\n\nCreate a map widget by calling leaflet() and telling it the data to use.\nAdd a base map using addTiles() (the default) or addProviderTiles().\nAdd layers to the map using layer functions (e.g. addMarkers(), addPolygons()).\nPrint the map widget to display it.\nExercise 1: A leaflet with markers / points\nEarlier this semester, I asked for the latitude and longitude of one of your favorite places. I rounded these to the nearest whole number, so that they’re near to but not exactly at those places. Let’s load the data and map it!\n\n\n  latitude longitude\n1       59        18\n2       45       -93\n3       33      -117\n4       40       116\n5       40       106\n6       37      -122\n\n\n\nfave_places &lt;- read.csv(\"https://hash-mac.github.io/stat112site-s25/data/our_fave_places.csv\")\n\n# Check it out\nhead(fave_places)\n\nPart a\nYou can use a “two-finger scroll” to zoom in and out.\n\n# Load the leaflet package\nlibrary(leaflet)\n\n# Just a plotting frame\nleaflet(data = fave_places)\n\n\n\n\n\n\n# Now what do we have?\nleaflet(data = fave_places) |&gt; \n  addTiles()\n\n\n\n\n\n\n# Now what do we have?\n# longitude and latitude refer to the variables in our data\nleaflet(data = fave_places) |&gt; \n  addTiles() |&gt; \n  addMarkers(lng = ~longitude, lat = ~latitude)\n\n\n\n\n\n\n# Since we named them \"longitude\" and \"latitude\", the function\n# automatically recognizes these variables. No need to write them!\nleaflet(data = fave_places) |&gt; \n  addTiles() |&gt; \n  addMarkers()\n\n\n\n\n\nPart b\nPLAY AROUND! This map is interactive. Zoom in on one location. Keep zooming – what level of detail can you get into?\n\n You get to very specific detail in many parts of the world that I tried to zoom in on.\n\nHow does that detail depend upon where you try to zoom in (thus what are the limitations of this tool)?\n\n If you try to zoom in to some very remote parts, you will not get place names or details. You cannot see the imagery or photos of places that you zoom into either, so a limitation of this tool is actually visualizing the places marked. \nExercise 2: Details\nWe can change all sorts of details in leaflet maps.\n\n# Load package needed to change color\nlibrary(gplots)\n\n\nAttaching package: 'gplots'\n\n\nThe following object is masked from 'package:stats':\n\n    lowess\n\n# We can add colored circles instead of markers at each location\nleaflet(data = fave_places) %&gt;%\n  addTiles() |&gt; \n  addCircles(color = col2hex(\"red\"))\n\n\n\n\n\n\n# We can change the background\n# Mark locations with yellow dots\n# And connect the dots, in their order in the dataset, with green lines\n# (These green lines don't mean anything here, but would if this were somebody's travel path!)\nleaflet(data = fave_places) |&gt;\n  addProviderTiles(\"USGS\") |&gt;\n  addCircles(weight = 10, opacity = 1, color = col2hex(\"yellow\")) |&gt;\n  addPolylines(\n    lng = ~longitude,\n    lat = ~latitude,\n    color = col2hex(\"green\")\n  )\n\n\n\n\n\nIn general:\n\naddProviderTiles() changes the base map.\nTo explore all available provider base maps, type providers in the console. (Though some don’t work :/)\n\nUse addMarkers() or addCircles() to mark locations. Type ?addControl into the console to pull up a help file which summarizes the aesthetics of these markers and how you can change them. For example:\n\n\nweight = how thick to make the lines, points, pixels\n\nopacity = transparency (like alpha in ggplot2)\ncolors need to be in “hex” form. We used the col2hex() function from the gplots library to do that\n\n\nExercise 3: Your turn\nThe starbucks data, compiled by Danny Kaplan, contains information about every Starbucks in the world at the time the data were collected, including Latitude and Longitude:\n\n# Import starbucks location data\nstarbucks &lt;- read.csv(\"https://mac-stat.github.io/data/starbucks.csv\")\n\nLet’s focus on only those in Minnesota for now:\n\n# Don't worry about the syntax\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nstarbucks_mn &lt;- starbucks |&gt;\n  filter(Country == \"US\", State.Province == \"MN\")\n\n# Creating the map\nleaflet(data = starbucks_mn) |&gt; \n  addTiles() |&gt; \n  addCircles(color = col2hex(\"darkgreen\"))\n\nAssuming \"Longitude\" and \"Latitude\" are longitude and latitude, respectively\n\n\n\n\n\n\nCreate a leaflet map of the Starbucks locations in Minnesota. Keep it simple – go back to Exercise 1 for an example.\nPart 2: Static points on a map\nLeaflet is very powerful and fun. But:\n\nIt’s not great when we have lots of points to map – it takes lots of time.\nIt makes good interactive maps, but we often need a static map (eg: we can not print interactive maps!).\n\nLet’s explore how to make point maps with ggplot(), not leaflet().\nExercise 3: A simple scatterplot\nLet’s start with the ggplot() tools we already know. Construct a scatterplot of all starbucks locations, not just those in Minnesota, with:\n\nLatitude and Longitude coordinates (which goes on the y-axis?!)\nMake the points transparent (alpha = 0.2) and smaller (size = 0.2)\n\nIt’s pretty cool that the plots we already know can provide some spatial context. But what don’t you like about this plot?\n\n There are no borders to places and I can’t see where the oceans or any other landmasses are. \n\n\nggplot(starbucks, aes(x = Longitude, y = Latitude)) +\n  geom_point(alpha = 0.2, size = 0.2)\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nExercise 4: Adding a country-level background\nLet’s add a background map of country-level boundaries.\nPart a\nFirst, we can grab country-level boundaries from the rnaturalearth package.\n\n# Load the package\nlibrary(rnaturalearth)\n\n# Get info about country boundaries across the world\n# in a \"sf\" or simple feature format\nworld_boundaries &lt;- ne_countries(returnclass = \"sf\")\n\nIn your console, type world_boundaries to check out what’s stored there. Don’t print it our in your Rmd – printing it would be really messy there (even just the head()).\nPart b\nRun the chunks below to build up a new map.\n\n# What does this code produce? A map!\n# What geom are we using for the point map? `geom_sf`\nggplot(world_boundaries) + \n  geom_sf()\n\n\n\n\n\n\n\n\n# Load package needed to change map theme\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\n\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\n\n\nAttaching package: 'mosaic'\n\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\n# Add a point for each Starbucks\n# NOTE: The Starbucks info is in our starbucks data, not world_boundaries\n# How does this change how we use geom_point?! We put the data that we want to show up on the map within the `geom_point` command\nggplot(world_boundaries) + \n  geom_sf() + \n  geom_point(\n    data = starbucks,\n    aes(x = Longitude, y = Latitude),\n    alpha = 0.3, size = 0.2, color = \"darkgreen\"\n  ) +\n  theme_map()\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nPart c\nSummarize what you learned about Starbucks from this map.\n\n There is the highest density of Starbucks in the US and nearby countries, as well as in Europe and far eastern Asia. There are a few Starbucks scatters around Russia and spreading into more western east asia. \nExercise 5: Zooming in on some countries\nInstead of world_boundaries &lt;- ne_countries(returnclass = 'sf') we could zoom in on…\n\nthe continent of Africa: ne_countries(continent = 'Africa', returnclass = 'sf')\n\na set of countries: ne_countries(country = c('france', 'united kingdom', 'germany'), returnclass = 'sf')\n\nboundaries within a country: ne_states(country = 'united states of america', returnclass = 'sf')\n\n\nOur goal here will be to map the Starbucks locations in Canada, Mexico, and the US.\nPart a\nTo make this map, we again need two pieces of information.\n\nData on Starbucks for only Canada, Mexico, and the US, labeled as “CA”, “MX”, “US” in the starbucks data.\n\n\n# We'll learn this syntax soon! Don't worry about it now.\nstarbucks_cma &lt;- starbucks |&gt; \n  filter(Country %in% c('CA', 'MX', 'US'))\n\n\nA background map of state- and national-level boundaries in Canada, Mexico, and the US. This requires ne_states() in the rnaturalearth package where the countries are labeled ‘canada’, ‘mexico’, ‘united states of america’.\n\n\ncma_boundaries &lt;- ne_states(\n  country = c(\"canada\", \"mexico\", \"united states of america\"),\n  returnclass = \"sf\")\n\nPart b\nMake the map!\n\n# Just the boundaries\nggplot(cma_boundaries) + \n  geom_sf()\n\n\n\n\n\n\n\n\n# Add the points\n# And zoom in\nggplot(cma_boundaries) + \n  geom_sf() + \n  geom_point(\n    data = starbucks_cma,\n    aes(x = Longitude, y = Latitude),\n    alpha = 0.3,\n    size = 0.2,\n    color = \"darkgreen\"\n  ) +\n  coord_sf(xlim = c(-179.14, -50)) +\n  theme_map()\n\n\n\n\n\n\n\nExercise 6: A state and county-level map\nLet’s get an even higher resolution map of Starbucks locations within the states of Minnesota, Wisconsin, North Dakota, and South Dakota, with a background map at the county-level.\nPart a\nTo make this map, we again need two pieces of information.\n\nData on Starbucks for only the states of interest.\n\n\nstarbucks_midwest &lt;- starbucks |&gt; \n  filter(State.Province %in% c(\"MN\", \"ND\", \"SD\", \"WI\"))\n\n\nA background map of state- and county-level boundaries in these states. This requires st_as_sf() in the sf package, and map() in the maps package, where the countries are labeled ‘minnesota’, ‘north dakota’, etc.\n\n\n# Load packages\nlibrary(sf)\n\nLinking to GEOS 3.13.0, GDAL 3.10.1, PROJ 9.5.1; sf_use_s2() is TRUE\n\nlibrary(maps)\n\n\nAttaching package: 'maps'\n\n\nThe following object is masked from 'package:purrr':\n\n    map\n\n# Get the boundaries\nmidwest_boundaries &lt;- st_as_sf(\n  maps::map(\"county\",\n            region = c(\"minnesota\", \"wisconsin\", \"north dakota\", \"south dakota\"), \n            fill = TRUE, plot = FALSE))\n\n# Check it out\nhead(midwest_boundaries)\n\nSimple feature collection with 6 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -96.81268 ymin: 45.05167 xmax: -93.01397 ymax: 48.53526\nGeodetic CRS:  +proj=longlat +ellps=clrk66 +no_defs +type=crs\n                                     ID                           geom\nminnesota,aitkin       minnesota,aitkin MULTIPOLYGON (((-93.03689 4...\nminnesota,anoka         minnesota,anoka MULTIPOLYGON (((-93.51817 4...\nminnesota,becker       minnesota,becker MULTIPOLYGON (((-95.14537 4...\nminnesota,beltrami   minnesota,beltrami MULTIPOLYGON (((-95.58655 4...\nminnesota,benton       minnesota,benton MULTIPOLYGON (((-93.77027 4...\nminnesota,big stone minnesota,big stone MULTIPOLYGON (((-96.10794 4...\n\n\nPart b\nAdjust the code below to make the plot! Remove the # to run it.\n\nggplot(midwest_boundaries) + \n   geom_sf() + \n   geom_point(\n     data = starbucks_midwest,\n     aes(x = Longitude, y = Latitude),\n     alpha = 0.7,\n     size = 0.2, \n     color = 'darkgreen'\n   ) + \n   theme_map()\n\n\n\n\n\n\n\nExercise 7: Contour maps\nEspecially when there are lots of point locations, and those locations start overlapping on a map, it can be tough to visualize areas of higher density. Consider the Starbucks locations in Canada, Mexico, and the US that we mapped earlier:\n\n# Point map (we made this earlier)\nggplot(cma_boundaries) + \n  geom_sf() + \n  geom_point(\n    data = starbucks_cma,\n    aes(x = Longitude, y = Latitude),\n    alpha = 0.3,\n    size = 0.2,\n    color = \"darkgreen\"\n  ) +\n  coord_sf(xlim = c(-179.14, -50), ylim = c(14.54, 83.11)) +\n  theme_map()\n\n\n\n\n\n\n\nNow check out the contour map.\n\n# What changed in the plot?\n# What changed in our code?!\nggplot(cma_boundaries) + \n  geom_sf() + \n  geom_density_2d(\n    data = starbucks_cma,\n    aes(x = Longitude, y = Latitude),\n    size = 0.2,\n    color = \"darkgreen\"\n  ) +\n  coord_sf(xlim = c(-179.14, -50), ylim = c(14.54, 83.11)) +\n  theme_map()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\nPart 3: Choropleth maps\nSpatial data isn’t always in the form of point locations! For example, recall the state and county-level data on presidential elections.\n\nelections_by_state &lt;-  read.csv(\"https://mac-stat.github.io/data/election_2020_by_state.csv\")\nelections_by_counties &lt;- read.csv(\"https://mac-stat.github.io/data/election_2020_county.csv\")\n\nIn these datasets, we’re interested in the overall election outcome by region (state or county), not the specific geographic location of some observation. Let’s wrangle our data first. We’ll focus on just a few variables of interest, and create a new variable (repub_20_categories) that discretizes the repub_pct_20 variable into increments of 5 percentage points (for states) or 10 percentage points (for counties):\n\n# Don't worry about the code!\n\nelections_by_state &lt;- elections_by_state |&gt; \n  filter(state_abbr != \"DC\") |&gt; \n  select(state_name, state_abbr, repub_pct_20) |&gt; \n  mutate(repub_20_categories = \n           cut(repub_pct_20, \n               breaks = seq(30, 70, by = 5), \n               labels = c(\"30-34\", \"35-39\", \"40-44\", \"45-49\",\n                          \"50-54\", \"55-59\", \"60-64\", \"65-70\"), \n               include.lowest = TRUE))\n\nelections_by_counties &lt;- elections_by_counties |&gt; \n  select(state_name, state_abbr, county_name, county_fips,\n          repub_pct_20, median_age, median_rent) |&gt; \n  mutate(repub_20_categories = \n           cut(repub_pct_20, \n               breaks = seq(0, 100, by = 10),\n               labels = c(\"0-9\", \"10-19\", \"20-29\", \"30-39\", \"40-49\",\n                          \"50-59\", \"60-69\", \"70-79\", \"80-89\", \"90-100\"),\n               include.lowest = TRUE))\n\nExercise 8: State-level choropleth maps\nLet’s map the 2020 Republican support in each state, repub_pct_20.\nPart a\nWe again need two pieces of information.\n\nData on elections in each state, which we already have: elections_by_state.\nA background map of state boundaries in the US. The boundaries we used for point maps don’t work here. (Optional detail: they’re sf objects and we now need a data.frame object.) Instead, we can use the map_data() function from the ggplot2 package:\n\n\n# Get the latitude and longitude coordinates of state boundaries\nstates_map &lt;- map_data(\"state\")\n\n# Check it out\nhead(states_map)\n\n       long      lat group order  region subregion\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;\n\n\nPause\nImportant detail: Note that the region variable in states_map, and the state_name variable in elections_by_state both label states by the full name in lower case letters. This is critical to the background map and our data being able to communicate.\n\nhead(states_map)\n\n       long      lat group order  region subregion\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;\n\nhead(elections_by_state) \n\n   state_name state_abbr repub_pct_20 repub_20_categories\n1     alabama         AL        62.03               60-64\n2    arkansas         AR        62.40               60-64\n3     arizona         AZ        49.06               45-49\n4  california         CA        34.33               30-34\n5    colorado         CO        41.90               40-44\n6 connecticut         CT        39.21               35-39\n\n\nPart b\nNow map repub_pct_20 by state.\n\n# Note where the dataset, elections_by_state, is used\n# Note where the background map, states_map, is used\nggplot(elections_by_state, aes(map_id = state_name, fill = repub_pct_20)) +\n  geom_map(map = states_map) +\n  expand_limits(x = states_map$long, y = states_map$lat) +\n  theme_map() \n\n\n\n\n\n\n\n\n# Make it nicer!\nggplot(elections_by_state, aes(map_id = state_name, fill = repub_pct_20)) +\n  geom_map(map = states_map) +\n  expand_limits(x = states_map$long, y = states_map$lat) +\n  theme_map() + \n  scale_fill_gradientn(name = \"% Republican\", colors = c(\"blue\", \"purple\", \"red\"), values = scales::rescale(seq(0, 100, by = 5)))\n\n\n\n\n\n\n\nIt’s not easy to get fine control over the color scale for the quantitative repub_pct_20 variable. Instead, let’s plot the discretized version, repub_20_categories:\n\nggplot(elections_by_state, aes(map_id = state_name, fill = repub_20_categories)) +\n  geom_map(map = states_map) +\n  expand_limits(x = states_map$long, y = states_map$lat) +\n  theme_map()\n\n\n\n\n\n\n\n\n# Load package needed for refining color palette\nlibrary(RColorBrewer)\n\n# Now fix the colors\nggplot(elections_by_state, aes(map_id = state_name, fill = repub_20_categories)) +\n  geom_map(map = states_map) +\n  expand_limits(x = states_map$long, y = states_map$lat) +\n  theme_map() + \n  scale_fill_manual(values = rev(brewer.pal(8, \"RdBu\")), name = \"% Republican\")\n\n\n\n\n\n\n\nPart c\nWe can add other layers, like points, on top of a choropleth map. Add a Starbucks layer! Do you notice any relationship between Starbucks and elections? Or are we just doing things at this point? It seems like there are more Starbucks in less republican areas, but maybe that;s a coincidence ;)\n\n# Get only the starbucks data from the US\nstarbucks_us &lt;- starbucks |&gt; \n  filter(Country == \"US\")\n\n# Map it\nggplot(elections_by_state, aes(map_id = state_name, fill = repub_20_categories)) +\n  geom_map(map = states_map) +\n  geom_point(\n    data = starbucks_us,\n    aes(x = Longitude, y = Latitude),\n    size = 0.05,\n    alpha = 0.2,\n    inherit.aes = FALSE\n  ) +\n  expand_limits(x = states_map$long, y = states_map$lat) +\n  theme_map() + \n  scale_fill_manual(values = rev(brewer.pal(8, \"RdBu\")), name = \"% Republican\")\n\n\n\n\n\n\n\nDetails (if you’re curious)\n\n\nmap_id is a required aesthetic for geom_map().\n\nIt specifies which variable in our dataset indicates the region (here state_name).\nIt connects this variable (state_name) to the region variable in our mapping background (states_map). These variables must have the same possible outcomes in order to be matched up (alabama, alaska, arizona,…).\n\n\n\nexpand_limits() assures that the map covers the entire area it’s supposed to, by pulling longitudes and latitudes from the states_map.\nPart d\nWe used geom_sf() for point maps. What geom do we use for choropleth maps? geom_point\nExercise 9: County-level choropleth maps\nLet’s map the 2020 Republican support in each county.\nPart a\nWe again need two pieces of information.\n\nData on elections in each county, which we already have: elections_by_county.\nA background map of county boundaries in the US, stored in the county_map dataset in the socviz package:\n\n\n# Get the latitude and longitude coordinates of county boundaries\nlibrary(socviz)\ndata(county_map) \n\n# Check it out\nhead(county_map)\n\n     long      lat order  hole piece            group    id\n1 1225889 -1275020     1 FALSE     1 0500000US01001.1 01001\n2 1235324 -1274008     2 FALSE     1 0500000US01001.1 01001\n3 1244873 -1272331     3 FALSE     1 0500000US01001.1 01001\n4 1244129 -1267515     4 FALSE     1 0500000US01001.1 01001\n5 1272010 -1262889     5 FALSE     1 0500000US01001.1 01001\n6 1276797 -1295514     6 FALSE     1 0500000US01001.1 01001\n\n\nPause\nImportant detail: We officially have a headache. Our county_map refers to each county by a 5-number id. Our elections_by_counties data refers to each county by a county_fips code, which is mostly the same as id, BUT drops any 0’s at the beginning of the code.\n\nhead(county_map)\n\n     long      lat order  hole piece            group    id\n1 1225889 -1275020     1 FALSE     1 0500000US01001.1 01001\n2 1235324 -1274008     2 FALSE     1 0500000US01001.1 01001\n3 1244873 -1272331     3 FALSE     1 0500000US01001.1 01001\n4 1244129 -1267515     4 FALSE     1 0500000US01001.1 01001\n5 1272010 -1262889     5 FALSE     1 0500000US01001.1 01001\n6 1276797 -1295514     6 FALSE     1 0500000US01001.1 01001\n\nhead(elections_by_counties)\n\n  state_name state_abbr    county_name county_fips repub_pct_20 median_age\n1    Alabama         AL Autauga County        1001        71.44       37.5\n2    Alabama         AL Baldwin County        1003        76.17       41.5\n3    Alabama         AL Barbour County        1005        53.45       38.3\n4    Alabama         AL    Bibb County        1007        78.43       39.4\n5    Alabama         AL  Blount County        1009        89.57       39.6\n6    Alabama         AL Bullock County        1011        24.84       39.6\n  median_rent repub_20_categories\n1         668               70-79\n2         693               70-79\n3         382               50-59\n4         351               70-79\n5         403               80-89\n6         276               20-29\n\n\nThis just means that we have to wrangle the data so that it can communicate with the background map.\n\n# Add 0's at the beginning of any fips_code that's fewer than 5 numbers long\n# Don't worry about the syntax\nelections_by_counties &lt;- elections_by_counties |&gt; \n  mutate(county_fips = as.character(county_fips)) |&gt; \n  mutate(county_fips = \n           ifelse(nchar(county_fips) == 4, paste0(\"0\", county_fips), county_fips))\n\nPart b\nNow map Republican support by county. Let’s go straight to the discretized repub_20_categories variable, and a good color scale.\n\nggplot(elections_by_counties, aes(map_id = county_fips, fill = repub_20_categories)) +\n  geom_map(map = county_map) +\n  scale_fill_manual(values = rev(brewer.pal(10, \"RdBu\")), name = \"% Republican\") +\n  expand_limits(x = county_map$long, y = county_map$lat) +\n  theme_map() +\n  theme(legend.position = \"right\") + \n  coord_equal()\n\n\n\n\n\n\n\nExercise 10: Play around!\nConstruct county-level maps of median_rent and median_age.\n\nelections_by_counties &lt;- elections_by_counties |&gt; \n  select(state_name, state_abbr, county_name, county_fips,\n          repub_pct_20, median_age, median_rent) |&gt; \n  mutate(rent_categories = \n           cut(median_rent, \n               breaks = seq(0, 1660, by = 160),\n               include.lowest = TRUE))\n\nggplot(elections_by_counties, aes(map_id = county_fips, fill = rent_categories)) +\n  geom_map(map = county_map) +\n  scale_fill_manual(values = rev(brewer.pal(10, \"RdBu\")), name = \"Median Rent Category\") +\n  expand_limits(x = county_map$long, y = county_map$lat) +\n  theme_map() +\n  theme(legend.position = \"right\") + \n  coord_equal()\n\n\n\n\n\n\n\nExercise 11: Choropleth maps with leaflet\nThough ggplot() is often better for this purpose, we can also make choropleth maps with leaflet(). If you’re curious, check out the leaflet documentation:\nhttps://rstudio.github.io/leaflet/choropleths.html",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Spatial Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-effective.html",
    "href": "ica/ica-effective.html",
    "title": "\n13  Effective Viz\n",
    "section": "",
    "text": "13.1 Exercises",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Effective Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-effective.html#exercises",
    "href": "ica/ica-effective.html#exercises",
    "title": "\n13  Effective Viz\n",
    "section": "",
    "text": "Exercise 1: Professionalism\nLet’s examine weather in 3 Australian locations.\n\n# Load tidyverse package for plotting and wrangling\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Import the data\nweather &lt;- read.csv(\"https://mac-stat.github.io/data/weather_3_locations.csv\") |&gt; \n  mutate(date = as.Date(date))\n\nThe following plot is fine for things like homework or just playing around. But we’ll make it more “professional” looking below.\n\nggplot(weather, aes(y = temp3pm, x = temp9am, color = location)) + \n  geom_point()\n\nWarning: Removed 27 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nPart a\nReplace A, B, C, and D in the code below to:\n\nAdd a short, but descriptive title. Under 10 words.\nChange the x- and y-axis labels, currently just the names of the variables in the dataset. These should be short and include units.\nChange the legend title to “Location” (just for practice, not because it’s better than “location”).\n\n\nggplot(weather, aes(y = temp3pm, x = temp9am, color = location)) + \n  geom_point() + \n  labs(x = \"Temp at 9 am (Celcius)\", y = \"Temp at 3 pm (Celcius)\", title = \"Australian City 9 am and 3 pm Temperatures\", color = \"City\")  \n\nWarning: Removed 27 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nPart b\nWhen we’re including our plot in an article, paper, book, or other similar outlet, we should (and are expected to) provide a more descriptive figure caption. Typically, this is instead of a title and is more descriptive of what exactly is being plotted.\n\nAdd a figure caption in the top of the chunk.\nInclude your x-axis, y-axis, and legend labels from Part a.\nRender your Rmd and check out how the figure caption appears.\n\n\nggplot(weather, aes(y = temp3pm, x = temp9am, color = location)) + \n  geom_point() + \n  labs(x = \"Temp at 9 am (Celcius)\", y = \"Temp at 3 pm (Celcius)\", color = \"City\")  \n\nWarning: Removed 27 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\nA scatterplot comparing temperatures at 9 am and 3 pm in 3 Australian cities, Hobart, Uluru, and Wollongong. Points are color-coded by city, and 9 am temperature is used as the independent variable predicting the 3 pm temperature.\n\n\n\nExercise 2: Accessibility\nLet’s now make a graphic more accessible.\n\nggplot(weather, aes(x = temp3pm, fill = location)) + \n  geom_density(alpha = 0.5) + \n  labs(x = \"3pm temperature (Celsius)\")  \n\nWarning: Removed 19 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\nDensity plots of 3pm temperatures in 3 Australian locations.\n\n\n\nPart a\nLet’s add some alt text that can be picked up by screen readers. This is a great resource on writing alt text for data viz. In short, whereas figure captions are quick descriptions which assume that the viz is accessible, alt text is a longer description which assumes the viz is not accessible. Alt text should concisely articulate:\n\nWhat your visualization is (e.g. a density plot of 3pm temperatures in Hobart, Uluru, and Wollongong, Australia).\nA 1-sentence description of the most important takeaway.\nA link to your data source if it’s not already in the caption.\n\nAdd appropriate alt text at the top of the chunk, in fig-alt. Then render your qmd, and hover over the image in your rendered html file to check out the alt text.\n\nggplot(weather, aes(x = temp3pm, fill = location)) + \n  geom_density(alpha = 0.5) + \n  labs(x = \"3pm temperature (Celsius)\")  \n\nWarning: Removed 19 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\nDensity plots of 3pm temperatures in 3 Australian locations.\n\n\n\n\n\nWarning: Removed 19 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\nDensity plots of 3pm temperatures in 3 Australian locations.\n\n\n\n\nPart b\nColor is another important accessibility consideration. Let’s check out the color accessibility of our density plot.\n\nRun the ggplot() code from Part a in your console. The viz will pop up in the Plots tab.\nIn the Plots tab, click “Export” then “Save as image”. Save the image somewhere.\nNavigate to https://www.color-blindness.com/coblis-color-blindness-simulator/\n\nAbove the image of crayons (I think it’s crayons?), click “Choose file” and choose the plot file you just saved.\nClick the various simulator buttons (eg: Red-Weak/Protanomaly) to check out how the colors in this plot might appear to others.\nSummarize what you learn. What impact might our color choices have on one’s ability to interpret the viz?\nPart c\nWe can change our color schemes! There are many color-blind friendly palettes in R. In the future, we’ll set a default, more color-blind friendly color theme at the top of our Rmds. We can also do this individually for any plot that uses color. Run the chunks below to explore various options.\n\nggplot(weather, aes(x = temp3pm, fill = location)) + \n  geom_density(alpha = 0.5) + \n  labs(x = \"3pm temperature (Celsius)\") + \n  scale_fill_viridis_d()    \n\nWarning: Removed 19 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\n# In the color scale line:\n# Change \"fill\" to \"color\" since we use color in the aes()\n# Change \"d\" (discrete) to \"c\" (continuous) since maxtemp is on a continuous scale\nggplot(weather, aes(y = temp3pm, x = temp9am, color = maxtemp)) + \n  geom_point(alpha = 0.5) + \n  labs(x = \"3pm temperature (Celsius)\") + \n  scale_color_viridis_c()\n\nWarning: Removed 27 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nExercise 3: Ethics\nLet’s scratch the surface of ethics in data viz. Central to this discussion is the consideration of impact.\nPart a\nAt a minimum, our data viz should not mislead. Reconsider the climate change example from above. Why is this plot unethical and what impact might it have on policy, public opinion, etc?\n\nPart b\nAgain, data viz ethical considerations go beyond whether or not a plot is misleading. As described in the warm-up, we need to consider: visibility, privacy, power, emotion & embodiment, pluralism, & context. Depending upon the audience and goals of a data viz, addressing these points might require more nuance. Mainly, the viz tools we’ve learned are a great base or foundation, but aren’t the only approaches to data viz. \nPick one or more of the following examples of data viz to discuss with your group. How do the approaches taken:\n\nemphasize one or more of: visibility, privacy, power, emotion, embodiment, pluralism, and/or context?\nimprove upon what we might be able to convey with a simpler bar chart, scatterplot, etc?\n\n\nExample: W.E.B. Du Bois (1868–1963)\nDu Bois (“Doo Boys”) was a “sociologist, socialist, historian, civil rights activist, Pan-Africanist, author, writer, and editor”1. He was also a pioneer in elevating emotion and embodiment in data visualization. For the Paris World Fair of 1900, Du Bois and his team of students from Atlanta University presented 60 data visualizations of the Black experience in America, less than 50 years after the abolishment of slavery. Du Bois noted: “I wanted to set down its aim and method in some outstanding way which would bring my work to notice by the thinking world.” That is, he wanted to increase the impact of his work by partnering technical visualizations with design that better connects to lived experiences. NOTE: This work uses language common to that time period and addresses the topic of slavery. Check out:\n\nA complete set of the data visualizations provided by Anthony Starks (@ajstarks).\nAn article by Allen Hillery (@AlDatavizguy).\n\n\nExample: One person’s experience with long COVID\nNYT article\n\nExample: Decolonizing data viz\nblog post\n\nExample: Visualizing climate change through art\nFutures North with Prof John Kim & Mac students (by Prof Kim, Mac research students)\n\nExample: Personal data collection\nDear Data\n\nPart c\nFor a deeper treatment of similar topics, and more examples, read Data Feminism.\n\nExercise 4: Critique\nPractice critiquing some more complicated data viz listed at Modern Data Science with R, Exercise 2.5.\nThink about the following questions:\n\nWhat story does the data graphic tell? What is the main message that you take away from it?\nCan the data graphic be described in terms of the Grammar of Graphics (frame, glyphs, aesthetics, facet, scale, guide)? If so, please describe.\nCritique and/or praise the visualization choices made by the designer. Do they work? Are they misleading? Thought-provoking? Are there things that you would have done differently?\n\n\nExercise 5: Design Details\nThis final exercise is just “food for thought”. It’s more of a discussion than an exercise, and gets into some of the finer design details and data viz theory. Go as deep or not deep as you’d like here.\nIn refining the details of our data viz, Visualize This and Storytelling with Data provide some of their guiding principles. But again, every context is different.\n\nPut yourself in a reader’s shoes. What parts of the data need explanation?\nShine a light on your data. Try to remove any “chart junk” that distracts from the data.\nVary color and style to emphasize the viz elements that are most important to the story you’re telling.\nIt is easier to judge length than it is to judge area or angles.\nBe thoughtful about how your categories are ordered for categorical data.\n\nGetting into even more of the nitty gritty, we need to be mindful of what geometric elements and aesthetics we use. The following elements/aesthetics are listed in roughly descending order of human ability to perceive and compare nearby objects:2\n\nPosition\nLength\nAngle\nDirection\nShape (but only a very few different shapes)\nArea\nVolume\nShade\nColor. (Color is the most difficult, because it is a 3-dimensional quantity.)\n\nFinally, here are some facts to keep in mind about visual perception from Now You See It.\nPart a: Selectivity\nVisual perception is selective, and our attention is often drawn to contrasts from the norm.\nImplication: We should design visualizations so that the features we want to highlight stand out in contrast from those that are not worth the audience’s attention.\nExample: What stands out in this example image? This is originally from C. Ware, Information Visualization: Perception for Design, 2004? Source: S. Few, Now You See It, 2009, p. 33.\n\nPart b: Familiarity\nOur eyes are drawn to familiar patterns. We observe what we know and expect.\nImplication: Visualizations work best when they display information as patterns that familiar and easy to spot.\nExample: Do you notice anything embedded in this rose image from coolbubble.com? Source: S. Few, Now You See It, 2009, p. 34.\n\nPart c: Revisit\nRevisit Part b. Do you notice anything in the shadows? Go to https://mac-stat.github.io/images/112/rose2.png for an image.\n\nWrapping up\nIf you finish early:\n\nWork on homework if not done already\nComplete any activities you haven’t finished yet, eg, spatial viz, the optional but fun exercises in the Multivariate viz and Bivariate viz activities.\nIf you’ve done all that, explore some datasets in TidyTuesday.",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Effective Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-effective.html#solutions",
    "href": "ica/ica-effective.html#solutions",
    "title": "\n13  Effective Viz\n",
    "section": "\n13.2 Solutions",
    "text": "13.2 Solutions\nThe exercises today are discussion based. There are no “solutions”. Happy to chat in office hours about any ideas here!",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Effective Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-effective.html#footnotes",
    "href": "ica/ica-effective.html#footnotes",
    "title": "\n13  Effective Viz\n",
    "section": "",
    "text": "https://en.wikipedia.org/wiki/W._E._B._Du_Bois↩︎\nB. S. Baumer, D. T. Kaplan, and N. J. Horton, Modern Data Science with R, 2017, p. 15.↩︎",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Effective Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-wrangling.html",
    "href": "ica/ica-wrangling.html",
    "title": "\n14  Wrangling\n",
    "section": "",
    "text": "Exercise 1: select Practice\nUse select() to create a simplified dataset that we’ll use throughout the exercises below.\n\nStore this dataset as elections_small.\nOnly keep the following variables: state_name, county_name, total_votes_20, repub_pct_20, dem_pct_20, total_votes_16, dem_pct_16\n\n\n\n#Testing changes\n# Load tidyverse & data\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nelections &lt;- read.csv(\"https://mac-stat.github.io/data/election_2020_county.csv\")\n\n\n# Define elections_small\nelections_small &lt;- elections |&gt;\n  select(state_name, county_name, total_votes_20, repub_pct_20, dem_pct_20, total_votes_16, dem_pct_16)\n\n# Check out the first 6 rows to confirm your code did what you think it did!\nhead(elections_small)\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16\n1          24661      23.96\n2          94090      19.57\n3          10390      46.66\n4           8748      21.42\n5          25384       8.47\n6           4701      75.09\n\n\nExercise 2: filter Demo\nWhereas select() selects certain variables or columns, filter() keeps certain units of observation or rows relative to their outcome on certain variables. To this end, we must:\n\nIdentify the variable(s) that are relevant to the filter.\n\nUse a “logical comparison operator” to define which values of the variable to keep:\n\n\nsymbol\nmeaning\n\n\n\n==\nequal to\n\n\n!=\nnot equal to\n\n\n&gt;\ngreater than\n\n\n&gt;=\ngreater than or equal to\n\n\n&lt;\nless than\n\n\n&lt;=\nless than or equal to\n\n\n%in% c(???, ???)\na list of multiple values\n\n\n\n\nUse quotes \"\" when specifying outcomes of interest for a categorical variable.\n\n\n\n\n\n\n\nCommenting/Uncommenting Code\n\n\n\nTo comment/uncomment several lines of code at once, highlight them then click ctrl/cmd+shift+c.\n\n\n\n# Keep only data on counties in Hawaii\nelections_small |&gt;\n filter(state_name == \"Hawaii\")\n\n  state_name     county_name total_votes_20 repub_pct_20 dem_pct_20\n1     Hawaii   Hawaii County          87814        30.63      66.88\n2     Hawaii Honolulu County         382114        35.66      62.51\n3     Hawaii    Kauai County          33497        34.58      63.36\n4     Hawaii     Maui County          71044        31.14      66.59\n  total_votes_16 dem_pct_16\n1          64865      63.61\n2         285683      61.48\n3          26335      62.49\n4          51942      64.45\n\n\n\n# What does this do? This keeps only counties in Hawaii or Delaware\nelections_small |&gt;\n  filter(state_name %in% c(\"Hawaii\", \"Delaware\"))\n\n  state_name       county_name total_votes_20 repub_pct_20 dem_pct_20\n1   Delaware       Kent County          87025        47.12      51.19\n2   Delaware New Castle County         287633        30.72      67.81\n3   Delaware     Sussex County         129352        55.07      43.82\n4     Hawaii     Hawaii County          87814        30.63      66.88\n5     Hawaii   Honolulu County         382114        35.66      62.51\n6     Hawaii      Kauai County          33497        34.58      63.36\n7     Hawaii       Maui County          71044        31.14      66.59\n  total_votes_16 dem_pct_16\n1          74253      44.91\n2         261468      62.30\n3         105814      37.17\n4          64865      63.61\n5         285683      61.48\n6          26335      62.49\n7          51942      64.45\n\n\n\n# Keep only data on counties where the Republican got MORE THAN 93.97% of the vote in 2020\n# THINK: What variable is relevant here?\nelections_small |&gt;\n  filter(repub_pct_20 &gt; 93.97)\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1      Texas  Borden County            416        95.43       3.85\n2      Texas    King County            159        94.97       5.03\n3      Texas Roberts County            550        96.18       3.09\n  total_votes_16 dem_pct_16\n1            365       8.49\n2            159       3.14\n3            550       3.64\n\n\n\n# Keep only data on counties where the Republican got AT LEAST 93.97% of the vote in 2020\n# This should have 1 more row (observation) than your answer above\nelections_small |&gt;\n  filter(repub_pct_20 &gt;= 93.97)\n\n  state_name     county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Montana Garfield County            813        93.97       5.04\n2      Texas   Borden County            416        95.43       3.85\n3      Texas     King County            159        94.97       5.03\n4      Texas  Roberts County            550        96.18       3.09\n  total_votes_16 dem_pct_16\n1            715       4.76\n2            365       8.49\n3            159       3.14\n4            550       3.64\n\n\nWe can also filter with respect to 2 rules! Here, think what variables are relevant.\n\n# Keep only data on counties in Texas where the Democrat got more than 65% of the vote in 2020\n# Do this 2 ways.\n# Method 1: 2 filters with 1 condition each\nelections_small |&gt;\n filter(state_name == \"Texas\") |&gt;\n filter(repub_pct_20 &gt; 65)\n\n    state_name          county_name total_votes_20 repub_pct_20 dem_pct_20\n1        Texas      Anderson County          19227        78.59      20.57\n2        Texas       Andrews County           5863        84.31      14.50\n3        Texas      Angelina County          34574        72.53      26.44\n4        Texas       Aransas County          12290        75.17      23.73\n5        Texas        Archer County           4796        89.66       9.30\n6        Texas     Armstrong County           1112        93.08       6.74\n7        Texas      Atascosa County          18118        66.45      32.43\n8        Texas        Austin County          14554        78.65      20.28\n9        Texas        Bailey County           1860        77.10      21.99\n10       Texas       Bandera County          12715        79.10      19.70\n11       Texas        Baylor County           1702        87.78      10.75\n12       Texas        Blanco County           7441        73.15      25.68\n13       Texas        Borden County            416        95.43       3.85\n14       Texas        Bosque County           9126        81.84      17.10\n15       Texas         Bowie County          38261        70.87      28.09\n16       Texas       Briscoe County            725        88.14      10.76\n17       Texas         Brown County          15969        85.78      13.19\n18       Texas      Burleson County           8609        78.33      20.77\n19       Texas        Burnet County          24717        75.93      22.81\n20       Texas       Calhoun County           7856        71.80      27.34\n21       Texas      Callahan County           6832        88.00      10.74\n22       Texas          Camp County           5060        71.66      27.55\n23       Texas        Carson County           3122        89.01       9.51\n24       Texas          Cass County          13927        79.22      20.07\n25       Texas        Castro County           2083        76.91      22.37\n26       Texas      Chambers County          21652        80.15      18.46\n27       Texas      Cherokee County          19508        77.41      21.58\n28       Texas     Childress County           2279        85.26      13.60\n29       Texas          Clay County           5741        88.29      10.70\n30       Texas       Cochran County           1000        80.90      17.70\n31       Texas          Coke County           1779        89.15      10.01\n32       Texas       Coleman County           4124        88.29      10.94\n33       Texas Collingsworth County           1218        86.04      12.73\n34       Texas      Colorado County           9975        74.91      24.26\n35       Texas         Comal County          88863        70.60      27.94\n36       Texas      Comanche County           6086        85.06      14.02\n37       Texas        Concho County           1268        83.44      15.54\n38       Texas         Cooke County          18988        82.14      16.91\n39       Texas       Coryell County          23490        65.72      32.21\n40       Texas        Cottle County            662        81.57      17.07\n41       Texas         Crane County           1503        82.97      16.03\n42       Texas      Crockett County           1574        77.51      21.86\n43       Texas        Crosby County           1953        71.48      26.98\n44       Texas        Dallam County           1609        86.33      12.24\n45       Texas        Dawson County           3789        77.88      21.32\n46       Texas    Deaf Smith County           4610        71.45      27.42\n47       Texas         Delta County           2592        83.41      15.55\n48       Texas        DeWitt County           8118        80.89      18.40\n49       Texas       Dickens County            988        86.34      13.16\n50       Texas        Donley County           1648        87.26      12.01\n51       Texas      Eastland County           8292        87.28      11.85\n52       Texas         Ector County          44581        73.34      25.50\n53       Texas       Edwards County           1066        83.77      15.76\n54       Texas         Ellis County          85493        66.34      32.24\n55       Texas         Erath County          16853        81.20      17.30\n56       Texas         Falls County           6133        68.11      30.96\n57       Texas        Fannin County          15004        81.12      17.70\n58       Texas       Fayette County          12941        78.60      20.56\n59       Texas        Fisher County           1826        79.30      19.28\n60       Texas         Floyd County           2039        77.69      21.48\n61       Texas         Foard County            551        80.76      17.97\n62       Texas      Franklin County           5009        83.07      16.05\n63       Texas     Freestone County           8711        80.25      18.77\n64       Texas        Gaines County           5995        89.32       9.61\n65       Texas         Garza County           1653        85.48      13.97\n66       Texas     Gillespie County          15846        78.97      20.04\n67       Texas     Glasscock County            653        93.57       5.97\n68       Texas        Goliad County           3988        77.36      21.99\n69       Texas      Gonzales County           7647        73.58      25.47\n70       Texas          Gray County           7782        87.90      10.65\n71       Texas       Grayson County          59439        74.30      24.40\n72       Texas         Gregg County          47982        67.72      30.84\n73       Texas        Grimes County          12413        75.98      22.82\n74       Texas          Hale County           9584        74.89      23.78\n75       Texas          Hall County           1168        85.19      14.38\n76       Texas      Hamilton County           4349        83.15      14.74\n77       Texas      Hansford County           2047        90.33       8.11\n78       Texas      Hardeman County           1580        84.18      15.25\n79       Texas        Hardin County          27635        86.33      12.57\n80       Texas      Harrison County          29710        72.25      26.62\n81       Texas       Hartley County           2078        89.89       9.38\n82       Texas       Haskell County           2214        83.11      15.94\n83       Texas      Hemphill County           1720        86.40      11.98\n84       Texas     Henderson County          36310        79.62      19.44\n85       Texas          Hill County          14931        79.87      19.15\n86       Texas       Hockley County           8095        80.74      18.31\n87       Texas          Hood County          32541        81.42      17.36\n88       Texas       Hopkins County          15939        79.80      19.11\n89       Texas       Houston County           9437        74.81      24.52\n90       Texas        Howard County          10240        78.65      20.21\n91       Texas      Hudspeth County           1165        66.87      31.85\n92       Texas          Hunt County          38574        75.60      23.09\n93       Texas    Hutchinson County           8771        87.57      11.00\n94       Texas         Irion County            889        85.38      13.50\n95       Texas          Jack County           3782        90.38       8.75\n96       Texas       Jackson County           6340        82.51      16.29\n97       Texas        Jasper County          15608        80.36      18.93\n98       Texas       Johnson County          72005        75.87      22.87\n99       Texas         Jones County           6741        83.96      14.82\n100      Texas        Karnes County           5249        75.60      23.51\n101      Texas       Kaufman County          56703        66.35      32.46\n102      Texas       Kendall County          26438        75.96      22.77\n103      Texas        Kenedy County            194        65.46      33.51\n104      Texas          Kent County            462        88.96      10.17\n105      Texas          Kerr County          27745        75.25      23.51\n106      Texas        Kimble County           2292        86.69      12.39\n107      Texas          King County            159        94.97       5.03\n108      Texas        Kinney County           1603        71.37      27.82\n109      Texas          Knox County           1456        81.04      18.20\n110      Texas         Lamar County          21418        78.25      20.81\n111      Texas          Lamb County           4408        79.88      19.06\n112      Texas      Lampasas County          10399        77.76      20.62\n113      Texas        Lavaca County          10197        86.34      13.07\n114      Texas           Lee County           8086        77.36      21.64\n115      Texas          Leon County           8674        86.73      12.36\n116      Texas       Liberty County          29334        79.44      19.72\n117      Texas     Limestone County           9095        74.65      24.33\n118      Texas      Lipscomb County           1353        89.06       9.68\n119      Texas      Live Oak County           5054        83.08      16.20\n120      Texas         Llano County          12660        79.61      19.47\n121      Texas        Loving County             66        90.91       6.06\n122      Texas       Lubbock County         120771        65.30      33.13\n123      Texas          Lynn County           2293        80.81      18.67\n124      Texas     McCulloch County           3436        84.52      14.26\n125      Texas      McMullen County            516        89.15      10.27\n126      Texas       Madison County           5297        78.70      20.54\n127      Texas        Marion County           4864        71.34      27.53\n128      Texas        Martin County           2160        85.97      13.33\n129      Texas         Mason County           2474        80.48      18.47\n130      Texas     Matagorda County          13726        71.73      27.20\n131      Texas        Medina County          22644        69.08      29.91\n132      Texas        Menard County           1028        80.06      19.16\n133      Texas       Midland County          58856        77.52      20.95\n134      Texas         Milam County          10576        75.49      23.60\n135      Texas         Mills County           2505        88.50      10.82\n136      Texas      Mitchell County           2579        84.14      15.39\n137      Texas      Montague County           9814        87.78      11.18\n138      Texas    Montgomery County         271451        71.24      27.40\n139      Texas         Moore County           5508        79.14      19.28\n140      Texas        Morris County           5587        69.30      29.87\n141      Texas        Motley County            652        92.64       7.06\n142      Texas       Navarro County          19121        72.17      26.68\n143      Texas        Newton County           6094        80.11      19.25\n144      Texas         Nolan County           5356        77.13      21.70\n145      Texas     Ochiltree County           3156        89.10       9.57\n146      Texas        Oldham County           1009        90.88       8.03\n147      Texas        Orange County          35994        81.09      17.66\n148      Texas    Palo Pinto County          12485        81.53      17.44\n149      Texas        Panola County          11451        81.44      17.96\n150      Texas        Parker County          76128        81.50      17.10\n151      Texas        Parmer County           2650        80.57      18.42\n152      Texas         Pecos County           4668        68.87      29.61\n153      Texas          Polk County          24181        76.81      22.28\n154      Texas        Potter County          33321        68.49      29.77\n155      Texas         Rains County           6053        85.16      13.91\n156      Texas       Randall County          64674        78.54      19.79\n157      Texas        Reagan County           1124        83.81      15.30\n158      Texas          Real County           1982        82.90      16.15\n159      Texas     Red River County           5806        77.80      21.46\n160      Texas       Refugio County           3366        65.66      32.92\n161      Texas       Roberts County            550        96.18       3.09\n162      Texas     Robertson County           8099        69.71      29.31\n163      Texas      Rockwall County          53891        68.15      30.45\n164      Texas       Runnels County           4409        86.35      12.52\n165      Texas          Rusk County          21368        77.38      21.66\n166      Texas        Sabine County           5487        87.19      12.19\n167      Texas San Augustine County           4002        75.14      24.49\n168      Texas   San Jacinto County          12638        80.40      18.49\n169      Texas      San Saba County           2602        88.70      11.03\n170      Texas    Schleicher County           1159        81.10      18.21\n171      Texas        Scurry County           5869        84.90      13.94\n172      Texas   Shackelford County           1628        91.15       7.99\n173      Texas        Shelby County          10084        79.09      20.51\n174      Texas       Sherman County            991        89.40       9.18\n175      Texas         Smith County         100075        69.03      29.59\n176      Texas     Somervell County           4947        82.98      15.52\n177      Texas      Stephens County           3800        89.08      10.45\n178      Texas      Sterling County            639        91.39       7.98\n179      Texas     Stonewall County            736        83.56      15.76\n180      Texas        Sutton County           1557        78.48      20.68\n181      Texas       Swisher County           2355        78.34      20.30\n182      Texas        Taylor County          55112        71.76      26.47\n183      Texas       Terrell County            458        72.93      25.98\n184      Texas         Terry County           3612        77.85      20.96\n185      Texas  Throckmorton County            899        89.66       9.12\n186      Texas         Titus County          10539        71.83      27.10\n187      Texas     Tom Green County          45210        71.47      27.07\n188      Texas       Trinity County           6938        80.41      19.07\n189      Texas         Tyler County           9660        84.82      14.52\n190      Texas        Upshur County          18887        83.70      15.23\n191      Texas         Upton County           1368        86.11      12.43\n192      Texas     Van Zandt County          25994        85.67      13.53\n193      Texas      Victoria County          34188        68.32      30.36\n194      Texas        Walker County          23612        65.12      33.39\n195      Texas          Ward County           4060        79.83      18.82\n196      Texas    Washington County          17418        74.40      24.46\n197      Texas       Wharton County          16761        71.15      28.01\n198      Texas       Wheeler County           2337        92.38       7.19\n199      Texas       Wichita County          46030        69.67      28.59\n200      Texas     Wilbarger County           4524        77.90      21.13\n201      Texas        Wilson County          25013        73.81      25.39\n202      Texas       Winkler County           2126        82.46      16.84\n203      Texas          Wise County          32362        83.53      15.37\n204      Texas          Wood County          22779        83.63      15.40\n205      Texas        Yoakum County           2631        82.63      15.96\n206      Texas         Young County           8239        86.30      12.55\n    total_votes_16 dem_pct_16\n1            16887      19.89\n2             4926      16.97\n3            29870      25.24\n4            10467      23.48\n5             4269       9.23\n6             1017       6.88\n7            13605      34.07\n8            12255      18.92\n9             1784      22.25\n10           10213      16.90\n11            1492      12.80\n12            5669      21.94\n13             365       8.49\n14            7797      16.38\n15           34470      25.62\n16             734      12.40\n17           13949      11.62\n18            6945      21.47\n19           19046      19.87\n20            6957      30.44\n21            5556      10.22\n22            4542      27.74\n23            2945       8.46\n24           11606      19.39\n25            1996      26.35\n26           16661      17.64\n27           16701      20.75\n28            2084      12.14\n29            5011      10.70\n30             901      21.09\n31            1423       9.84\n32            3635      10.65\n33            1154      12.56\n34            8480      23.31\n35           61567      23.01\n36            5208      15.13\n37            1067      13.87\n38           15894      14.75\n39           18127      27.90\n40             583      14.41\n41            1381      21.65\n42            1395      26.67\n43            1712      27.34\n44            1535      14.46\n45            3562      23.44\n46            4201      28.21\n47            2279      17.55\n48            6822      17.02\n49             909      14.08\n50            1460      13.01\n51            6935      11.19\n52           36373      28.11\n53            1065      28.36\n54           63064      25.68\n55           13810      15.63\n56            5232      32.15\n57           11972      17.73\n58           11029      19.29\n59            1722      23.40\n60            1958      22.22\n61             511      22.11\n62            4356      15.27\n63            7608      19.20\n64            4609      12.95\n65            1475      15.19\n66           13123      17.42\n67             602       5.65\n68            3713      26.21\n69            6304      24.86\n70            7360       9.50\n71           47068      21.83\n72           41365      28.10\n73            9528      23.03\n74            8802      23.74\n75            1089      15.06\n76            3609      13.27\n77            1947       8.78\n78            1511      16.48\n79           22768      12.21\n80           26364      27.04\n81            1945       8.84\n82            1765      17.79\n83            1687      10.73\n84           29855      18.92\n85           12906      19.64\n86            7282      17.26\n87           26120      15.32\n88           13476      18.61\n89            8302      23.83\n90            8692      20.34\n91             871      37.20\n92           31185      20.21\n93            8126      10.51\n94             760      11.84\n95            3339       9.40\n96            5275      17.14\n97           13351      19.34\n98           57270      19.17\n99            5932      15.76\n100           4185      27.34\n101          40979      25.05\n102          19936      18.22\n103            186      53.23\n104            433      13.63\n105          23090      20.20\n106           1945      10.59\n107            159       3.14\n108           1424      32.09\n109           1358      18.19\n110          18537      19.31\n111           3905      19.72\n112           8149      18.15\n113           8633      13.55\n114           6522      21.04\n115           7407      12.27\n116          24155      20.08\n117           7648      22.86\n118           1322      10.21\n119           4267      17.32\n120          10377      17.59\n121             64       6.25\n122          98060      28.51\n123           1997      20.18\n124           3086      15.55\n125            497       8.05\n126           4283      20.57\n127           4206      27.48\n128           1755      15.16\n129           2053      17.19\n130          11779      28.50\n131          17187      26.90\n132            860      17.79\n133          48753      20.50\n134           8607      23.72\n135           2238      10.81\n136           2190      16.12\n137           8604      10.29\n138         203083      22.52\n139           5262      20.83\n140           4964      28.71\n141            620       6.45\n142          16382      24.40\n143           4010      24.39\n144           4744      21.69\n145           2981       9.19\n146            945       8.25\n147          31761      18.00\n148          10175      16.71\n149          10413      17.62\n150          56413      14.76\n151           2818      30.09\n152           4167      37.22\n153          34444      20.68\n154          28330      26.90\n155           4630      13.37\n156          53813      15.50\n157            900      18.56\n158           1678      15.61\n159           5147      22.30\n160           2945      35.11\n161            550       3.64\n162           7010      31.43\n163          39529      24.37\n164           3765      12.01\n165          18994      20.72\n166           4642      13.23\n167           3563      25.46\n168          10305      19.74\n169           2354      12.45\n170           1054      19.83\n171           5261      13.93\n172           1502       6.86\n173           9072      19.37\n174            933      10.29\n175          83161      26.61\n176           3876      13.93\n177           3453      10.08\n178            633      11.06\n179            701      19.26\n180           1415      22.12\n181           2190      21.10\n182          45266      22.20\n183            437      32.04\n184           1763      24.05\n185            808      10.40\n186           9176      27.66\n187          38104      23.93\n188           5982      19.29\n189           7969      15.60\n190          15918      14.94\n191           1296      21.91\n192          21689      12.87\n193          31032      28.50\n194          19683      30.92\n195           3413      22.85\n196          14747      22.93\n197          14707      28.82\n198           2306       8.41\n199          37913      23.08\n200           4101      19.68\n201          19255      24.88\n202           1873      22.42\n203          24661      13.84\n204          18646      14.07\n205           2299      18.53\n206           7658      11.40\n\n# Method 2: 1 filter with 2 conditions\nelections_small |&gt;\n filter(state_name == \"Texas\", repub_pct_20 &gt; 65)\n\n    state_name          county_name total_votes_20 repub_pct_20 dem_pct_20\n1        Texas      Anderson County          19227        78.59      20.57\n2        Texas       Andrews County           5863        84.31      14.50\n3        Texas      Angelina County          34574        72.53      26.44\n4        Texas       Aransas County          12290        75.17      23.73\n5        Texas        Archer County           4796        89.66       9.30\n6        Texas     Armstrong County           1112        93.08       6.74\n7        Texas      Atascosa County          18118        66.45      32.43\n8        Texas        Austin County          14554        78.65      20.28\n9        Texas        Bailey County           1860        77.10      21.99\n10       Texas       Bandera County          12715        79.10      19.70\n11       Texas        Baylor County           1702        87.78      10.75\n12       Texas        Blanco County           7441        73.15      25.68\n13       Texas        Borden County            416        95.43       3.85\n14       Texas        Bosque County           9126        81.84      17.10\n15       Texas         Bowie County          38261        70.87      28.09\n16       Texas       Briscoe County            725        88.14      10.76\n17       Texas         Brown County          15969        85.78      13.19\n18       Texas      Burleson County           8609        78.33      20.77\n19       Texas        Burnet County          24717        75.93      22.81\n20       Texas       Calhoun County           7856        71.80      27.34\n21       Texas      Callahan County           6832        88.00      10.74\n22       Texas          Camp County           5060        71.66      27.55\n23       Texas        Carson County           3122        89.01       9.51\n24       Texas          Cass County          13927        79.22      20.07\n25       Texas        Castro County           2083        76.91      22.37\n26       Texas      Chambers County          21652        80.15      18.46\n27       Texas      Cherokee County          19508        77.41      21.58\n28       Texas     Childress County           2279        85.26      13.60\n29       Texas          Clay County           5741        88.29      10.70\n30       Texas       Cochran County           1000        80.90      17.70\n31       Texas          Coke County           1779        89.15      10.01\n32       Texas       Coleman County           4124        88.29      10.94\n33       Texas Collingsworth County           1218        86.04      12.73\n34       Texas      Colorado County           9975        74.91      24.26\n35       Texas         Comal County          88863        70.60      27.94\n36       Texas      Comanche County           6086        85.06      14.02\n37       Texas        Concho County           1268        83.44      15.54\n38       Texas         Cooke County          18988        82.14      16.91\n39       Texas       Coryell County          23490        65.72      32.21\n40       Texas        Cottle County            662        81.57      17.07\n41       Texas         Crane County           1503        82.97      16.03\n42       Texas      Crockett County           1574        77.51      21.86\n43       Texas        Crosby County           1953        71.48      26.98\n44       Texas        Dallam County           1609        86.33      12.24\n45       Texas        Dawson County           3789        77.88      21.32\n46       Texas    Deaf Smith County           4610        71.45      27.42\n47       Texas         Delta County           2592        83.41      15.55\n48       Texas        DeWitt County           8118        80.89      18.40\n49       Texas       Dickens County            988        86.34      13.16\n50       Texas        Donley County           1648        87.26      12.01\n51       Texas      Eastland County           8292        87.28      11.85\n52       Texas         Ector County          44581        73.34      25.50\n53       Texas       Edwards County           1066        83.77      15.76\n54       Texas         Ellis County          85493        66.34      32.24\n55       Texas         Erath County          16853        81.20      17.30\n56       Texas         Falls County           6133        68.11      30.96\n57       Texas        Fannin County          15004        81.12      17.70\n58       Texas       Fayette County          12941        78.60      20.56\n59       Texas        Fisher County           1826        79.30      19.28\n60       Texas         Floyd County           2039        77.69      21.48\n61       Texas         Foard County            551        80.76      17.97\n62       Texas      Franklin County           5009        83.07      16.05\n63       Texas     Freestone County           8711        80.25      18.77\n64       Texas        Gaines County           5995        89.32       9.61\n65       Texas         Garza County           1653        85.48      13.97\n66       Texas     Gillespie County          15846        78.97      20.04\n67       Texas     Glasscock County            653        93.57       5.97\n68       Texas        Goliad County           3988        77.36      21.99\n69       Texas      Gonzales County           7647        73.58      25.47\n70       Texas          Gray County           7782        87.90      10.65\n71       Texas       Grayson County          59439        74.30      24.40\n72       Texas         Gregg County          47982        67.72      30.84\n73       Texas        Grimes County          12413        75.98      22.82\n74       Texas          Hale County           9584        74.89      23.78\n75       Texas          Hall County           1168        85.19      14.38\n76       Texas      Hamilton County           4349        83.15      14.74\n77       Texas      Hansford County           2047        90.33       8.11\n78       Texas      Hardeman County           1580        84.18      15.25\n79       Texas        Hardin County          27635        86.33      12.57\n80       Texas      Harrison County          29710        72.25      26.62\n81       Texas       Hartley County           2078        89.89       9.38\n82       Texas       Haskell County           2214        83.11      15.94\n83       Texas      Hemphill County           1720        86.40      11.98\n84       Texas     Henderson County          36310        79.62      19.44\n85       Texas          Hill County          14931        79.87      19.15\n86       Texas       Hockley County           8095        80.74      18.31\n87       Texas          Hood County          32541        81.42      17.36\n88       Texas       Hopkins County          15939        79.80      19.11\n89       Texas       Houston County           9437        74.81      24.52\n90       Texas        Howard County          10240        78.65      20.21\n91       Texas      Hudspeth County           1165        66.87      31.85\n92       Texas          Hunt County          38574        75.60      23.09\n93       Texas    Hutchinson County           8771        87.57      11.00\n94       Texas         Irion County            889        85.38      13.50\n95       Texas          Jack County           3782        90.38       8.75\n96       Texas       Jackson County           6340        82.51      16.29\n97       Texas        Jasper County          15608        80.36      18.93\n98       Texas       Johnson County          72005        75.87      22.87\n99       Texas         Jones County           6741        83.96      14.82\n100      Texas        Karnes County           5249        75.60      23.51\n101      Texas       Kaufman County          56703        66.35      32.46\n102      Texas       Kendall County          26438        75.96      22.77\n103      Texas        Kenedy County            194        65.46      33.51\n104      Texas          Kent County            462        88.96      10.17\n105      Texas          Kerr County          27745        75.25      23.51\n106      Texas        Kimble County           2292        86.69      12.39\n107      Texas          King County            159        94.97       5.03\n108      Texas        Kinney County           1603        71.37      27.82\n109      Texas          Knox County           1456        81.04      18.20\n110      Texas         Lamar County          21418        78.25      20.81\n111      Texas          Lamb County           4408        79.88      19.06\n112      Texas      Lampasas County          10399        77.76      20.62\n113      Texas        Lavaca County          10197        86.34      13.07\n114      Texas           Lee County           8086        77.36      21.64\n115      Texas          Leon County           8674        86.73      12.36\n116      Texas       Liberty County          29334        79.44      19.72\n117      Texas     Limestone County           9095        74.65      24.33\n118      Texas      Lipscomb County           1353        89.06       9.68\n119      Texas      Live Oak County           5054        83.08      16.20\n120      Texas         Llano County          12660        79.61      19.47\n121      Texas        Loving County             66        90.91       6.06\n122      Texas       Lubbock County         120771        65.30      33.13\n123      Texas          Lynn County           2293        80.81      18.67\n124      Texas     McCulloch County           3436        84.52      14.26\n125      Texas      McMullen County            516        89.15      10.27\n126      Texas       Madison County           5297        78.70      20.54\n127      Texas        Marion County           4864        71.34      27.53\n128      Texas        Martin County           2160        85.97      13.33\n129      Texas         Mason County           2474        80.48      18.47\n130      Texas     Matagorda County          13726        71.73      27.20\n131      Texas        Medina County          22644        69.08      29.91\n132      Texas        Menard County           1028        80.06      19.16\n133      Texas       Midland County          58856        77.52      20.95\n134      Texas         Milam County          10576        75.49      23.60\n135      Texas         Mills County           2505        88.50      10.82\n136      Texas      Mitchell County           2579        84.14      15.39\n137      Texas      Montague County           9814        87.78      11.18\n138      Texas    Montgomery County         271451        71.24      27.40\n139      Texas         Moore County           5508        79.14      19.28\n140      Texas        Morris County           5587        69.30      29.87\n141      Texas        Motley County            652        92.64       7.06\n142      Texas       Navarro County          19121        72.17      26.68\n143      Texas        Newton County           6094        80.11      19.25\n144      Texas         Nolan County           5356        77.13      21.70\n145      Texas     Ochiltree County           3156        89.10       9.57\n146      Texas        Oldham County           1009        90.88       8.03\n147      Texas        Orange County          35994        81.09      17.66\n148      Texas    Palo Pinto County          12485        81.53      17.44\n149      Texas        Panola County          11451        81.44      17.96\n150      Texas        Parker County          76128        81.50      17.10\n151      Texas        Parmer County           2650        80.57      18.42\n152      Texas         Pecos County           4668        68.87      29.61\n153      Texas          Polk County          24181        76.81      22.28\n154      Texas        Potter County          33321        68.49      29.77\n155      Texas         Rains County           6053        85.16      13.91\n156      Texas       Randall County          64674        78.54      19.79\n157      Texas        Reagan County           1124        83.81      15.30\n158      Texas          Real County           1982        82.90      16.15\n159      Texas     Red River County           5806        77.80      21.46\n160      Texas       Refugio County           3366        65.66      32.92\n161      Texas       Roberts County            550        96.18       3.09\n162      Texas     Robertson County           8099        69.71      29.31\n163      Texas      Rockwall County          53891        68.15      30.45\n164      Texas       Runnels County           4409        86.35      12.52\n165      Texas          Rusk County          21368        77.38      21.66\n166      Texas        Sabine County           5487        87.19      12.19\n167      Texas San Augustine County           4002        75.14      24.49\n168      Texas   San Jacinto County          12638        80.40      18.49\n169      Texas      San Saba County           2602        88.70      11.03\n170      Texas    Schleicher County           1159        81.10      18.21\n171      Texas        Scurry County           5869        84.90      13.94\n172      Texas   Shackelford County           1628        91.15       7.99\n173      Texas        Shelby County          10084        79.09      20.51\n174      Texas       Sherman County            991        89.40       9.18\n175      Texas         Smith County         100075        69.03      29.59\n176      Texas     Somervell County           4947        82.98      15.52\n177      Texas      Stephens County           3800        89.08      10.45\n178      Texas      Sterling County            639        91.39       7.98\n179      Texas     Stonewall County            736        83.56      15.76\n180      Texas        Sutton County           1557        78.48      20.68\n181      Texas       Swisher County           2355        78.34      20.30\n182      Texas        Taylor County          55112        71.76      26.47\n183      Texas       Terrell County            458        72.93      25.98\n184      Texas         Terry County           3612        77.85      20.96\n185      Texas  Throckmorton County            899        89.66       9.12\n186      Texas         Titus County          10539        71.83      27.10\n187      Texas     Tom Green County          45210        71.47      27.07\n188      Texas       Trinity County           6938        80.41      19.07\n189      Texas         Tyler County           9660        84.82      14.52\n190      Texas        Upshur County          18887        83.70      15.23\n191      Texas         Upton County           1368        86.11      12.43\n192      Texas     Van Zandt County          25994        85.67      13.53\n193      Texas      Victoria County          34188        68.32      30.36\n194      Texas        Walker County          23612        65.12      33.39\n195      Texas          Ward County           4060        79.83      18.82\n196      Texas    Washington County          17418        74.40      24.46\n197      Texas       Wharton County          16761        71.15      28.01\n198      Texas       Wheeler County           2337        92.38       7.19\n199      Texas       Wichita County          46030        69.67      28.59\n200      Texas     Wilbarger County           4524        77.90      21.13\n201      Texas        Wilson County          25013        73.81      25.39\n202      Texas       Winkler County           2126        82.46      16.84\n203      Texas          Wise County          32362        83.53      15.37\n204      Texas          Wood County          22779        83.63      15.40\n205      Texas        Yoakum County           2631        82.63      15.96\n206      Texas         Young County           8239        86.30      12.55\n    total_votes_16 dem_pct_16\n1            16887      19.89\n2             4926      16.97\n3            29870      25.24\n4            10467      23.48\n5             4269       9.23\n6             1017       6.88\n7            13605      34.07\n8            12255      18.92\n9             1784      22.25\n10           10213      16.90\n11            1492      12.80\n12            5669      21.94\n13             365       8.49\n14            7797      16.38\n15           34470      25.62\n16             734      12.40\n17           13949      11.62\n18            6945      21.47\n19           19046      19.87\n20            6957      30.44\n21            5556      10.22\n22            4542      27.74\n23            2945       8.46\n24           11606      19.39\n25            1996      26.35\n26           16661      17.64\n27           16701      20.75\n28            2084      12.14\n29            5011      10.70\n30             901      21.09\n31            1423       9.84\n32            3635      10.65\n33            1154      12.56\n34            8480      23.31\n35           61567      23.01\n36            5208      15.13\n37            1067      13.87\n38           15894      14.75\n39           18127      27.90\n40             583      14.41\n41            1381      21.65\n42            1395      26.67\n43            1712      27.34\n44            1535      14.46\n45            3562      23.44\n46            4201      28.21\n47            2279      17.55\n48            6822      17.02\n49             909      14.08\n50            1460      13.01\n51            6935      11.19\n52           36373      28.11\n53            1065      28.36\n54           63064      25.68\n55           13810      15.63\n56            5232      32.15\n57           11972      17.73\n58           11029      19.29\n59            1722      23.40\n60            1958      22.22\n61             511      22.11\n62            4356      15.27\n63            7608      19.20\n64            4609      12.95\n65            1475      15.19\n66           13123      17.42\n67             602       5.65\n68            3713      26.21\n69            6304      24.86\n70            7360       9.50\n71           47068      21.83\n72           41365      28.10\n73            9528      23.03\n74            8802      23.74\n75            1089      15.06\n76            3609      13.27\n77            1947       8.78\n78            1511      16.48\n79           22768      12.21\n80           26364      27.04\n81            1945       8.84\n82            1765      17.79\n83            1687      10.73\n84           29855      18.92\n85           12906      19.64\n86            7282      17.26\n87           26120      15.32\n88           13476      18.61\n89            8302      23.83\n90            8692      20.34\n91             871      37.20\n92           31185      20.21\n93            8126      10.51\n94             760      11.84\n95            3339       9.40\n96            5275      17.14\n97           13351      19.34\n98           57270      19.17\n99            5932      15.76\n100           4185      27.34\n101          40979      25.05\n102          19936      18.22\n103            186      53.23\n104            433      13.63\n105          23090      20.20\n106           1945      10.59\n107            159       3.14\n108           1424      32.09\n109           1358      18.19\n110          18537      19.31\n111           3905      19.72\n112           8149      18.15\n113           8633      13.55\n114           6522      21.04\n115           7407      12.27\n116          24155      20.08\n117           7648      22.86\n118           1322      10.21\n119           4267      17.32\n120          10377      17.59\n121             64       6.25\n122          98060      28.51\n123           1997      20.18\n124           3086      15.55\n125            497       8.05\n126           4283      20.57\n127           4206      27.48\n128           1755      15.16\n129           2053      17.19\n130          11779      28.50\n131          17187      26.90\n132            860      17.79\n133          48753      20.50\n134           8607      23.72\n135           2238      10.81\n136           2190      16.12\n137           8604      10.29\n138         203083      22.52\n139           5262      20.83\n140           4964      28.71\n141            620       6.45\n142          16382      24.40\n143           4010      24.39\n144           4744      21.69\n145           2981       9.19\n146            945       8.25\n147          31761      18.00\n148          10175      16.71\n149          10413      17.62\n150          56413      14.76\n151           2818      30.09\n152           4167      37.22\n153          34444      20.68\n154          28330      26.90\n155           4630      13.37\n156          53813      15.50\n157            900      18.56\n158           1678      15.61\n159           5147      22.30\n160           2945      35.11\n161            550       3.64\n162           7010      31.43\n163          39529      24.37\n164           3765      12.01\n165          18994      20.72\n166           4642      13.23\n167           3563      25.46\n168          10305      19.74\n169           2354      12.45\n170           1054      19.83\n171           5261      13.93\n172           1502       6.86\n173           9072      19.37\n174            933      10.29\n175          83161      26.61\n176           3876      13.93\n177           3453      10.08\n178            633      11.06\n179            701      19.26\n180           1415      22.12\n181           2190      21.10\n182          45266      22.20\n183            437      32.04\n184           1763      24.05\n185            808      10.40\n186           9176      27.66\n187          38104      23.93\n188           5982      19.29\n189           7969      15.60\n190          15918      14.94\n191           1296      21.91\n192          21689      12.87\n193          31032      28.50\n194          19683      30.92\n195           3413      22.85\n196          14747      22.93\n197          14707      28.82\n198           2306       8.41\n199          37913      23.08\n200           4101      19.68\n201          19255      24.88\n202           1873      22.42\n203          24661      13.84\n204          18646      14.07\n205           2299      18.53\n206           7658      11.40\n\n\nExercise 3: arrange Demo\narrange() arranges or sorts the rows in a dataset according to a given column or variable, in ascending or descending order:\narrange(variable), arrange(desc(variable))\n\n# Arrange the counties in elections_small from lowest to highest percentage of 2020 Republican support\n# Print out just the first 6 rows\nelections_small |&gt;\n  arrange(repub_pct_20) |&gt;\n  head()\n\n            state_name            county_name total_votes_20 repub_pct_20\n1 District of Columbia   District of Columbia         344356         5.40\n2             Maryland Prince George's County         424855         8.73\n3             Maryland         Baltimore city         237461        10.69\n4             Virginia        Petersburg city          14118        11.22\n5             New York        New York County         694904        12.26\n6           California   San Francisco County         443458        12.72\n  dem_pct_20 total_votes_16 dem_pct_16\n1      92.15         280272      92.85\n2      89.26         351091      89.33\n3      87.28         208980      85.44\n4      87.75          13717      87.52\n5      86.78         591368      87.17\n6      85.27         365295      85.53\n\n\n\n# Arrange the counties in elections_small from highest to lowest percentage of 2020 Republican support\n# Print out just the first 6 rows\nelections_small |&gt;\n  arrange(desc(repub_pct_20)) |&gt;\n  head()\n\n  state_name      county_name total_votes_20 repub_pct_20 dem_pct_20\n1      Texas   Roberts County            550        96.18       3.09\n2      Texas    Borden County            416        95.43       3.85\n3      Texas      King County            159        94.97       5.03\n4    Montana  Garfield County            813        93.97       5.04\n5      Texas Glasscock County            653        93.57       5.97\n6   Nebraska     Grant County            402        93.28       4.98\n  total_votes_16 dem_pct_16\n1            550       3.64\n2            365       8.49\n3            159       3.14\n4            715       4.76\n5            602       5.65\n6            394       5.08\n\n\nExercise 4: mutate Demo\nmutate() can either transform / mutate an existing variable (column), or define a new variable based on existing ones.\nPart a\n\n# What did this code do? It created a new variable called diff_20 that is the difference between the republican percentage and democratic percentage voting in each county\nelections_small |&gt;\n  mutate(diff_20 = repub_pct_20 - dem_pct_20) |&gt;\n  head()\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16 diff_20\n1          24661      23.96   44.42\n2          94090      19.57   53.76\n3          10390      46.66    7.66\n4           8748      21.42   57.73\n5          25384       8.47   80.00\n6           4701      75.09  -49.86\n\n\n\n# What did this code do? It creates a new variable called repub_votes_20 that is the total number of people in each county who voted republican in 2020\nelections_small |&gt;\n  mutate(repub_votes_20 = round(total_votes_20 * repub_pct_20/100)) |&gt;\n  head()\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16 repub_votes_20\n1          24661      23.96          19839\n2          94090      19.57          83542\n3          10390      46.66           5622\n4           8748      21.42           7525\n5          25384       8.47          24711\n6           4701      75.09           1146\n\n\n\n# What did this code do? It creates a new variable to test if republicans got more votes than democrats in each county, or if republicans won the county.\nelections_small |&gt;\n  mutate(repub_win_20 = repub_pct_20 &gt; dem_pct_20) |&gt;\n  head()\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16 repub_win_20\n1          24661      23.96         TRUE\n2          94090      19.57         TRUE\n3          10390      46.66         TRUE\n4           8748      21.42         TRUE\n5          25384       8.47         TRUE\n6           4701      75.09        FALSE\n\n\nPart b\n\n# You try\n# Define a variable that calculates the change in Dem support in 2020 vs 2016\nelections_small |&gt;\n mutate(dem_diff = dem_pct_20 - dem_pct_16) |&gt;\n head()\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16 dem_diff\n1          24661      23.96     3.06\n2          94090      19.57     2.84\n3          10390      46.66    -0.87\n4           8748      21.42    -0.72\n5          25384       8.47     1.10\n6           4701      75.09    -0.39\n\n\n\n# You try\n# Define a variable that determines whether the Dem support was higher in 2020 than in 2016 (TRUE/FALSE)\nelections_small |&gt;\nmutate(dem_increase = (dem_pct_20 - dem_pct_16) &gt; 0) |&gt;\n head()\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16 dem_increase\n1          24661      23.96         TRUE\n2          94090      19.57         TRUE\n3          10390      46.66        FALSE\n4           8748      21.42        FALSE\n5          25384       8.47         TRUE\n6           4701      75.09        FALSE\n\n\nExercise 5: Pipe Series\nLet’s now combine these verbs into a pipe series!\nPart a\n\n\n\n\n\n\nThink then Run\n\n\n\nBEFORE running the below chunk, what do you think it will produce?\nI think it will give a list of counties in Wisconsin in which the democratic vote percentage was higher in 2020 than the republican percentage was in 2020, and then arrange them from counties with the most to least total votes.\n\n\n\nelections_small |&gt;\n  filter(state_name == \"Wisconsin\",\n         repub_pct_20 &lt; dem_pct_20) |&gt;\n  arrange(desc(total_votes_20)) |&gt;\n  head()\n\n  state_name       county_name total_votes_20 repub_pct_20 dem_pct_20\n1  Wisconsin  Milwaukee County         458971        29.27      69.13\n2  Wisconsin       Dane County         344791        22.85      75.46\n3  Wisconsin       Rock County          85360        43.51      54.66\n4  Wisconsin  La Crosse County          67884        42.25      55.75\n5  Wisconsin Eau Claire County          58275        43.49      54.26\n6  Wisconsin    Portage County          40603        47.53      50.31\n  total_votes_16 dem_pct_16\n1         434970      66.44\n2         304729      71.38\n3          75043      52.42\n4          62785      51.61\n5          54080      50.43\n6          38123      48.59\n\n\nPart b\n\n\n\n\n\n\nThink then Run\n\n\n\nBEFORE trying, what do you think will happen if you change the order of filter and arrange:\n\nthe results will be the same. I think this is the case.\n\n\n\n\n# Now try it. Change the order of filter and arrange below.\nelections_small |&gt;\n  arrange(desc(total_votes_20)) |&gt;\n  filter(state_name == \"Wisconsin\",\n         repub_pct_20 &lt; dem_pct_20) |&gt;\n  head()\n\n  state_name       county_name total_votes_20 repub_pct_20 dem_pct_20\n1  Wisconsin  Milwaukee County         458971        29.27      69.13\n2  Wisconsin       Dane County         344791        22.85      75.46\n3  Wisconsin       Rock County          85360        43.51      54.66\n4  Wisconsin  La Crosse County          67884        42.25      55.75\n5  Wisconsin Eau Claire County          58275        43.49      54.26\n6  Wisconsin    Portage County          40603        47.53      50.31\n  total_votes_16 dem_pct_16\n1         434970      66.44\n2         304729      71.38\n3          75043      52.42\n4          62785      51.61\n5          54080      50.43\n6          38123      48.59\n\n\nPart c\nSo the order of filter() and arrange() did not matter – rerranging them produces the same results. BUT what is one advantage of filtering before arranging? It means the computer has to process less data and will take a shorter time running the code overall. This is particularly nice I am guessing if we are working with very large data sets.\nPart d\n\n\n\n\n\n\nThink then Run\n\n\n\nBEFORE running the below chunk, what do you think it will produce? I think it will show only counties in Delaware and create a new variable for them showing if republicans won in 2020, then it will narrow the data frame to only include 4 variables.\n\n\n\nelections_small |&gt;\n  filter(state_name == \"Delaware\") |&gt;\n  mutate(repub_win_20 = repub_pct_20 &gt; dem_pct_20) |&gt;\n  select(county_name, repub_pct_20, dem_pct_20, repub_win_20)\n\n        county_name repub_pct_20 dem_pct_20 repub_win_20\n1       Kent County        47.12      51.19        FALSE\n2 New Castle County        30.72      67.81        FALSE\n3     Sussex County        55.07      43.82         TRUE\n\n\nPart e\n\n\n\n\n\n\nThink then Run\n\n\n\nBEFORE trying, what do you think will happen if you change the order of mutate and select:\n\nwe’ll get an error, because we can’t select a variable that hasn’t been created yet.\n\n\n\n\n# Now try it. Change the order of mutate and select below.\nelections_small |&gt;\n  filter(state_name == \"Delaware\") |&gt;\n  mutate(repub_win_20 = repub_pct_20 &gt; dem_pct_20) |&gt;\n  select(county_name, repub_pct_20, dem_pct_20, repub_win_20)\n\n        county_name repub_pct_20 dem_pct_20 repub_win_20\n1       Kent County        47.12      51.19        FALSE\n2 New Castle County        30.72      67.81        FALSE\n3     Sussex County        55.07      43.82         TRUE\n\n# The below code did not work because the variable county_name did not exist yet\n# elections_small |&gt;\n#   filter(state_name == \"Delaware\") |&gt;\n#   select(county_name, repub_pct_20, dem_pct_20, repub_win_20) |&gt;\n#   mutate(repub_win_20 = repub_pct_20 &gt; dem_pct_20)\n\nExercise 6: DIY Pipe Series\nWe’ve now learned 4 of the 6 wrangling verbs: select, filter, mutate, arrange. Let’s practice combining these into pipe series. Here are some hot tips:\n\nBefore writing any code, translate the prompt: how many distinct wrangling steps are needed and what verb do we need in each step?\nAdd each verb one at a time – don’t try writing a whole chunk at once.\n\nPart a\nShow just the counties in Minnesota and their Democratic 2020 vote percentage, from highest to lowest. Your answer should have just 2 columns.\n\nelections_small |&gt;\n  filter(state_name == \"Minnesota\") |&gt;\n  select(county_name, dem_pct_20) |&gt;\n  arrange(desc(dem_pct_20))\n\n                county_name dem_pct_20\n1             Ramsey County      71.50\n2           Hennepin County      70.46\n3               Cook County      65.58\n4          St. Louis County      56.64\n5             Dakota County      55.73\n6            Olmsted County      54.16\n7         Washington County      53.46\n8         Blue Earth County      50.84\n9               Clay County      50.74\n10              Lake County      50.64\n11          Nicollet County      50.31\n12           Carlton County      49.58\n13            Winona County      49.07\n14              Rice County      48.76\n15          Mahnomen County      48.26\n16             Anoka County      47.79\n17          Beltrami County      47.24\n18            Carver County      46.37\n19             Mower County      46.00\n20             Scott County      45.52\n21           Houston County      42.42\n22           Goodhue County      41.23\n23          Freeborn County      40.96\n24            Norman County      40.80\n25            Itasca County      40.61\n26       Koochiching County      38.41\n27          Watonwan County      38.20\n28           Kittson County      38.12\n29           Stevens County      37.80\n30           Stearns County      37.58\n31          Fillmore County      37.48\n32            Steele County      37.47\n33         Kandiyohi County      36.12\n34            Aitkin County      35.98\n35              Lyon County      35.94\n36     Lac qui Parle County      35.79\n37           Wabasha County      35.78\n38             Grant County      35.58\n39          Traverse County      35.46\n40         Big Stone County      35.41\n41        Pennington County      35.29\n42              Pope County      35.27\n43              Polk County      34.88\n44              Cass County      34.68\n45            Wright County      34.49\n46           Hubbard County      34.42\n47             Swift County      34.35\n48         Crow Wing County      34.17\n49           Chisago County      34.15\n50            Becker County      33.96\n51              Pine County      33.87\n52          Le Sueur County      33.73\n53          Chippewa County      33.67\n54            Nobles County      33.65\n55            Waseca County      33.65\n56             Dodge County      33.47\n57        Otter Tail County      32.85\n58            Benton County      32.70\n59           Douglas County      32.56\n60             Brown County      32.48\n61         Sherburne County      32.48\n62         Faribault County      31.98\n63          Red Lake County      31.47\n64          Renville County      30.71\n65            McLeod County      30.64\n66   Yellow Medicine County      30.54\n67           Lincoln County      30.08\n68        Cottonwood County      30.03\n69           Kanabec County      30.02\n70            Martin County      30.02\n71           Jackson County      29.99\n72        Mille Lacs County      29.98\n73            Wilkin County      29.91\n74              Rock County      29.69\n75            Murray County      29.60\n76            Isanti County      29.45\n77            Sibley County      28.60\n78            Meeker County      28.58\n79           Redwood County      28.43\n80 Lake of the Woods County      27.87\n81        Clearwater County      26.76\n82         Pipestone County      26.44\n83            Wadena County      26.35\n84            Roseau County      25.98\n85          Marshall County      25.33\n86              Todd County      24.79\n87          Morrison County      22.33\n\n\nPart b\nCreate a new dataset named mn_wi that sorts the counties in Minnesota and Wisconsin from lowest to highest in terms of the change in Democratic vote percentage in 2020 vs 2016. This dataset should include the following variables (and only these variables): state_name, county_name, dem_pct_20, dem_pct_16, and a variable measuring the change in Democratic vote percentage in 2020 vs 2016.\n\n# Define the dataset\n# Only store the results once you're confident that they're correct\n\nmn_wi &lt;- elections_small |&gt;\n  filter(state_name %in% c(\"Minnesota\", \"Wisconsin\")) |&gt;\n  mutate(dem_diff = dem_pct_20 - dem_pct_16) |&gt;\n  select(state_name, county_name, dem_pct_20, dem_pct_16, dem_diff) |&gt;\n  arrange(desc(dem_diff))\n\n\n# Check out the first 6 rows to confirm your results\nhead(mn_wi)\n\n  state_name       county_name dem_pct_20 dem_pct_16 dem_diff\n1  Minnesota       Cook County      65.58      56.90     8.68\n2  Minnesota    Olmsted County      54.16      45.75     8.41\n3  Minnesota     Dakota County      55.73      48.22     7.51\n4  Minnesota Blue Earth County      50.84      43.38     7.46\n5  Minnesota     Carver County      46.37      39.03     7.34\n6  Minnesota      Scott County      45.52      38.31     7.21\n\n\nPart c\nConstruct and discuss a plot of the county-level change in Democratic vote percent in 2020 vs 2016, and how this differs between Minnesota and Wisconsin. The plot below shows the change in Democratic support from 2016 to 2020 in Minnesota and Wisconsin respectively. The increase on average was noticeably higher in Minnesota between the two years, and both states had a positive median number of counties whose votes for democratic went up.\n\nggplot(mn_wi, aes(y = dem_diff)) +\n  geom_boxplot() +\n  facet_wrap(~state_name)\n\n\n\n\n\n\n\nExercise 7: summarize Demo\n6 verbs: select, filter, arrange, mutate, summarize, group_by\nLet’s talk about the last 2 verbs. summarize() (or equivalently summarise()) takes an entire data frame as input and outputs a single row with one or more summary statistics. For each chunk below, indicate what the code does.\n\n# What does this do? It gives the median value of the variable that we specify\nelections_small |&gt;\n  summarize(median(repub_pct_20))\n\n  median(repub_pct_20)\n1                68.29\n\n\n\n# What does this do? It creates a new variable that is the median of the republican counts overall\nelections_small |&gt;\n  summarize(median_repub = median(repub_pct_20))\n\n  median_repub\n1        68.29\n\n\n\n# What does this do? It creates and displays two new variables, showing the median percentage of republican votes in each county as well as the total votes all together.\nelections_small |&gt;\n  summarize(median_repub = median(repub_pct_20), total_votes = sum(total_votes_20))\n\n  median_repub total_votes\n1        68.29   157949293\n\n\nExercise 8: summarize + group_by demo\nFinally, group_by() groups the units of observation or rows of a data frame by a specified set of variables. Alone, this function doesn’t change the appearance of our dataset or seem to do anything at all:\n\nelections_small |&gt;\n  group_by(state_name)\n\n# A tibble: 3,109 × 7\n# Groups:   state_name [50]\n   state_name county_name  total_votes_20 repub_pct_20 dem_pct_20 total_votes_16\n   &lt;chr&gt;      &lt;chr&gt;                 &lt;int&gt;        &lt;dbl&gt;      &lt;dbl&gt;          &lt;int&gt;\n 1 Alabama    Autauga Cou…          27770         71.4      27.0           24661\n 2 Alabama    Baldwin Cou…         109679         76.2      22.4           94090\n 3 Alabama    Barbour Cou…          10518         53.4      45.8           10390\n 4 Alabama    Bibb County            9595         78.4      20.7            8748\n 5 Alabama    Blount Coun…          27588         89.6       9.57          25384\n 6 Alabama    Bullock Cou…           4613         24.8      74.7            4701\n 7 Alabama    Butler Coun…           9488         57.5      41.8            8685\n 8 Alabama    Calhoun Cou…          50983         68.8      29.8           47376\n 9 Alabama    Chambers Co…          15284         57.3      41.6           13778\n10 Alabama    Cherokee Co…          12301         86.0      13.2           10503\n# ℹ 3,099 more rows\n# ℹ 1 more variable: dem_pct_16 &lt;dbl&gt;\n\n\nThough it does change the underlying structure of the dataset:\n\n#Check out the structure before and after group_by\nelections_small |&gt;\n  class()\n\n[1] \"data.frame\"\n\nelections_small |&gt;\n  group_by(state_name) |&gt;\n  class()\n\n[1] \"grouped_df\" \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nWhere it really shines is in partnership with summarize().\n\n# What does this do?\n# (What if we didn't use group_by?)\nelections_small |&gt;\n  group_by(state_name) |&gt;\n  summarize(median_repub = median(repub_pct_20), total_votes = sum(total_votes_20))\n\n# A tibble: 50 × 3\n   state_name           median_repub total_votes\n   &lt;chr&gt;                       &lt;dbl&gt;       &lt;int&gt;\n 1 Alabama                      70.6     2323304\n 2 Arizona                      57.9     3387326\n 3 Arkansas                     72.1     1219069\n 4 California                   44.8    17495906\n 5 Colorado                     56.2     3256953\n 6 Connecticut                  41.0     1824280\n 7 Delaware                     47.1      504010\n 8 District of Columbia          5.4      344356\n 9 Florida                      64.6    11067456\n10 Georgia                      68       4997716\n# ℹ 40 more rows\n\n\n\n\n\n\n\n\nReflect\n\n\n\nNotice that group_by() with summarize() produces new data frame or tibble! But the units of observation are now states instead of counties within states.\n\n\nExercise 9: DIY\nLet’s practice (some of) our 6 verbs: select, filter, arrange, mutate, summarize, group_by Remember:\n\nBefore writing any code, translate the given prompts: how many distinct wrangling steps are needed and what verb do we need in each step?\nAdd each verb one at a time.\n\nPart a\nNOTE: Part a is a challenge exercise. If you get really stuck, move on to Part b which is the same overall question, but with hints.\n\n# Sort the *states* from the most to least total votes cast in 2020\nelections_small |&gt;\n  group_by(state_name) |&gt;\n  summarize(votes2020 = sum(total_votes_20)) |&gt;\n  arrange(desc(votes2020))\n\n# A tibble: 50 × 2\n   state_name     votes2020\n   &lt;chr&gt;              &lt;int&gt;\n 1 California      17495906\n 2 Texas           11317911\n 3 Florida         11067456\n 4 New York         8616205\n 5 Pennsylvania     6925255\n 6 Illinois         6038850\n 7 Ohio             5922202\n 8 Michigan         5539302\n 9 North Carolina   5524801\n10 Georgia          4997716\n# ℹ 40 more rows\n\n\n\n# In 2020, what were the total number of votes for the Democratic candidate and the total number of votes for the Republican candidate in each *state*?\nelections_small |&gt;\n  group_by(state_name) |&gt;\n  summarize(repubVotes2020 = sum(repub_pct_20 * total_votes_20), demVotes2020 = sum(dem_pct_20 * total_votes_20))\n\n# A tibble: 50 × 3\n   state_name           repubVotes2020 demVotes2020\n   &lt;chr&gt;                         &lt;dbl&gt;        &lt;dbl&gt;\n 1 Alabama                  144115325.    84966521.\n 2 Arizona                  166167238.   167212561.\n 3 Arkansas                  76063890.    42391888.\n 4 California               600603367.  1110964299.\n 5 Colorado                 136462476.   180439477.\n 6 Connecticut               71531461.   108067747.\n 7 Delaware                  20060118.    29627408.\n 8 District of Columbia       1859522.    31732405.\n 9 Florida                  566859937.   529712890.\n10 Georgia                  246187098.   247365644.\n# ℹ 40 more rows\n\n\n\n# What states did the Democratic candidate win in 2020?\nelections_small |&gt;\n  group_by(state_name) |&gt;\n  summarize(repubVotes2020 = sum(repub_pct_20 * total_votes_20), demVotes2020 = sum(dem_pct_20 * total_votes_20)) |&gt;\n  mutate(dem_win_20 = demVotes2020 &gt; repubVotes2020) |&gt;\n  filter(dem_win_20 == TRUE)\n\n# A tibble: 26 × 4\n   state_name           repubVotes2020 demVotes2020 dem_win_20\n   &lt;chr&gt;                         &lt;dbl&gt;        &lt;dbl&gt; &lt;lgl&gt;     \n 1 Arizona                  166167238.   167212561. TRUE      \n 2 California               600603367.  1110964299. TRUE      \n 3 Colorado                 136462476.   180439477. TRUE      \n 4 Connecticut               71531461.   108067747. TRUE      \n 5 Delaware                  20060118.    29627408. TRUE      \n 6 District of Columbia       1859522.    31732405. TRUE      \n 7 Georgia                  246187098.   247365644. TRUE      \n 8 Hawaii                    19686564.    36612136. TRUE      \n 9 Illinois                 244692971.   347191761. TRUE      \n10 Maine                     35989788.    43046558. TRUE      \n# ℹ 16 more rows\n\n\nPart b\n\n# Sort the states from the most to least total votes cast in 2020\n# HINT: Calculate the total number of votes in each state, then sort\n#COMPLETED IN PART A\n\n\n# In 2020, what were the total number of votes for the Democratic candidate and the total number of votes for the Republican candidate in each state?\n# HINT: First calculate the number of Dem and Repub votes in each *county*\n# Then group and summarize these by state\n#COMPLETED IN PART A\n\n\n# What states did the Democratic candidate win in 2020?\n# HINT: Start with the results from the previous chunk, and then keep only some rows\n#COMPLETED IN PART A\n\nExercise 10: Practice on New Data\nRecall the World Cup football/soccer data from TidyTuesday:\n\nworld_cup &lt;- read.csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-11-29/worldcups.csv\")\n\nYou can find a codebook here. Use (some of) our 6 verbs (select, filter, arrange, mutate, summarize, group_by) and data viz to address the following prompts.\n\n# In what years did Brazil win the World Cup?\nworld_cup |&gt;\n  filter(winner == \"Brazil\") |&gt;\n  select(Years_Brazil_Won = year)\n\n  Years_Brazil_Won\n1             1958\n2             1962\n3             1970\n4             1994\n5             2002\n\n\n\n# What were the 6 World Cups with the highest attendance?\nworld_cup |&gt;\n  arrange(desc(attendance)) |&gt;\n  head(6)\n\n  year               host  winner    second       third      fourth\n1 1994                USA  Brazil     Italy      Sweden    Bulgaria\n2 2014             Brazil Germany Argentina Netherlands      Brazil\n3 2006            Germany   Italy    France     Germany    Portugal\n4 2018             Russia  France   Croatia     Belgium     England\n5 1998             France  France    Brazil     Croatia Netherlands\n6 2002 Japan, South Korea  Brazil   Germany      Turkey South Korea\n  goals_scored teams games attendance\n1          141    24    52    3568567\n2          171    32    64    3441450\n3          147    32    64    3367000\n4          169    32    64    3031768\n5          171    32    64    2859234\n6          161    32    64    2724604\n\n\n\n# Construct a univariate plot of goals_scored (no wrangling necessary)\n# This provides a visual summary of how the number of goals_scored varies from World Cup to World Cup\nggplot(world_cup, aes(x = goals_scored)) + \n  geom_bar()\n\n\n\n\n\n\n\n\n# Let's follow up the plot with some more precise numerical summaries\n# Calculate the min, median, and max number of goals_scored across all World Cups\n# NOTE: Visually compare these numerical summaries to what you observed in the plot\nworld_cup |&gt;\n  summarize(min(goals_scored), max(goals_scored), median(goals_scored))\n\n  min(goals_scored) max(goals_scored) median(goals_scored)\n1                70               171                  126\n\n\n\n# Construct a bivariate plot of how the number of goals_scored in the World Cup has changed over the years\n# No wrangling necessary\nggplot(world_cup, aes(x = year, y = goals_scored)) +\n  geom_line()\n\n\n\n\n\n\n\n\n# Our above summaries might be a bit misleading.\n# The number of games played at the World Cup varies.\n# Construct a bivariate plot of how the typical number of goals per game has changed over the years\nworld_cup |&gt;\n  mutate(goals_per_game = goals_scored/games) |&gt;\n  ggplot( aes(x = year, y = goals_per_game)) +\n  geom_line()\n\n\n\n\n\n\n\nExercise 11: Practice on Your Data\nReturn to the TidyTuesday data you’re using in Homework 3. Use your new wrangling skills to play around. What new insights can you gain?!\nI was using the water insecurity data, imported below:\n\nwaterData &lt;- tidytuesdayR::tt_load('2025-01-28')\n\n---- Compiling #TidyTuesday Information for 2025-01-28 ----\n--- There are 2 files available ---\n\n\n── Downloading files ───────────────────────────────────────────────────────────\n\n  1 of 2: \"water_insecurity_2022.csv\"\n  2 of 2: \"water_insecurity_2023.csv\"\n\n\n\n#I am interested in finding the total number of people in 2023 who did not have plumbing according to the results collected in the survey.\nwaterData$water_insecurity_2023 |&gt;\n  select(name, total_pop, percent_lacking_plumbing) |&gt;\n  mutate(number_wo_plumbing = total_pop * (percent_lacking_plumbing/100)) |&gt;\n  arrange(desc(number_wo_plumbing))\n\n# A tibble: 854 × 4\n   name                      total_pop percent_lacking_plum…¹ number_wo_plumbing\n   &lt;chr&gt;                         &lt;dbl&gt;                  &lt;dbl&gt;              &lt;dbl&gt;\n 1 Los Angeles County, Cali…   9663345                 0.0543               5248\n 2 Maricopa County, Arizona    4585871                 0.114                5241\n 3 Harris County, Texas        4835125                 0.0843               4077\n 4 Cook County, Illinois       5087072                 0.0726               3694\n 5 Apache County, Arizona        65036                 3.91                 2545\n 6 Orange County, California   3135755                 0.0705               2210\n 7 Navajo County, Arizona       109175                 1.85                 2019\n 8 Riverside County, Califo…   2492442                 0.0759               1892\n 9 Miami-Dade County, Flori…   2686867                 0.0680               1826\n10 Fairbanks North Star Bor…     94840                 1.88                 1786\n# ℹ 844 more rows\n# ℹ abbreviated name: ¹​percent_lacking_plumbing\n\n\n\n#One last thing I will try to find is the number of people in Alaska without plumbing in counties that were surveyed\nwaterData$water_insecurity_2023 |&gt;\n  filter(str_detect(name, \"Alaska\")) |&gt;\n  select(name, total_pop, percent_lacking_plumbing) |&gt;\n  mutate(number_wo_plumbing = total_pop * (percent_lacking_plumbing/100)) |&gt;\n  summarize(AlaskaWoPlumbing = sum(number_wo_plumbing))\n\n# A tibble: 1 × 1\n  AlaskaWoPlumbing\n             &lt;dbl&gt;\n1             3306",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wrangling</span>"
    ]
  },
  {
    "objectID": "ica/ica-dates.html",
    "href": "ica/ica-dates.html",
    "title": "\n15  Dates\n",
    "section": "",
    "text": "Exercise 1: More Filtering\nRecall the “logical comparison operators” we can use to filter() our data:",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Dates</span>"
    ]
  },
  {
    "objectID": "ica/ica-dates.html#exercises-part-2-application",
    "href": "ica/ica-dates.html#exercises-part-2-application",
    "title": "\n15  Dates\n",
    "section": "\n15.1 Exercises Part 2: Application",
    "text": "15.1 Exercises Part 2: Application\nThe remaining exercises are similar to some of those on the homework. Hence, the solutions are not provided. Let’s apply these ideas to the daily Birthdays dataset in the mosaic package.\n\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\n\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\n\n\nAttaching package: 'mosaic'\n\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\ndata(\"Birthdays\")\nhead(Birthdays)\n\n  state year month day       date wday births\n1    AK 1969     1   1 1969-01-01  Wed     14\n2    AL 1969     1   1 1969-01-01  Wed    174\n3    AR 1969     1   1 1969-01-01  Wed     78\n4    AZ 1969     1   1 1969-01-01  Wed     84\n5    CA 1969     1   1 1969-01-01  Wed    824\n6    CO 1969     1   1 1969-01-01  Wed    100\n\n\nBirthdays gives the number of births recorded on each day of the year in each state from 1969 to 19881. We can use our wrangling skills to understand some drivers of daily births. Putting these all together can be challenging! Remember the following ways to make tasks more manageable:\n\nTranslate the prompt into our 6 verbs (and count()). That is, think before you type.\nBuild your code line by line. It’s important to understand what’s being piped into each function!\n\nExercise 5: Warming up\n\n# How many days of data do we have for each state? Assuming that we have the same number of days of data for each state, which we should, considering the parameters of the data described.\nBirthdays |&gt;\n  summarize(max(date) - min(date))\n\n  max(date) - min(date)\n1             7304 days\n\n# How many total births were there in this time period?\nBirthdays |&gt;\n  summarize(sum(births))\n\n  sum(births)\n1    70486538\n\n# How many total births were there per state in this time period, sorted from low to high?\nBirthdays |&gt;\n  group_by(state) |&gt;\n  mutate(births_total = (sum(births))) |&gt;\n  filter(day == 1, year == 1969, month == 1) |&gt;\n  arrange(births_total)\n\n# A tibble: 51 × 8\n# Groups:   state [51]\n   state  year month   day date                wday  births births_total\n   &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dttm&gt;              &lt;ord&gt;  &lt;int&gt;        &lt;int&gt;\n 1 VT     1969     1     1 1969-01-01 00:00:00 Wed       20       147886\n 2 WY     1969     1     1 1969-01-01 00:00:00 Wed       16       154019\n 3 AK     1969     1     1 1969-01-01 00:00:00 Wed       14       185385\n 4 DE     1969     1     1 1969-01-01 00:00:00 Wed       32       188705\n 5 SD     1969     1     1 1969-01-01 00:00:00 Wed       22       235734\n 6 ND     1969     1     1 1969-01-01 00:00:00 Wed       30       238696\n 7 NV     1969     1     1 1969-01-01 00:00:00 Wed       14       241470\n 8 MT     1969     1     1 1969-01-01 00:00:00 Wed       24       253884\n 9 NH     1969     1     1 1969-01-01 00:00:00 Wed       34       264984\n10 RI     1969     1     1 1969-01-01 00:00:00 Wed       52       265038\n# ℹ 41 more rows\n\n\nExercise 6: Homework Reprise\nCreate a new dataset named daily_births that includes the total number of births per day (across all states) and the corresponding day of the week, eg, Mon. NOTE: Name the column with total births so that it’s easier to wrangle and plot.\n\ndaily_births_wrangling &lt;- Birthdays |&gt;\n  select(date, wday, births) |&gt;\n  group_by(date) |&gt;\n  mutate(total_births = sum(births))\n\nUsing this data, construct a plot of births over time, indicating the day of week.\n\nlibrary(ggplot2)\n\nggplot(daily_births_wrangling, aes(x = date, y = total_births, color = wday)) +\n  geom_point()\n\n\n\n\n\n\n\nExercise 7: Wrangle & Plot\nFor each prompt below, you can decide whether you want to: (1) wrangle and store data, then plot; or (2) wrangle data and pipe directly into ggplot. For example:\n\npenguins |&gt; \n  filter(species != \"Gentoo\") |&gt; \n  ggplot(aes(y = bill_length_mm, x = bill_depth_mm, color = species)) + \n    geom_point()\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nPart a\nCalculate the total number of births in each month and year, eg, Jan 1969, Feb 1969, …. Label month by names not numbers, eg, Jan not 1. Then, plot the births by month and comment on what you learn. There seem to be the most births in the late summer and early fall months, compared to far fewer births January, February, March, and April.\n\nlibrary(tidyverse)\n\n\nyear_month_birthdays &lt;- Birthdays |&gt;\n  group_by(month, year) |&gt;\n  mutate(births_in_month = sum(births)) |&gt;\n  mutate(month_char = month(date, label = TRUE)) |&gt;\n  mutate(month_year = paste(month_char, year, sep = \" \"))\n\n\n  ggplot(year_month_birthdays, aes(x = date, y = births_in_month, color = month_char)) +\n  geom_point()\n\n\n\n\n\n\n\nPart b\nIn 1988, calculate the total number of births per week in each state. Get rid of week “53”, which isn’t a complete week! Then, make a line plot of births by week for each state and comment on what you learn. For example, do you notice any seasonal trends? Are these the same in every state? Any outliers?\n\nBirthdays |&gt;\n  filter(year == 1988) |&gt;\n  filter(week(date) != 53) |&gt;\n  group_by(week(date)) |&gt;\n  mutate(births_in_week = sum(births)) |&gt;\n  ggplot(aes(x = week(date), y = births_in_week)) +\n  geom_line() +\n  labs(x= \"Week of the year\",\n       y = \"Births in week\")\n\n\n\n\n\n\n\nPart c\nRepeat the above for just Minnesota (MN) and Louisiana (LA). MN has one of the coldest climates and LA has one of the warmest. How do their seasonal trends compare? Do you think these trends are similar in other colder and warmer states? Try it!\n\nBirthdays |&gt;\n  filter(state == c(\"MN\", \"LA\")) |&gt;\n  filter(year == 1988) |&gt;\n  filter(week(date) != 53) |&gt;\n  group_by(week(date), state) |&gt;\n  mutate(births_in_week = sum(births)) |&gt;\n  ggplot(aes(x = week(date), y = births_in_week, color = state)) +\n  geom_line() +\n  labs(x= \"Week of the year\",\n       y = \"Births in week\")\n\n\n\n\n\n\n\nExercise 8: More Practice\nPart a\nCreate a dataset with only births in Massachusetts (MA) in 1979 and sort the days from those with the most births to those with the fewest.\n\nma_births &lt;- Birthdays |&gt;\n  filter(year == 1979) |&gt;\n  filter(state == \"MA\") |&gt;\n  arrange(desc(births))\n\nhead(ma_births)\n\n  state year month day       date wday births\n1    MA 1979     9  28 1979-09-28  Fri    262\n2    MA 1979     9  11 1979-09-11 Tues    252\n3    MA 1979    12  28 1979-12-28  Fri    249\n4    MA 1979     9  26 1979-09-26  Wed    246\n5    MA 1979     7  24 1979-07-24 Tues    245\n6    MA 1979     4  27 1979-04-27  Fri    243\n\n\nPart b\nMake a table showing the five states with the most births between September 9, 1979 and September 12, 1979, including the 9th and 12th. Arrange the table in descending order of births.\n\nBirthdays |&gt;\n  filter(date &lt;= \"1979-09-12\", date &gt;= \"1979-09-09\") |&gt;\n  group_by(state) |&gt;\n  mutate(sum_births = sum(births)) |&gt;\n  select(state, sum_births) |&gt;\n  distinct(.keep_all = TRUE) |&gt;\n  arrange(desc(sum_births)) |&gt;\n  head(5)\n\n# A tibble: 5 × 2\n# Groups:   state [5]\n  state sum_births\n  &lt;chr&gt;      &lt;int&gt;\n1 CA          3454\n2 TX          2467\n3 NY          2036\n4 IL          1758\n5 OH          1527",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Dates</span>"
    ]
  },
  {
    "objectID": "ica/ica-dates.html#footnotes",
    "href": "ica/ica-dates.html#footnotes",
    "title": "\n15  Dates\n",
    "section": "",
    "text": "The fivethirtyeight package has more recent data.↩︎",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Dates</span>"
    ]
  },
  {
    "objectID": "ica/ica-reshaping.html",
    "href": "ica/ica-reshaping.html",
    "title": "\n16  Reshaping\n",
    "section": "",
    "text": "Exercise 1: What’s the problem?\nConsider data on a sleep study in which subjects received only 3 hours of sleep per night. Each day, their reaction time to a stimulus (in ms) was recorded.1\nsleep_wide &lt;- read.csv(\"https://mac-stat.github.io/data/sleep_wide.csv\")\n\nhead(sleep_wide)\n\n  Subject  day_0  day_1  day_2  day_3  day_4  day_5  day_6  day_7  day_8  day_9\n1     308 249.56 258.70 250.80 321.44 356.85 414.69 382.20 290.15 430.59 466.35\n2     309 222.73 205.27 202.98 204.71 207.72 215.96 213.63 217.73 224.30 237.31\n3     310 199.05 194.33 234.32 232.84 229.31 220.46 235.42 255.75 261.01 247.52\n4     330 321.54 300.40 283.86 285.13 285.80 297.59 280.24 318.26 305.35 354.05\n5     331 287.61 285.00 301.82 320.12 316.28 293.32 290.08 334.82 293.75 371.58\n6     332 234.86 242.81 272.96 309.77 317.46 310.00 454.16 346.83 330.30 253.86",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Reshaping</span>"
    ]
  },
  {
    "objectID": "ica/ica-reshaping.html#footnotes",
    "href": "ica/ica-reshaping.html#footnotes",
    "title": "\n16  Reshaping\n",
    "section": "",
    "text": "Gregory Belenky, Nancy J. Wesensten, David R. Thorne, Maria L. Thomas, Helen C. Sing, Daniel P. Redmond, Michael B. Russo and Thomas J. Balkin (2003) Patterns of performance degradation and restoration during sleep restriction and subsequent recovery: a sleep dose-response study. Journal of Sleep Research 12, 1–12.↩︎",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Reshaping</span>"
    ]
  },
  {
    "objectID": "ica/ica-joining.html",
    "href": "ica/ica-joining.html",
    "title": "Joining",
    "section": "",
    "text": "17.1 Review\nWhere are we? Data preparation\nThus far, we’ve learned how to:",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Joining</span>"
    ]
  },
  {
    "objectID": "ica/ica-joining.html#review",
    "href": "ica/ica-joining.html#review",
    "title": "Joining",
    "section": "",
    "text": "arrange() our data in a meaningful order\nsubset the data to only filter() the rows and select() the columns of interest\n\nmutate() existing variables and define new variables\n\nsummarize() various aspects of a variable, both overall and by group (group_by())\nreshape our data to fit the task at hand (pivot_longer(), pivot_wider())",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Joining</span>"
    ]
  },
  {
    "objectID": "ica/ica-joining.html#motivation",
    "href": "ica/ica-joining.html#motivation",
    "title": "Joining",
    "section": "\n17.2 Motivation",
    "text": "17.2 Motivation\nIn practice, we often have to collect and combine data from various sources in order to address our research questions. Example:\n\nWhat are the best predictors of album sales?\nCombine:\n\nSpotify data on individual songs (eg: popularity, genre, characteristics)\nsales data on individual songs\n\n\nWhat are the best predictors of flight delays?\nCombine:\n\ndata on individual flights including airline, starting airport, and destination airport\ndata on different airlines (eg: ticket prices, reliability, etc)\ndata on different airports (eg: location, reliability, etc)\n\n\n\nExample 1\nConsider the following (made up) data on students and course enrollments:\n\nstudents_1 &lt;- data.frame(\n  student = c(\"A\", \"B\", \"C\"),\n  class = c(\"STAT 101\", \"GEOL 101\", \"ANTH 101\")\n)\n\n# Check it out\nstudents_1\n\n  student    class\n1       A STAT 101\n2       B GEOL 101\n3       C ANTH 101\n\n\n\nenrollments_1 &lt;- data.frame(\n  class = c(\"STAT 101\", \"ART 101\", \"GEOL 101\"),\n  enrollment = c(18, 17, 24)\n)\n\n# Check it out\nenrollments_1\n\n     class enrollment\n1 STAT 101         18\n2  ART 101         17\n3 GEOL 101         24\n\n\nOur goal is to combine or join these datasets into one. For reference, here they are side by side:\n\nFirst, consider the following:\n\nWhat variable or key do these datasets have in common? Thus by what information can we match the observations in these datasets?\nRelative to this key, what info does students_1 have that enrollments_1 doesn’t?\nRelative to this key, what info does enrollments_1 have that students_1 doesn’t?",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Joining</span>"
    ]
  },
  {
    "objectID": "ica/ica-joining.html#mutating-joins-left-inner-full",
    "href": "ica/ica-joining.html#mutating-joins-left-inner-full",
    "title": "Joining",
    "section": "\n17.3 Mutating Joins: left, inner, full\n",
    "text": "17.3 Mutating Joins: left, inner, full\n\nExample 2\nThere are various ways to join these datasets:\n\nLet’s learn by doing. First, try the left_join() function:\n\nlibrary(tidyverse)\nstudents_1 |&gt; \n  left_join(enrollments_1)\n\n  student    class enrollment\n1       A STAT 101         18\n2       B GEOL 101         24\n3       C ANTH 101         NA\n\n\n\nWhat did this do? What are the roles of students_1 (the left table) and enrollments_1 (the right table)?\nWhat, if anything, would change if we reversed the order of the data tables? Think about it, then try.\n\n\nlibrary(tidyverse)\nenrollments_1 |&gt; \n  left_join(students_1)\n\n     class enrollment student\n1 STAT 101         18       A\n2  ART 101         17    &lt;NA&gt;\n3 GEOL 101         24       B\n\n\nExample 3\nNext, explore how our datasets are joined using inner_join():\n\n\nstudents_1 |&gt; \n  inner_join(enrollments_1)\n\n  student    class enrollment\n1       A STAT 101         18\n2       B GEOL 101         24\n\n\n\nWhat did this do? What are the roles of students_1 (the left table) and enrollments_1 (the right table)?\nWhat, if anything, would change if we reversed the order of the data tables? Think about it, then try.\n\n\nenrollments_1 |&gt; \n  inner_join(students_1)\n\n     class enrollment student\n1 STAT 101         18       A\n2 GEOL 101         24       B\n\n\nExample 4\nNext, explore how our datasets are joined using full_join():\n\n\nstudents_1 |&gt; \n  full_join(enrollments_1)\n\n  student    class enrollment\n1       A STAT 101         18\n2       B GEOL 101         24\n3       C ANTH 101         NA\n4    &lt;NA&gt;  ART 101         17\n\n\n\nWhat did this do? What are the roles of students_1 (the left table) and enrollments_1 (the right table)?\nWhat, if anything, would change if we reversed the order of the data tables? Think about it, then try.\n\n\nenrollments_1 |&gt; \n  full_join(students_1)\n\n     class enrollment student\n1 STAT 101         18       A\n2  ART 101         17    &lt;NA&gt;\n3 GEOL 101         24       B\n4 ANTH 101         NA       C\n\n\n\n17.3.1 Summary\nMutating joins add new variables (columns) to the left data table from matching observations in the right table:\nleft_data |&gt; mutating_join(right_data)\nThe most common mutating joins are:\n\nleft_join()\nKeeps all observations from the left, but discards any observations in the right that do not have a match in the left.1\ninner_join()\nKeeps only the observations from the left with a match in the right.\nfull_join()\nKeeps all observations from the left and the right. (This is less common than left_join() and inner_join()).\n\nNOTE: When an observation in the left table has multiple matches in the right table, these mutating joins produce a separate observation in the new table for each match.",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Joining</span>"
    ]
  },
  {
    "objectID": "ica/ica-joining.html#filtering-joins-semi-anti",
    "href": "ica/ica-joining.html#filtering-joins-semi-anti",
    "title": "Joining",
    "section": "\n17.4 Filtering Joins: semi, anti\n",
    "text": "17.4 Filtering Joins: semi, anti\n\nMutating joins combine information, thus increase the number of columns in a dataset (like mutate()). Filtering joins keep only certain observations in one dataset (like filter()), not based on rules related to any variables in the dataset, but on the observations that exist in another dataset. This is useful when we merely care about the membership or non-membership of an observation in the other dataset, not the raw data itself.\nExample 5\nIn our example data, suppose enrollments_1 only included courses being taught in the Theater building:\n\n\nstudents_1 |&gt; \n  semi_join(enrollments_1)\n\n  student    class\n1       A STAT 101\n2       B GEOL 101\n\n\n\nWhat did this do? What info would it give us? Semi join did not add any new data, only showed the data that is included in the first data set and the second dataset and kept all relevant observations from the first dataset.\nHow does semi_join() differ from inner_join()? Inner join adds data from the other dataset.\nWhat, if anything, would change if we reversed the order of the data tables? Think about it, then try.\n\n\nenrollments_1 |&gt; \n  semi_join(students_1)\n\n     class enrollment\n1 STAT 101         18\n2 GEOL 101         24\n\n\nExample 6\nLet’s try another filtering join for our example data:\n\n\nstudents_1 |&gt; \n  anti_join(enrollments_1)\n\n  student    class\n1       C ANTH 101\n\n\n\nWhat did this do? What info would it give us? It only shows the observation data that is not included in the second data set.\nWhat, if anything, would change if we reversed the order of the data tables? Think about it, then try.\n\n\nenrollments_1 |&gt; \n  anti_join(students_1)\n\n    class enrollment\n1 ART 101         17\n\n\n\n17.4.1 Summary\nFiltering joins keep specific observations from the left table based on whether they match an observation in the right table.\n\nsemi_join()\nDiscards any observations in the left table that do not have a match in the right table. If there are multiple matches of right cases to a left case, it keeps just one copy of the left case.\nanti_join()\nDiscards any observations in the left table that do have a match in the right table.",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Joining</span>"
    ]
  },
  {
    "objectID": "ica/ica-joining.html#summary-of-all-joins",
    "href": "ica/ica-joining.html#summary-of-all-joins",
    "title": "Joining",
    "section": "\n17.5 Summary of All Joins",
    "text": "17.5 Summary of All Joins",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Joining</span>"
    ]
  },
  {
    "objectID": "ica/ica-joining.html#exercises",
    "href": "ica/ica-joining.html#exercises",
    "title": "Joining",
    "section": "\n17.6 Exercises",
    "text": "17.6 Exercises\nExercise 1: Where are my keys?\nPart a\nDefine two new datasets, with different students and courses:\n\nstudents_2 &lt;- data.frame(\n  student = c(\"D\", \"E\", \"F\"),\n  class = c(\"COMP 101\", \"BIOL 101\", \"POLI 101\")\n)\n\n# Check it out\nstudents_2\n\n  student    class\n1       D COMP 101\n2       E BIOL 101\n3       F POLI 101\n\nenrollments_2 &lt;- data.frame(\n  course = c(\"ART 101\", \"BIOL 101\", \"COMP 101\"),\n  enrollment = c(18, 20, 19)\n)\n\n# Check it out\nenrollments_2\n\n    course enrollment\n1  ART 101         18\n2 BIOL 101         20\n3 COMP 101         19\n\n\nTo connect the course enrollments to the students’ courses, try do a left_join(). You get an error! Identify the problem by reviewing the error message and the datasets we’re trying to join.\n\n# eval = FALSE: don't evaluate this chunk when knitting. it produces an error.\n\n# students_2 |&gt; \n#   left_join(enrollments_2)\n\nPart b\nThe problem is that course name, the key or variable that links these two datasets, is labeled differently: class in the students_2 data and course in the enrollments_2 data. Thus we have to specify these keys in our code:\n\nstudents_2 |&gt; \n  left_join(enrollments_2, join_by(class == course))\n\n  student    class enrollment\n1       D COMP 101         19\n2       E BIOL 101         20\n3       F POLI 101         NA\n\n\n\n# The order of the keys is important:\n# join_by(\"left data key\" == \"right data key\")\n# The order is mixed up here, thus we get an error:\n\n# students_2 |&gt; \n#   left_join(enrollments_2, join_by(course == class))\n\nPart c\nDefine another set of fake data which adds grade information:\n\n# Add student grades in each course\nstudents_3 &lt;- data.frame(\n  student = c(\"Y\", \"Y\", \"Z\", \"Z\"),\n  class = c(\"COMP 101\", \"BIOL 101\", \"POLI 101\", \"COMP 101\"),\n  grade = c(\"B\", \"S\", \"C\", \"A\")\n)\n\n# Check it out\nstudents_3\n\n  student    class grade\n1       Y COMP 101     B\n2       Y BIOL 101     S\n3       Z POLI 101     C\n4       Z COMP 101     A\n\n# Add average grades in each course\nenrollments_3 &lt;- data.frame(\n  class = c(\"ART 101\", \"BIOL 101\",\"COMP 101\"),\n  grade = c(\"B\", \"A\", \"A-\"),\n  enrollment = c(20, 18, 19)\n)\n\n# Check it out\nenrollments_3\n\n     class grade enrollment\n1  ART 101     B         20\n2 BIOL 101     A         18\n3 COMP 101    A-         19\n\n\nTry doing a left_join() to link the students’ classes to their enrollment info. Did this work? Try and figure out the culprit by examining the output.\n\nstudents_3 |&gt; \n  left_join(enrollments_3)\n\n  student    class grade enrollment\n1       Y COMP 101     B         NA\n2       Y BIOL 101     S         NA\n3       Z POLI 101     C         NA\n4       Z COMP 101     A         NA\n\n\nPart d\nThe issue here is that our datasets have 2 column names in common: class and grade. BUT grade is measuring 2 different things here: individual student grades in students_3 and average student grades in enrollments_3. Thus it doesn’t make sense to try to join the datasets with respect to this variable. We can again solve this by specifying that we want to join the datasets using the class variable as a key. What are grade.x and grade.y? grade.x is the grade from the first dataset and grade.y is the grade from the second dataset.\n\nstudents_3 |&gt; \n  left_join(enrollments_3, join_by(class == class))\n\n  student    class grade.x grade.y enrollment\n1       Y COMP 101       B      A-         19\n2       Y BIOL 101       S       A         18\n3       Z POLI 101       C    &lt;NA&gt;         NA\n4       Z COMP 101       A      A-         19\n\n\nExercise 2: More small practice\nBefore applying these ideas to bigger datasets, let’s practice identifying which join is appropriate in different scenarios. Define the following fake data on voters (people who have voted) and contact info for voting age adults (people who could vote):\n\n# People who have voted\nvoters &lt;- data.frame(\n  id = c(\"A\", \"D\", \"E\", \"F\", \"G\"),\n  times_voted = c(2, 4, 17, 6, 20)\n)\n\nvoters\n\n  id times_voted\n1  A           2\n2  D           4\n3  E          17\n4  F           6\n5  G          20\n\n# Contact info for voting age adults\ncontact &lt;- data.frame(\n  name = c(\"A\", \"B\", \"C\", \"D\"),\n  address = c(\"summit\", \"grand\", \"snelling\", \"fairview\"),\n  age = c(24, 89, 43, 38)\n)\n\ncontact\n\n  name  address age\n1    A   summit  24\n2    B    grand  89\n3    C snelling  43\n4    D fairview  38\n\n\nUse the appropriate join for each prompt below. In each case, think before you type:\n\nWhat dataset goes on the left?\nWhat do you want the resulting dataset to look like? How many rows and columns will it have?\n\n\n# 1. We want contact info for people who HAVEN'T voted\ncontact |&gt;\n  anti_join(voters, join_by(name == id))\n\n  name  address age\n1    B    grand  89\n2    C snelling  43\n\n# 2. We want contact info for people who HAVE voted\ncontact |&gt;\n  inner_join(voters, join_by(name == id))\n\n  name  address age times_voted\n1    A   summit  24           2\n2    D fairview  38           4\n\n# 3. We want any data available on each person\ncontact |&gt;\n  full_join(voters, join_by(name == id))\n\n  name  address age times_voted\n1    A   summit  24           2\n2    B    grand  89          NA\n3    C snelling  43          NA\n4    D fairview  38           4\n5    E     &lt;NA&gt;  NA          17\n6    F     &lt;NA&gt;  NA           6\n7    G     &lt;NA&gt;  NA          20\n\n# 4. When possible, we want to add contact info to the voting roster\nvoters |&gt;\n  left_join(contact, join_by(id == name))\n\n  id times_voted  address age\n1  A           2   summit  24\n2  D           4 fairview  38\n3  E          17     &lt;NA&gt;  NA\n4  F           6     &lt;NA&gt;  NA\n5  G          20     &lt;NA&gt;  NA\n\n\nExercise 3: Bigger datasets\nLet’s apply these ideas to some bigger datasets. In grades, each row is a student-class pair with information on:\n\n\nsid = student ID\n\ngrade = student’s grade\n\nsessionID = an identifier of the class section\n\n\n\n     sid grade   sessionID\n1 S31185    D+ session1784\n2 S31185    B+ session1785\n3 S31185    A- session1791\n4 S31185    B+ session1792\n5 S31185    B- session1794\n6 S31185    C+ session1795\n\n\nIn courses, each row corresponds to a class section with information on:\n\n\nsessionID = an identifier of the class section\n\ndept = department\n\nlevel = course level (eg: 100)\n\nsem = semester\n\nenroll = enrollment (number of students)\n\niid = instructor ID\n\n\n\n    sessionID dept level    sem enroll     iid\n1 session1784    M   100 FA1991     22 inst265\n2 session1785    k   100 FA1991     52 inst458\n3 session1791    J   100 FA1993     22 inst223\n4 session1792    J   300 FA1993     20 inst235\n5 session1794    J   200 FA1993     22 inst234\n6 session1795    J   200 SP1994     26 inst230\n\n\nUse R code to take a quick glance at the data.\n\n# How many observations (rows) and variables (columns) are there in the grades data?\nnrow(grades)\n\n[1] 5844\n\nncol(grades)\n\n[1] 3\n\n# How many observations (rows) and variables (columns) are there in the courses data?\nnrow(courses)\n\n[1] 1718\n\nncol(courses)\n\n[1] 6\n\n\nExercise 4: Class size\nHow big are the classes?\nPart a\nBefore digging in, note that some courses are listed twice in the courses data:\n\ncourses |&gt; \n  count(sessionID) |&gt; \n  filter(n &gt; 1)\n\n     sessionID n\n1  session2047 2\n2  session2067 2\n3  session2448 2\n4  session2509 2\n5  session2541 2\n6  session2824 2\n7  session2826 2\n8  session2862 2\n9  session2897 2\n10 session3046 2\n11 session3057 2\n12 session3123 2\n13 session3243 2\n14 session3257 2\n15 session3387 2\n16 session3400 2\n17 session3414 2\n18 session3430 2\n19 session3489 2\n20 session3524 2\n21 session3629 2\n22 session3643 2\n23 session3821 2\n\n\nIf we pick out just 1 of these, we learn that some courses are cross-listed in multiple departments:\n\ncourses |&gt; \n  filter(sessionID == \"session2047\")\n\n    sessionID dept level    sem enroll     iid\n1 session2047    g   100 FA2001     12 inst436\n2 session2047    m   100 FA2001     28 inst436\n\n\nFor our class size exploration, obtain the total enrollments in each sessionID, combining any cross-listed sections. Save this as courses_combined. NOTE: There’s no joining to do here!\n\ncourses_combined &lt;- courses |&gt;\n  group_by(sessionID) |&gt;\n  summarize(enroll = sum(enroll))\n\n# Check that this has 1695 rows and 2 columns\ndim(courses_combined)\n\n[1] 1695    2\n\n\nPart b\nLet’s first examine the question of class size from the administration’s viewpoint. To this end, calculate the median class size across all class sections. (The median is the middle or 50th percentile. Unlike the mean, it’s not skewed by outliers.) THINK FIRST:\n\nWhich of the 2 datasets do you need to answer this question? One? Both? We will use the one with combined listings, the courses_combined dataset\nIf you need course information, use courses_combined not courses.\nDo you have to do any joining? If so, which dataset will go on the left, i.e. which dataset includes your primary observations of interest? Which join function will you need?\n\n\ncourses_combined |&gt;\n  summarize(median(enroll))\n\n# A tibble: 1 × 1\n  `median(enroll)`\n             &lt;int&gt;\n1               18\n\n\nPart c\nBut how big are classes from the student perspective? To this end, calculate the median class size for each individual student. Once you have the correct output, store it as student_class_size. THINK FIRST:\n\nWhich of the 2 datasets do you need to answer this question? One? Both?\nIf you need course information, use courses_combined not courses.\nDo you have to do any joining? If so, which dataset will go on the left, i.e. which dataset includes your primary observations of interest? Which join function will you need?\n\n\nstudent_class_size &lt;- grades |&gt;\n  left_join(courses_combined) |&gt;\n  group_by(sid) |&gt;\n  summarize(med_class = median(enroll))\n\nhead(student_class_size)\n\n# A tibble: 6 × 2\n  sid    med_class\n  &lt;chr&gt;      &lt;dbl&gt;\n1 S31185      23.5\n2 S31188      21  \n3 S31191      25  \n4 S31194      15  \n5 S31197      24  \n6 S31200      21  \n\n\nPart d\nThe median class size varies from student to student. To get a sense for the typical student experience and range in student experiences, construct and discuss a histogram of the median class sizes experienced by the students.\nMost students have a median class size of around 20 students. Some have a median class size of almost 40, but there are very few students with this number. The chart is right skewed, and most students experiences an average class size of between 15 and 25.\n\nggplot(student_class_size, aes(x = med_class)) +\n  geom_histogram() +\n  labs(title = \"Average class size experienced by students\",\n       x = \"Size of Class\",\n       y = \"Number of students with size as median\")\n\n\n\n\n\n\n\nExercise 5: Narrowing in on classes\nPart a\nShow data on the students that enrolled in session1986. THINK FIRST: Which of the 2 datasets do you need to answer this question? One? Both?\n\ngrades |&gt;\n  filter(sessionID == \"session1986\")\n\n     sid grade   sessionID\n1 S31401    B+ session1986\n2 S32247     B session1986\n\n\nPart b\nBelow is a dataset with all courses in department E:\n\ndept_E &lt;- courses |&gt; \n  filter(dept == \"E\")\n\nWhat students enrolled in classes in department E? (We just want info on the students, not the classes.)\n\ngrades |&gt;\n  left_join(dept_E) |&gt;\n  filter(dept == \"E\") |&gt;\n  select(sid, grade, sessionID, sem)\n\n      sid grade   sessionID    sem\n1  S31245     A session2326 SP2002\n2  S31470     B session3658 FA2004\n3  S31470     B session3798 FA2004\n4  S31470     A session3799 FA2004\n5  S31938     A session2326 SP2002\n6  S31968     A session3104 SP2004\n7  S32022     A session3798 FA2004\n8  S32046    A- session2326 SP2002\n9  S32226     A session2326 SP2002\n10 S32415     B session2835 SP2003\n11 S32415    B+ session3799 FA2004\n12 S32484    A- session3658 FA2004\n\n#or\n\ngrades |&gt;\n  semi_join(dept_E)\n\n      sid grade   sessionID\n1  S31245     A session2326\n2  S31470     B session3658\n3  S31470     B session3798\n4  S31470     A session3799\n5  S31938     A session2326\n6  S31968     A session3104\n7  S32022     A session3798\n8  S32046    A- session2326\n9  S32226     A session2326\n10 S32415     B session2835\n11 S32415    B+ session3799\n12 S32484    A- session3658\n\n\nExercise 6: All the wrangling\nUse all of your wrangling skills to answer the following prompts! THINK FIRST:\n\nThink about what tables you might need to join (if any). Identify the corresponding variables to match.\nYou’ll need an extra table to convert grades to grade point averages:\n\n\ngpa_conversion &lt;- tibble(\n  grade = c(\"A+\", \"A\", \"A-\", \"B+\", \"B\", \"B-\", \"C+\", \"C\", \"C-\", \"D+\", \"D\", \"D-\", \"NC\", \"AU\", \"S\"), \n  gp = c(4.3, 4, 3.7, 3.3, 3, 2.7, 2.3, 2, 1.7, 1.3, 1, 0.7, 0, NA, NA)\n)\n\ngpa_conversion\n\n# A tibble: 15 × 2\n   grade    gp\n   &lt;chr&gt; &lt;dbl&gt;\n 1 A+      4.3\n 2 A       4  \n 3 A-      3.7\n 4 B+      3.3\n 5 B       3  \n 6 B-      2.7\n 7 C+      2.3\n 8 C       2  \n 9 C-      1.7\n10 D+      1.3\n11 D       1  \n12 D-      0.7\n13 NC      0  \n14 AU     NA  \n15 S      NA  \n\n\nPart a\nHow many total student enrollments are there in each department? Order from high to low.\n\ncourses |&gt;\n  group_by(dept) |&gt;\n  summarize(total = sum(enroll)) |&gt;\n  arrange(desc(total))\n\n# A tibble: 40 × 2\n   dept  total\n   &lt;chr&gt; &lt;int&gt;\n 1 d      3046\n 2 j      2312\n 3 O      2178\n 4 M      2129\n 5 m      2105\n 6 D      2003\n 7 W      1960\n 8 q      1859\n 9 k      1824\n10 F      1587\n# ℹ 30 more rows\n\n\nPart b\nWhat’s the grade-point average (GPA) for each student?\n\ngrades |&gt;\n  left_join(gpa_conversion) |&gt;\n  group_by(sid) |&gt;\n  summarize(mean(gp, na.rm = TRUE))\n\n# A tibble: 443 × 2\n   sid    `mean(gp, na.rm = TRUE)`\n   &lt;chr&gt;                     &lt;dbl&gt;\n 1 S31185                     2.41\n 2 S31188                     3.02\n 3 S31191                     3.21\n 4 S31194                     3.36\n 5 S31197                     3.35\n 6 S31200                     2.2 \n 7 S31203                     3.82\n 8 S31206                     2.46\n 9 S31209                     3.13\n10 S31212                     3.67\n# ℹ 433 more rows\n\n\nPart c\nWhat’s the median GPA across all students?\n\ngrades |&gt;\n  left_join(gpa_conversion) |&gt;\n  group_by(sid) |&gt;\n  summarize(gpa = mean(gp, na.rm = TRUE)) |&gt;\n  summarize(median(gpa))\n\n# A tibble: 1 × 1\n  `median(gpa)`\n          &lt;dbl&gt;\n1          3.47\n\n\nPart d\nWhat fraction of grades are below B+?\n\ngrades |&gt;\n  left_join(gpa_conversion) |&gt;\n  filter(gp &lt; 3.3) |&gt;\n  nrow()\n\n[1] 1539\n\n  # summarize(nrow()/nrow(grades))\nnrow(grades)\n\n[1] 5844\n\n1539/5844\n\n[1] 0.263347\n\n#I tried my own approach above and then needed to look at the answers. I have commented in the code what each line does.\n\ngrades |&gt; \n  left_join(gpa_conversion) |&gt;                  # Adds the GPA conversion chart onto each letter grade value for math calculations\n  mutate(below_b_plus = (gp &lt; 3.3)) |&gt;          # Creates a new variable that is either 1 or 0 based on whether the gp for that row is less than 3.3.\n  summarize(mean(below_b_plus, na.rm = TRUE))   # Removes NA values from the new variable and summarizes the mean of the column created with a bunch of ones and 0s.\n\n  mean(below_b_plus, na.rm = TRUE)\n1                        0.2834776\n\n\nPart e\nWhat’s the grade-point average for each instructor? Order from low to high.\n\ngrades |&gt; \n  left_join(gpa_conversion) |&gt; \n  left_join(courses) |&gt; \n  group_by(iid) |&gt; \n  summarize(gpa = mean(gp, na.rm = TRUE)) |&gt; \n  arrange(gpa)\n\n# A tibble: 364 × 2\n   iid       gpa\n   &lt;chr&gt;   &lt;dbl&gt;\n 1 inst265  1.3 \n 2 inst444  1.7 \n 3 inst513  1.85\n 4 inst200  2   \n 5 inst507  2.2 \n 6 inst445  2.3 \n 7 inst420  2.6 \n 8 inst262  2.65\n 9 inst176  2.66\n10 inst234  2.7 \n# ℹ 354 more rows\n\n\nPart f\nCHALLENGE: Estimate the grade-point average for each department, and sort from low to high. NOTE: Don’t include cross-listed courses. Students in cross-listed courses could be enrolled under either department, and we do not know which department to assign the grade to. HINT: You’ll need to do multiple joins.\n\ncross_listed &lt;- courses |&gt;    # Creates a new dataset that is courses with cross-listing\n  count(sessionID) |&gt;         # Counts the number of each unique session ID\n  filter(n &gt; 1)               # Filters the dataset to be only those courses that are cross listed in 2 or more departments. \n\ngrades |&gt;                     # Puts the whole grades dataset through a new process\n  anti_join(cross_listed) |&gt;  # Gets rid of all observations that included cross listed courses, leaving only one of each\n  inner_join(courses) |&gt;      # Keeps only the values from grades that are also in the courses data set\n  left_join(gpa_conversion) |&gt; # Adds the GPA conversion dataset for easier accessibility to math features\n  group_by(dept) |&gt;           # Separates the dataset by categories in the `dept` variable for the next calculation\n  summarize(gpa = mean(gp, na.rm = TRUE)) |&gt; # Gives the mean value for gpa as categorized by dept, removing NA values in the gp column\n  arrange(gpa)                # Arranges in ascending order by the new gpa column that has been creates by `summarize`\n\n# A tibble: 39 × 2\n   dept    gpa\n   &lt;chr&gt; &lt;dbl&gt;\n 1 o      3.08\n 2 M      3.10\n 3 K      3.17\n 4 G      3.18\n 5 B      3.2 \n 6 J      3.22\n 7 T      3.23\n 8 b      3.25\n 9 F      3.30\n10 d      3.31\n# ℹ 29 more rows",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Joining</span>"
    ]
  },
  {
    "objectID": "ica/ica-joining.html#footnotes",
    "href": "ica/ica-joining.html#footnotes",
    "title": "Joining",
    "section": "",
    "text": "There is also a right_join() that adds variables in the reverse direction from the left table to the right table, but we do not really need it as we can always switch the roles of the two tables.︎↩︎",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Joining</span>"
    ]
  },
  {
    "objectID": "ica/ica-factors.html",
    "href": "ica/ica-factors.html",
    "title": "\n18  Factors\n",
    "section": "",
    "text": "18.1 Exercises\nThe exercises revisit our grades data:\n# Get rid of some duplicate rows!\ngrades &lt;- read.csv(\"https://mac-stat.github.io/data/grades.csv\") |&gt; \n  distinct(sid, sessionID, .keep_all = TRUE)\n\n# Check it out\nhead(grades)\n\n     sid grade   sessionID\n1 S31185    D+ session1784\n2 S31185    B+ session1785\n3 S31185    A- session1791\n4 S31185    B+ session1792\n5 S31185    B- session1794\n6 S31185    C+ session1795\nWe’ll explore the number of times each grade was assigned:\ngrade_distribution &lt;- grades |&gt; \n  count(grade)\n\nhead(grade_distribution)\n\n  grade    n\n1     A 1506\n2    A- 1381\n3    AU   27\n4     B  804\n5    B+ 1003\n6    B-  330",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Factors</span>"
    ]
  },
  {
    "objectID": "ica/ica-factors.html#exercises",
    "href": "ica/ica-factors.html#exercises",
    "title": "\n18  Factors\n",
    "section": "",
    "text": "Exercise 1: Changing Order\nCheck out a column plot of the number of times each grade was assigned during the study period. This is similar to a bar plot, but where we define the height of a bar according to variable in our dataset.\n\ngrade_distribution |&gt; \n  ggplot(aes(x = grade, y = n)) +\n    geom_col()\n\n\n\n\n\n\n\nThe order of the grades is goofy! Construct a new column plot, manually reordering the grades from high (A) to low (NC) with “S” and “AU” at the end:\n\ngrade_distribution |&gt;\n  mutate(grade = fct_relevel(grade, c(\"A\", \"A-\", \"B+\", \"B\", \"B-\", \"C+\", \"C\", \"C-\", \"D+\", \"D\", \"D-\", \"NC\", \"S\", \"AU\"))) |&gt;\n  ggplot(aes(x = grade, y = n)) +\n    geom_col()\n\n\n\n\n\n\n\nConstruct a new column plot, reordering the grades in ascending frequency (i.e. how often the grades were assigned):\n\ngrade_distribution |&gt;\n  mutate(grade = fct_reorder(grade, n)) |&gt;\n  ggplot(aes(x = grade, y = n)) +\n    geom_col()\n\n\n\n\n\n\n\nConstruct a new column plot, reordering the grades in descending frequency (i.e. how often the grades were assigned):\n\ngrade_distribution |&gt;\n  mutate(grade = fct_reorder(grade, n, .desc = TRUE)) |&gt;\n  ggplot(aes(x = grade, y = n)) +\n    geom_col()\n\n\n\n\n\n\n\nExercise 2: Changing Factor Level Labels\nIt may not be clear what “AU” and “S” stand for. Construct a new column plot that renames these levels “Audit” and “Satisfactory”, while keeping the other grade labels the same and in a meaningful order:\n\ngrade_distribution |&gt;\n  mutate(grade = fct_relevel(grade, c(\"A\", \"A-\", \"B+\", \"B\", \"B-\", \"C+\", \"C\", \"C-\", \"D+\", \"D\", \"D-\", \"NC\", \"S\", \"AU\"))) |&gt;\n  mutate(grade = fct_recode(grade, \"Audit\" = \"AU\", \"Satisfactory\" = \"S\")) |&gt;  # Multiple pieces go into the last 2 blanks\n  ggplot(aes(x = grade, y = n)) +\n    geom_col()",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Factors</span>"
    ]
  },
  {
    "objectID": "ica/ica-strings.html",
    "href": "ica/ica-strings.html",
    "title": "\n19  Strings\n",
    "section": "",
    "text": "19.1 Exercises",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Strings</span>"
    ]
  },
  {
    "objectID": "ica/ica-strings.html#exercises",
    "href": "ica/ica-strings.html#exercises",
    "title": "\n19  Strings\n",
    "section": "",
    "text": "Exercise 1: Time slots\nThe courses data includes actual data scraped from Mac’s class schedule. (Thanks to Prof Leslie Myint for the scraping code!!)\nIf you want to learn how to scrape data, take COMP/STAT 212, Intermediate Data Science! NOTE: For simplicity, I removed classes that had “TBA” for the days.\n\ncourses &lt;- read.csv(\"https://mac-stat.github.io/data/registrar.csv\")\n\n# Check it out\nhead(courses)\n\n       number   crn                                                name  days\n1 AMST 112-01 10318         Introduction to African American Literature M W F\n2 AMST 194-01 10073              Introduction to Asian American Studies M W F\n3 AMST 194-F1 10072 What’s After White Empire - And Is It Already Here?  T R \n4 AMST 203-01 10646 Politics and Inequality: The American Welfare State M W F\n5 AMST 205-01 10842                         Trans Theories and Politics  T R \n6 AMST 209-01 10474                   Civil Rights in the United States   W  \n             time      room             instructor avail_max\n1 9:40 - 10:40 am  MAIN 009       Daylanne English    3 / 20\n2  1:10 - 2:10 pm MUSIC 219          Jake Nagasawa   -4 / 16\n3  3:00 - 4:30 pm   HUM 214 Karin Aguilar-San Juan    0 / 14\n4 9:40 - 10:40 am  CARN 305          Lesley Lavery    3 / 25\n5  3:00 - 4:30 pm  MAIN 009              Myrl Beam   -2 / 20\n6 7:00 - 10:00 pm  MAIN 010         Walter Greason   -1 / 15\n\n\nUse our more familiar wrangling tools to warm up.\n\n# Construct a table that indicates the number of classes offered in each day/time slot\n# Print only the 6 most popular time slots\ncourses |&gt;\n  group_by(days, time) |&gt;\n  summarize(number = n()) |&gt;\n  arrange(desc(number))\n\n`summarise()` has grouped output by 'days'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 106 × 3\n# Groups:   days [12]\n   days    time             number\n   &lt;chr&gt;   &lt;chr&gt;             &lt;int&gt;\n 1 \"M W F\" 10:50 - 11:50 am     76\n 2 \" T R \" 9:40 - 11:10 am      71\n 3 \"M W F\" 9:40 - 10:40 am      68\n 4 \"M W F\" 1:10 - 2:10 pm       66\n 5 \" T R \" 3:00 - 4:30 pm       62\n 6 \" T R \" 1:20 - 2:50 pm       59\n 7 \"M W F\" 2:20 - 3:20 pm       41\n 8 \"M W F\" 12:00 - 1:00 pm      36\n 9 \"M    \" 7:00 - 10:00 pm      23\n10 \"M W F\" 3:30 - 4:30 pm       18\n# ℹ 96 more rows\n\n\nExercise 2: Prep the data\nSo that we can analyze it later, we want to wrangle the courses data:\n\nLet’s get some enrollment info:\n\nSplit avail_max into 2 separate variables: avail and max.\nUse avail and max to define a new variable called enroll. HINT: You’ll need as.numeric()\n\n\n\nSplit the course number into 3 separate variables: dept, number, and section. HINT: You can use separate() to split a variable into 3, not just 2 new variables.\n\nStore this as courses_clean so that you can use it later.\n\ncourses_w &lt;- courses |&gt;\n  separate(avail_max, c(\"avail\", \"max\"), sep = \"/\") |&gt;\n  mutate(enroll = as.numeric(max)-as.numeric(avail)) |&gt;\n  separate(number, c(\"dept\", \"number_section\"), sep = \" \") |&gt;\n  separate(number_section, c(\"number\", \"section\"), sep = \"-\")\n\nhead(courses_w)\n\n  dept number section   crn                                                name\n1 AMST    112      01 10318         Introduction to African American Literature\n2 AMST    194      01 10073              Introduction to Asian American Studies\n3 AMST    194      F1 10072 What’s After White Empire - And Is It Already Here?\n4 AMST    203      01 10646 Politics and Inequality: The American Welfare State\n5 AMST    205      01 10842                         Trans Theories and Politics\n6 AMST    209      01 10474                   Civil Rights in the United States\n   days            time      room             instructor avail max enroll\n1 M W F 9:40 - 10:40 am  MAIN 009       Daylanne English    3   20     17\n2 M W F  1:10 - 2:10 pm MUSIC 219          Jake Nagasawa   -4   16     20\n3  T R   3:00 - 4:30 pm   HUM 214 Karin Aguilar-San Juan    0   14     14\n4 M W F 9:40 - 10:40 am  CARN 305          Lesley Lavery    3   25     22\n5  T R   3:00 - 4:30 pm  MAIN 009              Myrl Beam   -2   20     22\n6   W   7:00 - 10:00 pm  MAIN 010         Walter Greason   -1   15     16\n\n\nExercise 3: Courses by department\nUsing courses_clean…\n\n# Identify the 6 departments that offered the most sections\ncourses_w |&gt;\n  group_by(dept) |&gt;\n  summarize(sections_n = n()) |&gt;\n  arrange(desc(sections_n))\n\n# A tibble: 40 × 2\n   dept  sections_n\n   &lt;chr&gt;      &lt;int&gt;\n 1 SPAN          45\n 2 BIOL          44\n 3 ENVI          38\n 4 PSYC          37\n 5 CHEM          33\n 6 COMP          31\n 7 ENGL          30\n 8 ECON          29\n 9 POLI          27\n10 FREN          26\n# ℹ 30 more rows\n\n# Identify the 6 departments with the longest average course titles\ncourses_w |&gt;\n  group_by(dept) |&gt;\n  summarize(average_name_length = mean(str_length(name))) |&gt;\n  arrange(desc(average_name_length))\n\n# A tibble: 40 × 2\n   dept  average_name_length\n   &lt;chr&gt;               &lt;dbl&gt;\n 1 WGSS                 46.3\n 2 INTL                 41.4\n 3 EDUC                 39.4\n 4 MCST                 39.4\n 5 POLI                 37.4\n 6 AMST                 37.3\n 7 ASIA                 36.8\n 8 SOCI                 36.2\n 9 ENGL                 35.7\n10 LATI                 34.4\n# ℹ 30 more rows\n\n\nExercise 4: STAT courses\nPart a\nGet a subset of courses_clean that only includes courses taught by Alicia Johnson.\n\ncourses_w |&gt;\n  filter(instructor == \"Alicia Johnson\")\n\n  dept number section   crn                         name  days            time\n1 STAT    253      01 10806 Statistical Machine Learning  T R  9:40 - 11:10 am\n2 STAT    253      02 10807 Statistical Machine Learning  T R   1:20 - 2:50 pm\n3 STAT    253      03 10808 Statistical Machine Learning  T R   3:00 - 4:30 pm\n        room     instructor avail max enroll\n1 THEATR 206 Alicia Johnson   -3   20     23\n2 THEATR 206 Alicia Johnson   -3   20     23\n3 THEATR 206 Alicia Johnson    2   20     18\n\n\nPart b\nCreate a new dataset from courses_clean, named stat, that only includes STAT sections. In this dataset:\n\n\nIn the course names:\n\nRemove “Introduction to” from any name.\nShorten “Statistical” to “Stat” where relevant.\n\n\nDefine a variable that records the start_time for the course.\nKeep only the number, name, start_time, enroll columns.\nThe result should have 19 rows and 4 columns.\n\n\nstat &lt;- courses_w |&gt;\n  filter(dept == \"STAT\") |&gt;\n  mutate(name = str_replace(name, \"Statistical\", \"Stat\")) |&gt;\n  mutate(name = str_replace(name, \"Introduction to\", \"\"))\n\nhead(stat)\n\n  dept number section   crn           name  days            time       room\n1 STAT    112      01 10249   Data Science  T R   3:00 - 4:30 pm   OLRI 254\n2 STAT    112      02 10251   Data Science  T R  9:40 - 11:10 am THEATR 205\n3 STAT    112      03 10253   Data Science  T R   1:20 - 2:50 pm THEATR 205\n4 STAT    125      01 10799   Epidemiology M W F 12:00 - 1:00 pm THEATR 206\n5 STAT    155      01 10800  Stat Modeling M W F  1:10 - 2:10 pm THEATR 206\n6 STAT    155      02 10801  Stat Modeling M W F 9:40 - 10:40 am   OLRI 254\n         instructor avail max enroll\n1 Brianna Heggeseth    1   28     27\n2     Amin Alhashim    3   24     21\n3     Amin Alhashim    1   26     25\n4   Vittorio Addona   -2   24     26\n5   Vittorio Addona   -8   24     32\n6     Taylor Okonek    0   24     24\n\n\nExercise 5: More cleaning\nIn the next exercises, we’ll dig into enrollments. Let’s get the data ready for that analysis here. Make the following changes to the courses_clean data. Because they have different enrollment structures, and we don’t want to compare apples and oranges, remove the following:\n\nall sections in PE and INTD (interdisciplinary studies courses)\nall music ensembles and dance practicums, i.e. all MUSI and THDA classes with numbers less than 100. HINT: !(dept == \"MUSI\" & as.numeric(number) &lt; 100)\nall lab sections. Be careful which variable you use here. For example, you don’t want to search by “Lab” and accidentally eliminate courses with words such as “Labor”.\n\nSave the results as enrollments (don’t overwrite courses_clean).\n\nenrollments &lt;- courses_w |&gt;\n  filter(dept != \"PE\") |&gt;\n  filter(dept != \"INTD\") |&gt;\n  filter(!(dept == \"MUSI\" & as.numeric(number) &lt; 100)) |&gt;\n  filter(str_detect(section, \"L\") == FALSE)\n\nhead(enrollments)\n\n  dept number section   crn                                                name\n1 AMST    112      01 10318         Introduction to African American Literature\n2 AMST    194      01 10073              Introduction to Asian American Studies\n3 AMST    194      F1 10072 What’s After White Empire - And Is It Already Here?\n4 AMST    203      01 10646 Politics and Inequality: The American Welfare State\n5 AMST    205      01 10842                         Trans Theories and Politics\n6 AMST    209      01 10474                   Civil Rights in the United States\n   days            time      room             instructor avail max enroll\n1 M W F 9:40 - 10:40 am  MAIN 009       Daylanne English    3   20     17\n2 M W F  1:10 - 2:10 pm MUSIC 219          Jake Nagasawa   -4   16     20\n3  T R   3:00 - 4:30 pm   HUM 214 Karin Aguilar-San Juan    0   14     14\n4 M W F 9:40 - 10:40 am  CARN 305          Lesley Lavery    3   25     22\n5  T R   3:00 - 4:30 pm  MAIN 009              Myrl Beam   -2   20     22\n6   W   7:00 - 10:00 pm  MAIN 010         Walter Greason   -1   15     16\n\n\nExercise 6: Enrollment & departments\nExplore enrollments by department. You decide what research questions to focus on. Use both visual and numerical summaries.\nI am curious about which departments have the most full classes (100% enrollment). Looking at the numerical summary and the graph generated in the code below, I can see that math classes have the highest percentage of spots filled, with an average enrollment of over 100% of what is allowed! Only ~50% of classes in the Chinese department are filled, making it the least percent full department at Macalester.\n\nenrollments |&gt;\n  mutate(percent_full = as.numeric(enroll)/as.numeric(max)) |&gt;\n  group_by(dept) |&gt;\n  summarize(avg_percent_full = mean(percent_full)) |&gt;\n  arrange(desc(avg_percent_full))\n\n# A tibble: 38 × 2\n   dept  avg_percent_full\n   &lt;chr&gt;            &lt;dbl&gt;\n 1 MATH             1.01 \n 2 PSYC             0.984\n 3 INTL             0.938\n 4 BIOL             0.935\n 5 STAT             0.934\n 6 ART              0.885\n 7 COMP             0.881\n 8 HIST             0.880\n 9 EDUC             0.875\n10 ENGL             0.873\n# ℹ 28 more rows\n\nenrollments |&gt;\n  mutate(percent_full = as.numeric(enroll)/as.numeric(max)) |&gt;\n  group_by(dept) |&gt;\n  summarize(avg_percent_full = mean(percent_full)) |&gt;\nggplot(aes(x = dept, y = avg_percent_full)) +\n  geom_bar(stat = \"identity\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n\n\nI am also curious about how many individual students each professor teaches each semester. The data does not show us students taking more than one class under the same professor, so we will have to assume that that does not happen when we conduct the analysis below.\n\n#We are interested in the `instructor` and `enroll` variable in this section.\n\nenrollments |&gt;\n  group_by(instructor) |&gt;\n  summarize(students_per_instructor = sum(enroll)) |&gt;\n  arrange(desc(students_per_instructor))\n\n# A tibble: 249 × 2\n   instructor        students_per_instructor\n   &lt;chr&gt;                               &lt;dbl&gt;\n 1 Kiarina Kordela                       126\n 2 Christie Manning                      116\n 3 Tina Kruse                            116\n 4 Serdar Yalçin                         107\n 5 James Doyle                           104\n 6 Daylanne English                      100\n 7 Chris Wells                            98\n 8 Jenna Rice Rahaim                      97\n 9 Amin Alhashim                          92\n10 Mary Heskel                            92\n# ℹ 239 more rows\n\nenrollments |&gt;\n  group_by(instructor) |&gt;\n  summarize(students_per_instructor = sum(enroll)) |&gt;\n  arrange(desc(students_per_instructor))\n\n# A tibble: 249 × 2\n   instructor        students_per_instructor\n   &lt;chr&gt;                               &lt;dbl&gt;\n 1 Kiarina Kordela                       126\n 2 Christie Manning                      116\n 3 Tina Kruse                            116\n 4 Serdar Yalçin                         107\n 5 James Doyle                           104\n 6 Daylanne English                      100\n 7 Chris Wells                            98\n 8 Jenna Rice Rahaim                      97\n 9 Amin Alhashim                          92\n10 Mary Heskel                            92\n# ℹ 239 more rows\n\n# Then we attempt to make a visualization that shows variation between departments, taking into account that some professors teach in more than one department. I have created a new smaller dataset with just the necessary information to do that, as well as the teachers enrolled with each professor in each department.\nenrollments_by_profdept &lt;- enrollments |&gt;\n  group_by(dept, instructor) |&gt;\n  summarize(enrollment = sum(enroll))\n\n`summarise()` has grouped output by 'dept'. You can override using the\n`.groups` argument.\n\n\n\n  ggplot(enrollments_by_profdept, aes(y = fct_reorder(instructor, enrollment), x = enrollment, fill = dept)) +\n  geom_bar(stat = \"identity\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n\n\nThe above chart is more experimental than anything, in terms of color-coding professors by the departments they teach. Still, it gives a different impression, as we see that Andrew Beveridge has the most student enrollments in any one department, compared to Kiarina Kordela with the most enrollments overall. In both cases, Kyoshin Sasahara has the lowest enrollment overall, followed closely by Kathy Maxwell, who have 3 and 4 students enrolled in their classes respectively. Looking ahead at the next activity, I see that many professors are listed with a lot of enrollment because their courses are cross listed.\nExercise 7: Enrollment & faculty\nLet’s now explore enrollments by instructor. In doing so, we have to be cautious of cross-listed courses that are listed under multiple different departments. Uncomment the code lines in the chunk below for an example.\n\n\n\n\n\n\nCommenting/Uncommenting Code\n\n\n\nTo comment/uncomment several lines of code at once, highlight them then click ctrl/cmd+shift+c.\n\n\n\nenrollments |&gt;\n  filter(dept %in% c(\"STAT\", \"COMP\"), number == 112, section == \"01\")\n\n  dept number section   crn                         name  days           time\n1 COMP    112      01 10248 Introduction to Data Science  T R  3:00 - 4:30 pm\n2 STAT    112      01 10249 Introduction to Data Science  T R  3:00 - 4:30 pm\n      room        instructor avail max enroll\n1 OLRI 254 Brianna Heggeseth    1   28     27\n2 OLRI 254 Brianna Heggeseth    1   28     27\n\n\nNotice that these are the exact same section! In order to not double count an instructor’s enrollments, we can keep only the courses that have distinct() combinations of days, time, instructor values. Uncomment the code lines in the chunk below.\n\nenrollments_2 &lt;- enrollments |&gt;\n  distinct(days, time, instructor, .keep_all = TRUE)\n\n# NOTE: By default this keeps the first department alphabetically\n# That's fine because we won't use this to analyze department enrollments!\nenrollments_2 |&gt;\n  filter(instructor == \"Brianna Heggeseth\", name == \"Introduction to Data Science\")\n\n  dept number section   crn                         name  days           time\n1 COMP    112      01 10248 Introduction to Data Science  T R  3:00 - 4:30 pm\n      room        instructor avail max enroll\n1 OLRI 254 Brianna Heggeseth    1   28     27\n\n\nNow, explore enrollments by instructor. You decide what research questions to focus on. Use both visual and numerical summaries.\nCAVEAT: The above code doesn’t deal with co-taught courses that have more than one instructor. Thus instructors that co-taught are recorded as a pair, and their co-taught enrollments aren’t added to their total enrollments. This is tough to get around with how the data were scraped as the instructor names are smushed together, not separated by a comma!\n\n# Let's try the same visualization from above that orders professors by the number of students taught and color codes by the department that they teach in.\nenrollments_nocrosslist &lt;- enrollments |&gt;\n  group_by(dept, instructor) |&gt;\n  summarize(enrollment = sum(enroll))\n\n`summarise()` has grouped output by 'dept'. You can override using the\n`.groups` argument.\n\n  ggplot(enrollments_nocrosslist, aes(y = fct_reorder(instructor, enrollment), x = enrollment, fill = dept)) +\n  geom_bar(stat = \"identity\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n\nenrollments |&gt;\n  group_by(dept, instructor) |&gt;\n  summarize(enrollment = sum(enroll))\n\n`summarise()` has grouped output by 'dept'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 346 × 3\n# Groups:   dept [38]\n   dept  instructor             enrollment\n   &lt;chr&gt; &lt;chr&gt;                       &lt;dbl&gt;\n 1 AMST  Alicia Muñoz                   20\n 2 AMST  Amy Sullivan                   25\n 3 AMST  Brian Lozenski                 28\n 4 AMST  Daylanne English               50\n 5 AMST  Duchess Harris                 34\n 6 AMST  Ebony Aya                      11\n 7 AMST  Jake Nagasawa                  34\n 8 AMST  Karin Aguilar-San Juan         25\n 9 AMST  Katrina Phillips               18\n10 AMST  Kirisitina Sailiata            31\n# ℹ 336 more rows\n\n\nOptional extra practice\n\n# Make a bar plot showing the number of night courses by day of the week\n# Use courses_clean\n# I used a different data set for my courses clean data\n\ncourses_w |&gt;\n  select(time) |&gt;\n  group_by(time) |&gt;\n  summarize(n())\n\n# A tibble: 55 × 2\n   time               `n()`\n   &lt;chr&gt;              &lt;int&gt;\n 1 \"\"                     1\n 2 \"10:10 - 11:10 am\"     5\n 3 \"10:30 - 11:10 am\"     1\n 4 \"10:50 - 11:50 am\"    77\n 5 \"10:50 - 12:20 pm\"     1\n 6 \"12:00 - 1:00 pm\"     38\n 7 \"1:10 - 2:10 pm\"      69\n 8 \"1:10 - 3:10 pm\"       2\n 9 \"1:10 - 4:20 pm\"       3\n10 \"1:10 - 4:30 pm\"       3\n# ℹ 45 more rows\n\ncourses_w |&gt;\n  filter(time %in% c(\"6:30 - 10:00 pm\", \"6:45 - 8:15 pm\", \"7:00 - 10:00 pm\", \"7:00 - 8:00 pm\", \"7:00 - 8:30 pm\", \"7:00 - 9:00 pm\", \"7:15 - 8:45 pm\")) |&gt;\n  ggplot(aes(x = days)) +\n  geom_bar()",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Strings</span>"
    ]
  },
  {
    "objectID": "ica/quarto-demo.html",
    "href": "ica/quarto-demo.html",
    "title": "\n20  My first Quarto document\n",
    "section": "",
    "text": "20.1 Intro\nMacalester College is in the Twin Cities. It has:\nCheck it out for yourself:",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>My first Quarto document</span>"
    ]
  },
  {
    "objectID": "ica/quarto-demo.html#intro",
    "href": "ica/quarto-demo.html#intro",
    "title": "\n20  My first Quarto document\n",
    "section": "",
    "text": "four seasons\nbagpipes\ndelightful students",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>My first Quarto document</span>"
    ]
  },
  {
    "objectID": "ica/quarto-demo.html#exercise-1-deduce-quarto-features",
    "href": "ica/quarto-demo.html#exercise-1-deduce-quarto-features",
    "title": "\n20  My first Quarto document\n",
    "section": "\n20.2 Exercise 1: Deduce Quarto features",
    "text": "20.2 Exercise 1: Deduce Quarto features\nCheck out the appearance and contents of this document. Thoughts?\nIn the toolbar at the top of this document, Render the .qmd file into a .html file. Where is this file stored? Thoughts about its appearance / contents? Can you edit it?\nThis file is stored in my repository initially created for class, and renders into html, where I cannot edit it.\nToggling between the .qmd and .html files, explain the purpose of the following features in the .qmd file:\n*\n**\n#\n-\n\\\n![](url)\nThese features appear to make things bold",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>My first Quarto document</span>"
    ]
  },
  {
    "objectID": "ica/quarto-demo.html#exercise-2-code",
    "href": "ica/quarto-demo.html#exercise-2-code",
    "title": "\n20  My first Quarto document\n",
    "section": "\n20.3 Exercise 2: Code",
    "text": "20.3 Exercise 2: Code\nHow does this appear in the .qmd? The .html? So…?!\nseq(from = 100, to = 1000, by = 50)\nThey look the same and it’s not doing the sequence command. It is simply showing these sequence of characters as a text string.",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>My first Quarto document</span>"
    ]
  },
  {
    "objectID": "ica/quarto-demo.html#exercise-3-chunks",
    "href": "ica/quarto-demo.html#exercise-3-chunks",
    "title": "\n20  My first Quarto document\n",
    "section": "\n20.4 Exercise 3: Chunks",
    "text": "20.4 Exercise 3: Chunks\nQuarto isn’t a mind reader – we must distinguish R code from text. We do so by putting code inside an R chunk:\n\nseq(from = 100, to = 1000, by = 50)\n\n [1]  100  150  200  250  300  350  400  450  500  550  600  650  700  750  800\n[16]  850  900  950 1000\n\n\n\nPut the seq() code in the chunk.\nPress the green arrow in the top right of the chunk. What happens in the qmd? The qmd shows the processed results of the code.\nRender. What appears in the html: R code, output, or both? The R code and the output both show up when you render the file, but with slightly different formating than in the qmd",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>My first Quarto document</span>"
    ]
  },
  {
    "objectID": "ica/quarto-demo.html#exercise-4-practice",
    "href": "ica/quarto-demo.html#exercise-4-practice",
    "title": "\n20  My first Quarto document\n",
    "section": "\n20.5 Exercise 4: Practice",
    "text": "20.5 Exercise 4: Practice\n\nUse R code to create the following sequence: 10 10 10 10\nStore the sequence as four_tens.\nUse an R function (which we haven’t learned!) to add up the numbers in four_tens.\n\n\nfour_tens &lt;- rep(10,4)\nsum(four_tens)\n\n[1] 40",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>My first Quarto document</span>"
    ]
  },
  {
    "objectID": "ica/quarto-demo.html#exercise-5-fix-this-code",
    "href": "ica/quarto-demo.html#exercise-5-fix-this-code",
    "title": "\n20  My first Quarto document\n",
    "section": "\n20.6 Exercise 5: Fix this code",
    "text": "20.6 Exercise 5: Fix this code\nCode is a form of communication, and the code below doesn’t cut it.\nPut the code in a chunk and fix it.\n\nrep(x = 1, times = 10)\n\n [1] 1 1 1 1 1 1 1 1 1 1\n\nseq(from = 100, to = 1000, length = 20)\n\n [1]  100.0000  147.3684  194.7368  242.1053  289.4737  336.8421  384.2105\n [8]  431.5789  478.9474  526.3158  573.6842  621.0526  668.4211  715.7895\n[15]  763.1579  810.5263  857.8947  905.2632  952.6316 1000.0000\n\nnumber_of_students&lt;-27",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>My first Quarto document</span>"
    ]
  },
  {
    "objectID": "ica/quarto-demo.html#exercise-6-comments",
    "href": "ica/quarto-demo.html#exercise-6-comments",
    "title": "\n20  My first Quarto document\n",
    "section": "\n20.7 Exercise 6: Comments",
    "text": "20.7 Exercise 6: Comments\nRun the chunk below. Notice that R ignores anything in a line starting with a pound sign (#). If we took the # away we’d get an error!\n\n# This is a comment\n4 + 5\n\n[1] 9\n\n\nWe’ll utilize this feature to comment our code, i.e. leave short notes about what our code is doing. Below, replace the ??? with an appropriate comment.\n\n# This code calculates what 10 degrees celcius is in fahrenheit\ntemperature_c &lt;- 10\ntemperature_f &lt;- temperature_c * 9/5 + 32\ntemperature_f\n\n[1] 50",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>My first Quarto document</span>"
    ]
  },
  {
    "objectID": "bw/bw-exam1-cheatsheet.html",
    "href": "bw/bw-exam1-cheatsheet.html",
    "title": "\n21  Exam 1 Summary Sheet\n",
    "section": "",
    "text": "library(grid)\n\nimg &lt;- png::readPNG(\"../exam1cheatsheet.png\")\ngrid.raster(img, height = unit(1, \"npc\"))\n\n\n\n\n\n\n\nAbove is my cheatsheet summary of visualizations from all visualization units in this class.",
    "crumbs": [
      "Summaries",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Exam 1 Summary Sheet</span>"
    ]
  },
  {
    "objectID": "bw/bw-exam2-cheatsheet.html",
    "href": "bw/bw-exam2-cheatsheet.html",
    "title": "\n22  Exam 2 Summary Sheet\n",
    "section": "",
    "text": "library(grid)\n\nimg &lt;- png::readPNG(\"../exam2cheatsheet.png\")\ngrid.raster(img, height = unit(1, \"npc\"))\n\n\n\n\n\n\n\nAbove is my cheatsheet summary of visualizations from all units for exam 2 in this class.",
    "crumbs": [
      "Summaries",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Exam 2 Summary Sheet</span>"
    ]
  }
]